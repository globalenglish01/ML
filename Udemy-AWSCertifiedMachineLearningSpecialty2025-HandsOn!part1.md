### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/001_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p01 01. Udemy 101.ai-zh.srt

```
# 🎬 001_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p01 01. Udemy 101

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                嘿
只需几个 快速提示，教你如何使用Udemy的用户界面
以便充分利用这门课程
在你学习的过程中
首先 你在学习过程中有任何问题
可以点击这里的浏览问答按钮
这将打开本课的问答窗口
很可能你的问题已经有人问过
并且已经得到解答 你可以浏览与本课相关的以前问题
如果你有问题
点击新建问题按钮，详细描述问题
Sundog教育有专人每天监控
所以你会在两到四小时内得到回复
在这里
如果你的母语不是英语
或者在嘈杂的环境中
很难听清我说的话
当你播放视频时
这里有一个速度控制
默认情况下
它是1x 但你可以将其降至0.7
或5x，甚至减半
或者你可以加快速度 如果你想更快
记住，这些工具随时可用
你也可以查看转录
这实际上是我说的文字
你可以在这里跟随
或者你可以点击字幕
CC按钮
选择你的语言
这将为你提供字幕
这将以文字形式显示我说的话
这可以帮助你巩固听到的内容
也很方便 如果你在播放视频时遇到任何问题
点击这里的齿轮图标
你可以选择不同的分辨率
自动通常是最佳选择
但你可以减慢速度
或者选择较低分辨率
以确保视频流畅播放
此外，如果你在学习过程中遇到任何问题
请随时联系我
如果你在学习过程中遇到任何问题
请随时联系我
然而，底部有一个'稍后观看'按钮，你可以点击它
你可以稍后再回来观看
如果你还没有准备好，请点击那个按钮
不要随意点击一些随机的星级评价来关闭窗口
因为每一条评价都很重要
这对我有很大的影响，可以帮助我更直接地为你制作更多的课程
当你被提示时，请留下真实的评价
你随时都可以更新你的评价
也可以在课程控制面板中编辑它
我希望这些对你来说会有一些有用的提示，帮助你更好地利用这门课程 那么，让我们回到课程本身
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/003_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p03 04. Get the Course Materials.ai-zh.srt

```
# 🎬 003_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p03 04. Get the Course Materials

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在开始之前
让我们确保你对本课程所需的材料熟悉
特别是那些贯穿本课程的动手练习和实验室
前往 sundog gdash
教育点 Com Slash aws
认证机器学习课程材料
就像那样 请注意每个小横杠的大小写
它们都很重要 我会在讲义的资源中提供链接
这样你不必手动输入所有内容
一旦到达该页面
点击下载课程材料的链接
下载完成后
打开它保存在你操作系统的任何地方
解压缩生成的zip文件
你应该会看到这样的东西
记住放在哪里
放在一个安全的地方 你可能想将文件移动出默认下载文件夹
以便更容易跟踪
你将需要在课程后期使用这些文件
我们会告诉你何时使用它们
别担心
如果你想要课程幻灯片的副本以供后期参考
这里也有提供
如果你跟着这些动手活动
记住它们需要真金白银
机器学习通常不在aws的免费层中
你可以预期花费大约5美元，美国资金
如果你自己完成这些练习
如果你在完成服务后忘记关闭服务
它有可能花费更多
你已经被警告了
在使用这些练习后小心关闭服务
否则你可能会得到一个非常糟糕的惊喜
你不必做动手练习
它们是可选的
此外，有多种方式与我们保持联系
你可以订阅我们的邮件列表或在社交媒体上关注我们
完全自愿
当然，但我们很乐意收到你的消息
当然 就这样 让我们继续课程的主体
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/004_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p04 01. Section Intro Data Engineering.ai-zh.srt

```
# 🎬 004_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p04 01. Section Intro Data Engineering

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们要讨论的第一个领域是数据工程
你会发现在实际操作中，你花费大量时间收集和处理数据
你需要训练你的人工智能算法
通常，这比构建机器学习算法本身需要更多的努力
挑战在于将你的数据移动到训练模型所需的位置
以能够处理大规模数据并满足严格的安全要求的方式
我将此事交给Stefan Marek
他是AWS存储解决方案的专家，并获得了AWS解决方案架构师的认证
除了在AWS认证世界中是一位非常受欢迎的讲师 所以，开始吧 Stefan，带我们走进AWS中的数据工程世界
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/005_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p05 02. [Important] AWS Console UI Update.ai-zh.srt

```
# 🎬 005_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p05 02. [Important] AWS Console UI Update

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                你好，在我们开始这个课程之前
关于aws的UI变化，请注意
你可能看到这样的界面
一个大而明亮的
白色 然后是一些圆角按钮和亮蓝色
这是aws的新UI
而旧UI看起来有点更灰
蓝色稍微淡一些
按钮是方形而不是圆形
嗯 这是更圆滑的新界面
更现代的界面
在视觉层面上
看起来可能会有所不同
好的 但在可用性方面，界面应该完全相同
如果你看到这种界面录制的视频
请放心，新界面从可用性角度来看应该完全相同
不仅仅是从视觉角度来看
嗯 如果变化太大
如果有按钮
位置移动等等
请告诉我 我会尽力更新些视频
但在此期间我会留下我的视频
因为它们有很多运行在所谓的
所谓的旧UI，新UI只是为了让你去摸索
但它们完全相同
只是按钮的新颜色和新形状
好的 所以非常感谢 我会在下次讲座再见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/006_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p06 03. Set up an AWS Billing Alarm.ai-zh.srt

```
# 🎬 006_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p06 03. Set up an AWS Billing Alarm

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                如果你选择跟随本课程中的实际操作活动
你可能想设置一个计费警报
只是为了确保你没有无意中留下正在运行的东西
这可能会花费你钱
如果你只是按照这里的指示进行操作并清理现场
就像我们向你展示的那样 完成本课程不应该花费你超过几美元
但无意中留下正在运行的东西实在是太容易了
最终会在aws账单上给你一个糟糕的惊喜
所以，去你的仪表板这里
一旦你创建了aws账户
搜索计费
让我们设置一个警报
这样我们就可以收到通知
如果我们的开支开始超过我们不舒服的门槛
从这里 往下到预算和规划，然后选择预算
创建一个新的预算
使用模板
如果你不愿意花费任何东西
你可以选择零预算
但再次，你不会免费完成本课程
如果你真的不想花费任何东西
那么你应该只看视频而不是跟随实际操作
但如果你有一个月想要坚持的预算
你可以选择月度成本预算
输入你愿意花费的金额
让我们说20美元，并指定你想要收到通知的电子邮件地址
如果你超过那个金额
那么设置这个
如果我开始超过20美元的aws费用
我会收到一封电子邮件 所以我可以查看我忘记关闭什么并尝试修复它
在产生更大的费用之前
一旦完成，点击创建预算
然后你会自动收到通知
如果你有任何未关闭的资源可能会产生费用
这是你不想要的，所以这是个好主意
只是为了保护自己
因为忘记关闭某事并让整个月份运行昂贵的资源实在是太容易了
你不想那样做 所以花点时间保护自己并设置一个计费警报，然后我们继续
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/007_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p07 04. Amazon S3 - Overview.ai-zh.srt

```
# 🎬 007_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p07 04. Amazon S3 - Overview

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 那么我们开始讲s three
首先你可能已经知道s three是什么
但我只想快速过一遍要点
你需要知道的东西来通过aws机器学习考试
s three允许你存储对象或文件在桶中
这将是你在aws中做所有事情的中心
桶必须有一个全球唯一的名称
对象是文件
它们有一个键，键是文件的完整路径
例如 你有一个名为bucket的文件夹
你也可以有一个非常长的文件名
正如我们在这门课程中看到的
这可能是我的文件夹
另一个文件夹 我的txt文件
实际上它们并不是真正的文件夹
这只是一个非常长的键名
当我们看分区时会很有趣
所以我们将在接下来的两页中看到如何进行分区
这将在我们在athena上进行查询时非常有帮助
或者当我们开始分区我们的数据以更好地查询它时
所以最大对象大小为5000GB
所以你必须记住，如果你有一个非常大的数据集
那么你无法将其放入一个对象中
因为一个对象的最大尺寸是5TB
你需要将其分成多个对象
因为一个对象的最大尺寸是5TB
最后，我们可以为对象添加标签
他们的键值对
它们将对分类我们的数据非常有帮助
对于安全和生命周期
对于s三的机器学习
我们有什么 它将成为许多aws ml服务的支柱
例如 sagemaker正如弗兰克将向你展示的那样，将有助于创建数据集
这就是为什么数据工程的这一节之所以存在
因此，有无限的尺寸
使用S3时你不需要准备任何东西
你有11个9的可靠性
这意味着你的数据是安全的
你不应该在很长时间内丢失一个对象
然后是存储与计算的解耦
这是S3，我们将在下面看到，用于所有数据处理
所以E C Athena
Redshift 光谱识别
胶水 所有这些都是计算侧
它们完全独立于存储侧
这是s3
这种范式允许您创建大规模的数据
例如s3 这是非常可扩展的
它与所有其他计算服务一起工作
这是一种集中式架构
这意味着您的所有数据都可以放在一个地方
这是对象存储
这意味着您支持任何文件格式
s3甚至不看您的数据是什么
您只需将您想要的任何东西放在那里
因此您可以处理任何格式的数据
例如 CSV
JSON Parquet
或c avro或proto buff
这使得s3
成为云中数据湖的完美用例
现在让我们谈谈数据分区
数据分区是一种模式，用于加快查询速度
例如 如果您使用Amazon Athena
这是一种无服务器方式查询数据的服务
并且在动手实验中我们会看到
那么您可能希望按日期分区
例如 我们有s3桶
然后我们有我的数据集
然后我们有年份，月份，日期，小时
这些被称为分区
因为它们会将我们的数据集分开到不同的文件夹和分区中
然后我们最终有我的数据.csv
它们在我们开始频繁查询数据时非常有用
因为我们可以根据选择的年份，月份，日期和小时快速找到正确的数据集
但如果您经常按产品查询
也许您想重新组织您的分区方案
因此我的数据集以产品ID为首要分区
然后您的数据就在这里
这将使我们能够快速找到符合定义的产品ID的正确数据
您可以定义任何您喜欢的分区策略
但日期和产品是非常常见的
一些我们将使用的工具
例如，Kinesis将为我们分区数据
例如 Kinesis
那么现在我们开始创建我们的第一个s三存储桶
让我们进入亚马逊s三
我们将为我们的课程创建一个存储桶
如你所见 我有很多存储桶
但我将要创建一个存储桶，我将其命名为
它是机器学习 stefan v四
我将选择一个离我最近的区域
例如 爱尔兰
我会往下滚动
保留所有默认设置
然后点击创建桶
所以我的桶没有创建
我可以进入并开始创建文件夹
命名讲师
这就是我们将要上传第一套数据集到亚马逊S3的地方
然后我们会添加一些分区
所以接下来创建文件夹
我将其命名为2022
这是为了增加一些分区
这是年份
然后我可以在里面
创建一个子文件夹10
这是月份
然后我会在其内创建一个子文件夹
创建文件夹 我将其命名为2021
然后创建文件夹
这就是这一天，在这天里我可以上传我的数据集
所以我会上传并添加一个文件
这就是导师数据，你必须在此之前下载
这只是一个csv文件，其中包含一些导师数据
我会在几秒钟内向你展示
数据现在已经上传
如果我们看一下
只有三行
所以导师姓名课程和爱情计分板作为列
然后我们有斯泰凡梅里克这门课程
拥有100个爱情计分器和弗兰克凯恩
这只是一些假数据
只是为了上传CSV并开始课程
如果我在我的桶里
我可以看到这一切
这就是我们开始这节课的内容，我们已经上传了一些数据
我们已经对数据进行了分区
实际上我们的S3桶
我们已经开始做这个 所以我希望你喜欢它 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/008_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p08 05. Amazon S3 Storage Classes + Glacier.ai-zh.srt

```
# 🎬 008_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p08 05. Amazon S3 Storage Classes + Glacier

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 那么我们来讨论一下亚马逊云存储的不同类别
首先，我们有亚马逊S3标准通用存储
然后我们有亚马逊云存储频繁访问
然后我们有亚马逊云存储一区频繁访问
然后我们有冰川即时检索
冰川灵活检索
冰川深度归档
然后是最后，Magens3智能分层
我们将深入了解这些类别在本课中
但你必须了解它们以便于考试
然后你在s three中创建对象时
你可以选择它的类
你也可以手动修改其存储类
或者正如我们将看到的
你可以使用亚马逊s three生命周期配置将对象自动移动到这些所有存储类之间
所以首先在我们深入这些类之前
让我们定义耐久性和可用性的概念
所以耐久性代表亚马逊s three将丢失一个对象多少次
并且亚马逊具有非常高的耐久性
它被称为119
所以是99点
然后乘以99%
这意味着平均来说
如果你在亚马逊s三中存储一千万个对象
你可以期望每十万年丢失一个对象
所以它是非常耐用的，并且在亚马逊s三中，所有存储类的耐用性都是一样的
亚马逊s三的可用性表示服务是多么容易获得
因此这取决于存储类
例如 S三标准有99.9%
99%的可用性
这意味着大约5
3分钟一年中
服务将不可用
这意味着当你处理服务时，你会遇到一些错误
所以你在开发应用程序时需要考虑这一点
因此S三标准有99.9%
99.9%的可用性
将被用于频繁访问的数据
这就是你默认使用的存储类型
它具有低延迟和高吞吐量
它可以在aws方面承受两并发设施故障
它的用例将会很大
数据 分析
移动和游戏应用
以及内容分发
我们有s三不频繁访问
这就是说，这将是较少频繁访问的数据
但在需要时需要快速访问
它将比标准s便宜
但你将支付检索费用
因此，s三标准a为99.9%
可用性为99.9%
因此，可用性略低
其用例将是灾难恢复
并且备份是免费的
一个区域在访问中
一个区域具有高耐久性
好的 在同一个区域中
数据将会丢失
如果这个区域受到破坏
以及耐用性
它的可用性更低 所以是99.5%
百分之五的可用性
因此S3 One A的使用场景是存储备份的次要副本，比如本地数据或者可以重建的数据
我们有冰川存储类
所以冰川是随着敌人变得非常寒冷
所以它是低成本的对象存储，用于存档和备份
定价是你将支付存储费用
再加上检索费用
你在Glacier这里有三种存储类别
你有亚马逊S3 Glacier的即时检索
这将给你毫秒级的检索
这很好 例如，对于每季度访问一次的数据
最低存储持续时间是九十天
这是备份
但你需要在毫秒内访问它
然后我们有冰川灵活检索
它以前被称为亚马逊冰川免费冰川
然后随着他们添加更多层次，他们重新命名了事物
所以亚马逊冰川灵活检索有三种灵活性
所以你有快速获取数据，时间在1到5分钟之间
你有标准获取数据，时间在三到五小时之间，或批量
这是免费的 数据在五到十二小时内恢复的地方
以及手术持续时间的最低限度以及九十天
所以这里实例意味着你的真实数据数据即时和灵活
这意味着你愿意等待到
例如十二小时以检索您的数据
然后我们有一个冰川深档案
这是用于长期存储的意思
所以我们有两个层次的 reas
好的 我们有一个标准的十二小时和大量的四十八小时
所以你可能准备好等待很长时间来检索数据
但这将给你最低的成本
并且最低存储时间也是一百八十天
所以你知道这是很多存储类别
并且有一个最后的叫做s智能分层
这将允许你根据使用模式移动对象到不同的层
为此
你将承担一小笔月度监控费和自动分层费
并且在s智能分层中没有额外费用
所以频繁访问是在这里是自动的
这里有默认的
然后我们有很少访问的层对于没有被访问的对象
例如三十天
然后你有存档即时访问层也是自动的对于没有被访问的对象超过九十天
然后有存档访问在这里是可选的
你可以从九十天到七加天配置
然后你有深度存档访问在这里也是可选的
你可以配置对于没有被访问的对象
在一百八十天到七百加天
好的 所以工程是为了让你坐回并放松
而as free mood会为你移动对象
所以如果你比较所有存储类别
你不需要记住这些数字
但这是为了让你理解它们是什么
所以你得到十一个数据冗余
然后能力随着你有的区域下降
当然 嗯
它只显示你像
例如 最低存储时间
费用等等 所以花点时间自己看这个图表
你应该理解它
但你不应该记住它
所以如果我们看看一些定价
例如在us east one
这是所有存储类别你将有的价格
再次你不应该记住一切
但这对你自己花时间去看是好的
只是为了确保你理解
如果你理解类别的名字
那么你应该能理解这些类别
好的 这就是这节课的内容 我希望你喜欢它 我将在下节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/009_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p09 06. Amazon S3 Storage + Glacier - Hands On.ai-zh.srt

```
# 🎬 009_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p09 06. Amazon S3 Storage + Glacier - Hands On

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么让我们在三中创建一个新的桶并命名为s三存储类
演示2022
好的 然后我创建在任何地区
我将继续创建这个桶
所以回到我的桶
我可以继续上传一个对象并点击添加文件
我将选择我的咖啡jpeg并看看选项
我们可以查看该对象的属性
在存储类下
我可以看到广泛的存储类
这是为最佳对象设计的
所以我们有s三标准
好的，我们有设计为列
以及我们有多少个az
以及一些其他作为最小存储持续时间
最小计费对象大小和监控
和o二级存储费用 那么让我们看看所有这些
我们有标准 这是默认的基本
然后我们得到智能分级，如果我们不知道我们的数据模式
因此我们希望它为我们执行数据点分级
标准a如果我们想要数据不经常访问
但具有低延迟
一a您可以重新创建此数据
它将存储在一个az中
因此您可以运行列表
运行丢失对象的风险
如果az被摧毁
然后我们有冰川
所以有冰川即时检索 冰川灵活检索
或冰川深度归档
它们确切地告诉你这里是什么情况
最后注册在c
这是一个弃用类型的存储类
因此我没有在课程中描述它
那么如果我们选择标准，例如
并创建一个对象在那里
所以我们在那里 然后我们说上传
回到我们的桶
所以现在这个对象有存储类标准
正如这里所示
但我可以做的是如果我想改变存储类
我可以更改存储类
所以我可以进入属性并滚动到下方
如果我想要的话
所以我可以进入属性并滚动到下方
我们可以实际上编辑存储类别以做不同的事情
所以我们可以移动它
例如将其移动到一个区域
在这种情况下这个对象将仅存储在一个区域
让我们保存这些更改
现在我的对象已成功编辑
因此对象类别已更改
所以我们向下滚动
我们现在在一个区域a中，再次
您可以编辑它并前往
例如前往冰川实例检索
现在它将被归档
或者您可以前往智能分层
它将被自动设置为正确的层
基于我们的模式等
因此您可以看到使用存储类别有很多强大的功能
最后我不会向您展示如何
我们可以自动化在这些不同存储类别之间移动这些对象
所以让我们回到我们的桶
它们在管理下
您可以创建生命周期规则
您可以创建一个规则
我们将其称为演示规则
然后您会说嘿
应用于桶中的所有对象
是的当然
然后我们可以说好吧
将当前版本移动到存储类别
您会说嘿
您将去标准a之后
例如
三十天
然后您将去智能分层后六十天
然后您将去冰川灵活检索后一百八十天
等等 因此您会得到一些过渡
在这里您可以查看所有您所做的过渡
因此您可以自动化在层之间移动对象
好的 就是这样
我们已经看到了所有需要了解的存储类别
我希望您喜欢它 我将在下次讲座见到您
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/010_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p10 07. Amazon S3 Lifecycle Rules (with S3 Analytics).ai-zh.srt

```
# 🎬 010_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p10 07. Amazon S3 Lifecycle Rules (with S3 Analytics)

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么现在我们来谈谈我们如何将对象在不同存储类之间移动
你可以转换它们
这是它如何可能的图表
正如你所见，你可以从标准
例如到标准A
到智能分层到A
然后从WASA
正如你所见，你可以通过灵活检索或深度归档
所以所有类型的排列组合都在这个图中显示
所以事实上
如果你知道你的物体将被不频繁地访问
那么将它们移动到标准a
如果你知道你将存档对象
将它们移动到冰川的级别或深档案级别
现在移动这些对象可以手动进行
当然 但我们可以使用生命周期规则来自动化这个过程
所以这些生命周期规则由多个部分组成
第一部分是转换操作，以配置对象以转换到另一个存储类别
例如 你说
在创建后六十天内将文件移动到标准文件夹
或者在六十天后将其移动到冰川文件夹进行归档
六个月后
你也可以设置过期操作
因此可以配置对象在过期一段时间后被删除
例如 你访问日志文件
你想要在三百六十五天后删除它们
或者你可以 例如
使用过期操作删除启用版本控制的旧文件版本
或者我们可以使用它来删除不完整的多部分上传
如果 例如 多部分上传超过两周
因为，东西应该现在已经完全上传了
规则也可以指定特定前缀
所以它们可以应用于整个桶或您桶中的特定路径
您还可以为特定对象标签指定它
所以，如果你想只为财务部门制定规则，你可以
所以，这里有一些场景
例如 你有一个在e上的申请
C 二 并且它创建了上传至亚马逊的个人资料照片后的缩略图
但这些缩略图可以很容易地从原始照片重新创建
而且他们只需要保存60天
但原始图像应该能够在接下来的60天内立即检索
用户可以在接下来的6小时内等待
你会如何设计这个呢
这是考试中你会遇到的问题
所以这三张源图片可以存储在标准存储类别中
通过生命周期配置在60天后将它们转换为冰川存储
至于缩略图
这就是你如何使用前缀来区分源图片和缩略图
例如 缩略图可以存储在一个区域中
因为它们不经常访问
并且可以很容易地重新创建
你可以通过生命周期配置在60天后过期或删除它们
另一个场景
公司规定你应该能够在30天内立即恢复已删除的
S3对象
尽管这可能很少见，但在接下来的365天内
删除的对象应该在48小时内可恢复
为此 我们可以启用S3版本控制以保留和保存对象版本
这样删除的对象实际上会被删除标记隐藏
然后可以恢复
然后你会创建一个规则将非当前版本的对象
转换为标准ISO，这意味着不是顶级版本的对象
然后这些非当前对象会被转换为冰川深档案以进行归档
莱斯利 我们如何确定
从一个类别转换到另一个类别的最佳天数
你可以通过亚马逊S3分析来做到这一点
它会给你推荐标准和标准A
它不适用于A或冰川
所以S3存储桶
会在上面运行S3分析
这会生成一个CSV报告，会给你一些推荐和一些统计数据
报告会每天更新
然后可能需要2
到48小时才开始看到数据分析
这是一个很好的第一步
CSV报告可以帮助你制定合理的生命周期规则或者改进它们
好的 这就是这节课的内容 希望你喜欢 我会在下节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/011_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p11 08. Amazon S3 Lifecycle Rules - Hands On.ai-zh.srt

```
# 🎬 011_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p11 08. Amazon S3 Lifecycle Rules - Hands On

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么我们开始为我们的桶创建一个生命周期规则
那么我们在管理下创建一个生命周期规则
这个将被称为演示规则
我们将其应用于桶中的所有对象
我承认
好的 正如你所看到的
我们有五种不同的操作规则
我们可以将对象当前版本之间的存储类别移动
非当前版本的对象
在特定类别之间移动 过期当前版本的永久
删除非转换对象
最后删除过期对象
删除标记或不完整的多部分上传
所以五种不同的使用案例
让我们一个接一个地看它们
所以将当前版本对象移动到存储类别
这意味着你有一个版本桶
当前版本是最新版本
用户显示的版本
例如 我们可以在30天后过渡到标准
然后我们可以在60天后进入智能
然后我们可以在90天后进入说冰川以供即时检索
然后在180天后灵活检索
然后也许在365天后的归档
你可以有尽可能多的过渡
好的 我们需要把这个带回来承认我们做了什么
我们也可以 例如
嗯移动非当前版本更快
所以这一个我们要移动一个非当前版本
因此一个被新者覆盖的对象
被引用
所以我们可以说 好的
这个我们要移动它到冰川灵活
因为我们知道在90天后我们不需要它来检索
所以这完美了我们就准备好了
但我们可以添加更多的过渡
例如我们希望过期当前版本的对象后和底部
您可以设置在700天后和相同的非当前选项
我们希望永久删除他们在700天后
好的 所以这是我们可以做的
现在我们可以看所有这些过渡和迁移操作
所以这很好 因为它显示了你当前版本和非当前版本的时间线
当前版本的对象
如果我们对这一切都满意
我们可以继续创建这个角色
这个角色会在后台执行它应该执行的任务
就是这样了
你知道如何自动化在亚马逊S3中不同存储类别之间的对象移动
我希望你喜欢它 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/012_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p12 09. Amazon S3 - Bucket Policy.ai-zh.srt

```
# 🎬 012_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p12 09. Amazon S3 - Bucket Policy

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈亚马逊S3的安全性
第一部分是用户基础
作为用户 你可以有策略
你和这个策略将授权特定的IAM用户允许哪些API调用
你也可以有基于资源的安全
所以这是新的
我们可以使用称为S3桶策略的东西
并且您可以直接从S3控制台分配桶规则
这将允许
例如，允许特定用户访问
或允许来自另一个账户的用户访问
这被称为跨账户访问您的s 3桶
这也是我们如何使s 3桶公开的方式
正如我会在几分钟内向你展示的那样，您有对象访问控制列表或acl
他们的更精细的安全性
并且他们可以被禁用
如果您需要到桶级别
您可以有桶 acl
这要少得多
也可以禁用
目前保护亚马逊s三号的最常见方式
是使用桶策略
那么i在原则上可以访问s三号对象在什么情况下呢
如果i am权限允许
或者资源策略允许
并且没有明确的拒绝操作
那么i在原则上可以在指定的api调用中访问s三号对象
所以我们将在一分钟内看到这些用例
最后，另一种在亚马逊S3上实现安全的方式
是使用加密密钥对对象进行加密
所以，S3桶策略实际上看起来是什么样的
因为这是我们关注的S3安全的焦点
所以它们是基于JSON的策略
它们看起来像这样
所以这是一个JSON文档
而且很容易阅读
首先，你有一个资源块
资源告诉策略
这个策略适用于哪些桶和对象
在这里我们可以看到，这个策略适用于example桶中的每个对象
这就是星号的作用
我们有效果，允许或拒绝
那么我们允许或拒绝什么
我们拒绝操作
所以我们有一组api
我们可以选择允许或拒绝
在这个例子中，我们允许的操作是获取对象
所以这允许任何人，多亏了主体
校长介绍账户
或用户应用到政策到原则是星
所以这里我们允许任何有星的人获取对象
或从我的示例桶中检索对象星
这意味着其中的任何对象
因此，这个s三个桶设置所有对象内的公共读取
所以我们可以使用必要的桶策略来授予桶公共访问权限
如右图所示
或强制对象在上传时加密
或授予另一个账户访问权限
那么让我们看看当前的情况
这里有一个公共访问的桶策略
我们有一个用户
他在万维网上
他是一个网站访客
他想访问三个桶中的文件
我们将添加一个允许公共访问的S3桶策略
这就是你在前一个幻灯片中看到的策略
一旦这个桶策略被附加到桶中
那么我们就可以访问其中的任何对象
这就是我们在手机上看到的
但另一种方法是，如果您账户中有一个用户
这是一个IAM用户，如果这个用户想访问亚马逊
我们可以通过策略为该用户分配IAM权限
因此，由于策略允许访问S3桶
那么用户现在可以访问S3桶
如果我们有一个EC two实例
并且想要从EC two实例访问S3桶
我们已经看到IAM用户并不适合
这种情况
我们需要使用我是规则
所以我们创建一个e c
两个实例角色具有正确的我是权限
并且这个e c 两个实例将能够访问亚马逊
S三号桶更先进的
如果我们想要允许跨账户访问
那么我们必须使用桶策略
所以我们有一个IAM用户在另一个AWS账户中
并且我们创建一个S3桶策略，允许该特定IAM用户进行跨账户访问
因此，用户将能够对我们的S3存储桶进行API调用
您需要了解的其他安全设置是存储桶设置中的阻止公共访问
这就是我们设置为开启的
当我们创建名为tes的存储桶时
这些设置是由AWS发明的，作为额外的安全层，以预防公司数据泄露
即使您设置了一个使存储桶公开的S3桶策略
如果这些设置已启用
存储桶永远不会公开
这是为了防止数据泄露
如果您知道您的存储桶永远不会公开
然后保持这些设置不变
这样你就能有足够的安全措施来防止那些设置错误的S3存储桶策略的人
如果你知道你的S3存储桶永远不会是公开的
那么你可以在账户级别设置
好的 好了，关于安全的部分就到这里 让我们进入实践环节
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/013_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p13 10. Amazon S3 - Bucket Policy Hands On.ai-zh.srt

```
# 🎬 013_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p13 10. Amazon S3 - Bucket Policy Hands On

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么我们开始制定一个桶策略
以便我们可以通过公共URL访问这个咖啡文件
为了做到这一点，让我们在权限选项下
首先我们需要做的是允许桶设置中的公共访问
因为目前所有内容都被阻止
我们编辑这个
我们将取消这个
因此我们将允许公共访问
但是再次强调，只有在你知道你想设置一个公共桶策略时才禁用它
并且只有在你知道你想设置一个公共桶策略时才启用它
所以这是一项危险的行动
所以我们说同意
因为当然 如果你设置数据
你公司的真实数据在一个s三存储桶上
并且你使其公开 你将有数据泄露
而这永远不可能是好的
所以现在在权限概述下
对象的访问可以是公开的
这就是第一步
我们向下滚动并查看桶策略
目前我们没有任何策略，我们希望创建一个
这就是我们使我们整个桶公开的地方
你可以做的第一件事是查看策略示例
这是文档，它会在右侧显示许多用例
这将显示适当的和相应的桶策略
但我们将使用策略生成器
这里是aws策略生成器
我们将创建一个s三桶策略
那么我们选择正确的类型将允许
然后主将是要去的
因为我们想让亚马逊s三服务的任何人执行
因为我们将在我们的桶中读取对象
我们希望执行获取对象
所以这是我们想要允许获取对象
并且亚马逊资源名称必须是桶名与斜杠
然后与一个星号 那么我们先看看
所以回到我们的三个桶
我们在这里放了一个桶
这里的亚马逊资源名称
所以我们复制它
我们把它粘贴到亚马逊资源名称中
但这还没有结束
我们添加一个斜杠
然后我们添加一个星号
我们之所以这样做是因为这个操作
这里的获取对象操作适用于您桶中的对象
因此，您桶中的对象在斜杠之后
并且开始代表这些对象
让我们添加这些声明
然后生成这个策略
而这个策略就是我们复制到这里的
这是一个公共的s3策略
这意味着任何人都可以从这个桶的任何对象中获取对象
好的 很好
让我们保存这些更改
这里有一个空格
那么我们删除这个完美的
保存这些更改
现在它起作用了 所以我们可以看到，我们的桶策略已经正确应用
所以现在让我们进入我们的对象咖啡
这个jpeg
让我们找到我们的对象URL
就在这里 我们复制它并输入它
正如你所看到的，我的咖啡图片完全可见
而且它也是公开的，就像我在亚马逊上的其他对象一样
三个桶 这就是这节课的内容
我们已经看到了 嗯
桶策略 我们已经看到了策略生成器
现在我们的图像是公开的
我希望你喜欢它 我会在下节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/014_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p14 11. Amazon S3 - Encryption.ai-zh.srt

```
# 🎬 014_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p14 11. Amazon S3 - Encryption

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈在亚马逊云存储中的对象加密
您可以使用以下四种方法之一，在S3桶中对象进行加密
第一种是服务器端加密SSC
您有多种口味
所以您有SSC S3
这是使用亚马逊S3管理密钥的服务器端加密
并且默认情况下为您的桶和对象启用
然后我们有SSC kms，我们用KMS密钥来管理加密密钥
然后是sscc使用客户提供的密钥
然后我们有sscc使用客户提供的密钥
所以这次我们提供自己的加密密钥，不用担心
我们将在下一个幻灯片中详细看到这些。
所以这只是一个概述
然后我们有客户端加密
当我们想在客户端进行所有内容的加密
然后上传到亚马逊是免费的
所以在考试中
理解哪些适用于哪种情况是很重要的
那么，让我们深入探讨所有这些，并理解它们的具体细节
所以第一个是亚马逊的s3 for ssc的s3加密
所以这种情况下，加密使用的是由aws处理的密钥
由aws管理并拥有
你从未接触过这个密钥
对象将由aws在服务器端加密
加密的安全类型是aes two five six
因此你必须设置头为x和z
服务器端加密as two five six
请求亚马逊免费为你加密对象
使用ssc s three机制
现在ssc s three是默认启用的，适用于新桶和新对象
那么这是怎么工作的，我们拥有免费的亚马逊
和我们的用户
您的用户
您将上传文件并带有正确的标头
然后它将成为亚马逊三下的一个对象
三会与s三拥有的密钥配对
好的 因为我们使用的是ssc s三机制
然后我们将通过混合密钥和对象进行加密
这将存储在你的s三桶中
这就是比较简单的 ssc s three
然后我们有 sskms
这次 而不是依赖由 aws 和 ds 免费服务拥有的密钥
您想使用 kms 服务自己管理自己的密钥
密钥管理服务
使用 kms 的优点是您对密钥有控制权
因此您可以在 kms 中自己创建密钥
您可以使用 cloudtrail 编辑密钥使用
因此每当有人在 kms 中使用密钥
这将是一个处理aws中发生的所有事情的服务
称为云追踪
因此，我们需要一个名为x-z服务器的标题
进行aws kms服务端加密
然后对象将被加密
所以我看到的任何东西
当然，它是服务端的
那么这是如何工作的？再次
我们上传对象，这次带有不同的标题
在标题中，我们实际上指定我们要使用的kms密钥
然后对象将出现在亚马逊s3中
这次 将使用的kms密钥将来自aws kms
这两者将结合在一起
然后你将得到加密
文件将最终出现在s3桶中
因此，现在从estuary buquette读取
你不仅需要访问对象本身
还需要访问用于加密此对象的底层kms密钥
因此，这是另一层安全
因此，ssc kms有一些限制
因为，现在你从亚马逊s3上传和下载文件
你需要利用kms密钥，kms密钥有自己的api
例如 生成数据密钥
当你解密时 你将使用解密api
因此，你将向kms服务发出api调用
并且每个api调用都将计入kms
每秒api调用配额
根据区域
您将拥有5000到30000个请求每秒
尽管它们可以通过服务配额控制台进行增加
因此，如果您有一个非常高的吞吐量
嗯，
s3桶并且所有内容都使用kms密钥加密
您可能会进入一种限速使用案例
所以这就是考试可能会测试你的地方
我们有sse c类型的加密
因此，这次密钥将由aws管理外部
但它仍然提供服务端加密
因为我们将密钥发送到aws
但是，亚马逊是免费的
永远不会存储您提供的加密密钥
它们将被丢弃
因此，由于我们将密钥发送到亚马逊s3
我们必须使用https
并且我们必须将密钥作为http标头传递
因此，它是如何工作的
用户将上传文件以及密钥
但是用户在aws外部管理密钥
然后亚马逊就免费了
我们将使用客户提供的密钥和对象进行一些加密
然后将文件加密后放入s3桶中
当然，要读取该文件
用户必须再次提供用于加密该文件的密钥
最后我们有客户端加密
如果我们利用一些客户端库，这将更容易实现
例如客户端加密库
并且 客户端加密的想法是客户端必须自己加密数据
在将数据发送到亚马逊s3之前
你也可以从亚马逊s3中检索数据
然后在客户端外部进行数据的解密
因此客户端完全管理密钥和加密周期
那么这是如何工作的
我们有一个文件
并且我们有一个在aws外部的客户端密钥
客户端本身将提供并执行加密
所以现在我们有一个加密的文件
可以发送到亚马逊s3进行上传
我们已经看到了对象加密的所有级别
但现在让我们谈谈传输加密
传输加密或飞行加密也称为ssl或tls
因此你的亚马逊s3桶有两个端点
http端点没有加密
和http端点有飞行加密
所以每当你访问一个网站并看到绿色锁定或锁定
通常这意味着它使用飞行加密
这意味着您与目标服务器之间的连接是安全的并且完全加密
因此当你使用亚马逊s3时
强烈建议使用https以安全传输数据
当然 如果你使用ssc c类型的机制
你必须使用https
现在这不是现实生活中需要担心的事情
因为嗯大多数客户将默认使用http端点
那么你如何强迫传输加密
我们可以使用桶策略
所以你为你的s3桶附加一个桶策略
并且附加这个声明
这意味着你拒绝任何get object操作
如果条件是aws secure transport false
所以secure transport将在使用https时为真
在未使用加密且加密连接时不为真
因此任何尝试在你的桶中使用http的用户将被阻止
但使用https的用户可能会被允许
好的 这就是加密的全部内容
我希望你喜欢它 我会在下一堂课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/015_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p15 12. Amazon S3 - Encryption Hands On.ai-zh.srt

```
# 🎬 015_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p15 12. Amazon S3 - Encryption Hands On

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么我们来练习加密
为此我将创建一个名为demo encryption的桶
Stefan v2并滚动向下
我们将保留此设置
我们将启用桶版本控制
然后在默认加密下
如我们所见 我们有三种不同的选择
但我们必须为我们的桶选择一个默认加密
所以我将现在查看ssc s3
稍后将查看ssc kms和d ssc kms
所以我们点击创建桶
现在我们已经创建了一个默认加密已启用的桶
所以我们通过实际上传对象来验证这一点
我们需要添加一个文件
我们将添加一个咖啡jpeg文件
然后点击上传
如您所见
我可以点击此文件并滚动向下
查找服务器端加密设置
确实文件已用服务器端加密进行加密
使用亚马逊s3管理密钥
ssc s3
我们也可以为文件编辑加密机制
所以我可以点击编辑
如您所见如果我们编辑服务器端加密
这将创建一个新版本的对象带有更新的设置
因此这就是我为什么为我的桶启用了版本控制
是为了向您展示文件的新版本将被创建
让我们更改加密
为此我们将覆盖桶设置以更改默认加密
对于此文件 我们可以选择使用ssc kms或dsc kms
dcc kms
我不会花太多时间
这是对kms的两级加密
这只是更强大的kms
我们将使用kms
它更简单而且不会让我们花钱
所以我们将使用ssc kms如我们所学
然后我们必须指定一个kms密钥
我们可以输入kms密钥arn
或者我们可以从您自己的kms密钥中选择
如果我们选择kms密钥
您应该只有一个密钥
可用的aws/s3密钥
它被称为s3服务的默认kms密钥
如果我们点击它
我们可以使用这个密钥
而且这不会花我们一分钱
因为这就是服务的默认密钥
如果你创建了自己的kms密钥
那么它就会出现在这列表中
但如果你自己创建
kms会每月收取一些费用
所以为了这次目的，我们就会使用默认的
地址 斜杠s3 kms密钥
好的 那么让我们 uh
保存更改
我们关闭这个
现在我们转到版本
如我们所见 我们现在有两个文件的版本可用
因此，当前版本
如果我们向下滚动并转到服务器端加密
如我们所见，它确实使用ssc kms加密
使用这个加密密钥
这对应于默认的
AWS S3 KMS密钥
好的 所以这真的很好接下来
我们在这一部分继续
所以我们可以通过上传文件进行相同的过程
然后我们将添加一个文件
例如 海滩jpeg
在属性下我们将找到服务器端加密
在这里我们可以指定一个加密密钥
可以使用默认的加密机制
或者用cs3 ssc kms或dsc kms覆盖它
这是实现它的一个方式
最后，让我们看一下默认的加密属性
让我们向下滚动
我们会找到默认加密
编辑这个
在这里我们有一个选项
我们可以启用ssc s3 ssc kms作为默认加密
所以如果我们做ssc kms
我们有可用的桶密钥选项
这样可以减少通过调用aws kms减少成本
这是由默认启用的
如果我们只使用ssc三
那么这个设置就不起作用
我们看到我们可以改变默认加密
你可能会问我
sec不见了
确实 它缺失了 因为sscc你只能在命令行界面做，而不是在控制台
这意味着你不能在这里启用sscc
嗯 你必须在客户端进行加密
然后将其上传到aws
然后在客户端解密
所以你不需要告诉aws
数据是客户端加密的
所以 因此，你在控制台中唯一可以处理的选项是s c s three
Ssc kms和dsc kms
就是这样 我们已经看到了aws中的所有加密选项
我希望你喜欢它 我会在下一节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/016_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p16 13. Amazon S3 - Default Encryption.ai-zh.srt

```
# 🎬 016_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p16 13. Amazon S3 - Default Encryption

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以，关于默认加密与桶策略的简短讲座
因此，现在所有桶都默认使用ssc s三进行加密
因此它会自动应用到新的对象，指向新的空桶
但你可以更改这个成为不同的默认加密
例如km
尽管如此 你也可以通过使用桶策略来拒绝任何API调用，强制加密。
为了将s添加到三个对象中而没有正确的加密头文件
因此，例如ssc
公里每小时或秒速厘米
所以这就是这种啊
但是但是桶策略你可以有在
例如这个说的是嘿
如果你做一个put object
但是你没有加密标头说aws kms
然后拒绝这个请求
或者嘿 如果你正在上传这个
但是客户侧算法没有
所以没有sscc
然后否认这个对象
所以这只是一个例子
但是至少 你可以看到，一个桶策略
也可以强制一种方式在你的桶中实现加密
顺便说一下
桶策略总是要在你的默认加密设置之前被评估
这就是全部 只需记住，默认加密是默认开启的，使用ssc s ray
但你可以更改它 你可以为它设置一个桶策略
提前为想要加密的对象启用强制加密
就是这样 希望你喜欢 下次课再见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/017_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p17 14. Amazon S3 - VPC Endpoints.ai-zh.srt

```
# 🎬 017_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p17 14. Amazon S3 - VPC Endpoints

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以这里有一个简短的理论讲座
但这对你理解VPC端点网关的工作方式很重要
对于亚马逊三
所以默认情况下，srubucket存储在AWS云中
好的 但要访问它
你需要通过公共互联网
这意味着你启动的实例在公共子网中
例如，将通过互联网网关访问
你的桶的公共端点
文件将这样传输
这意味着你的S3桶可以有一个桶策略
你可以通过源IP过滤为你的EC公共IP
两实例 现在
如果你想对你的S3桶进行私有访问
你不想让流量通过公共互联网传输
然后你部署你的实例
例如，在私有子网中
并创建一个VPC端点网关
所以VPC端点网关
允许你从实例直接建立与S3桶的私有连接
这允许你创建不同的桶策略
这个 你可以应用到桶策略
强制通过VPC端点网关访问
你可以使用桶策略by AOS源VPC
你可以指定一个或多个端点
或者你可以指定一个桶策略
并指定一个源VPC条件
涵盖所有预定义VPC中可能的VPC端点
就是这样，只是为了向你展示两种不同的方式
显然，VPC端点网关从安全方面来说会更受欢迎
而且我认为从成本方面来说也会更好
这就是这次讲座的内容 希望你喜欢 下次再见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/018_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p18 15. Amazon Kinesis Data Streams.ai-zh.srt

```
# 🎬 018_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p18 15. Amazon Kinesis Data Streams

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈Kinesis数据流
它是一个用于实时收集和存储训练数据的服务
在考试中你需要特别注意的关键词是实时
这里有一个图表来解释得更清楚
我们有实时数据
什么是实时数据
嗯，它是指在创建和使用时的数据
例如 每当用户点击我们的网站时
这叫点击流
或者当你有一个连接到互联网的设备时
例如，连接到自行车
或者当你在服务器上有指标和日志时
并且你想直接使用它们
所以你想把这些发送到亚马逊
你可以看到数据流吗？
我们必须使用所谓的生产者
生产者可以是应用程序
实际上你需要编写的代码
从你的网站或设备中获取数据并将其发送到
你可以看到数据流
或者 例如，对于指标和日志
你可以在你的服务器上安装称为Kinesis代理的东西
它也会作为Kinesis数据流的生产者
然后你的数据将实时发送到Kinesis数据流
就像它发生的那样
我们做到这一点的原因是我们希望有消费应用程序
在实时消费并利用这些数据
我们有我们的应用程序
再次，你应该编写一些代码来读取
你可以看到数据流
或者你可以有Lambda函数
我也可以读取它
你有称为Amazon数据火线的东西
我们将在以后的讲座中看到
以及
例如 在您的流上进行分析的服务
例如，Apache Flink管理的服务
Kdata流的一些特性
您的数据可以在流中保留长达365天
由于数据被持久化
您可以由消费者重新处理和重放数据
这意味着一旦你将数据发送到Ksee数据流
你不能删除它
你必须等待从时间角度来看它过期
以便它被删除
现在，最多1MB的数据可以发送到数据流
但是真实情况 实际的典型用例是拥有大量的实时数据
如果你的数据点使用相同的分区ID发送，数据将保持顺序
这是你说这两个数据点在时间上是相关的一种方式
通过共享相同的分区ID
您还具有安全功能
例如，静态KMS加密和传输中HTTPS加密
因此，如果您想编写一个优化的生产者应用程序
旨在实现高吞吐量
您应该使用Kinesis生产库KPL来编写优化的消费者应用程序
您应该使用Kinesis客户端库或KCL
好的 现在你可以看到流有两个容量模式
第一个是预配置模式
你可以选择你的流中的分片数量
那么什么是分片
分片是
基本上你的流有多大
所以你可以有一个分片
但你可以有一千个分片
如果你想要更多的分片
你将会有更多的吞吐量
这意味着你将获得每秒1MB或每秒1000条记录
每个分片在容量上
每个分片在读容量上
至于输出流量是每秒2MB
如果你想要发送
例如 每秒1万条记录或每秒10MB
那么你需要扩展到10个分片
如果你想随着时间的推移调整分片的数量
你可以手动调整分片的数量来增加或减少
当然，你需要能够监控你的处理量
以便知道在任何时间点你需要拥有多少分片
你将为每个小时预置的分片支付费用
另一种模式是min模式
在这种情况下，你不需要预置或管理你的能力
你可以看到数据流
你有一个默认的容量预置
大约是每秒4000条记录或4兆字节
然后自动地
你可以看到流是根据过去30天的观察吞吐量进行缩放的
所以这里你将按每小时的流支付
你将按流入和流出的数据量计费
你可以看到数据流，好的
就是这样 希望你喜欢 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/019_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p19 16. Amazon Data Firehose.ai-zh.srt

```
# 🎬 019_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p19 16. Amazon Data Firehose

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈亚马逊数据流
亚马逊数据流是一个将数据从源发送到目标目的地的服务
要将数据发送到亚马逊数据流
你可能需要生产者
它们可以是你的应用程序
你的客户
或者是你编写的东西
然后你使用SDK将数据发送到数据流
或者是kinesis代理
以及数据流本身
它还具有直接从某些服务拉取的能力
例如
如kdata流或亚马逊云监控日志和事件
或者 例如 AWS
物联网 所有这些服务要么是将数据推送到亚马逊数据流
或者是数据流从例如
一个数据流 它将接收记录
现在这些记录将被接收
然后它们可以可选择地使用lambda函数进行转换
如果你想进行一些数据转换或格式
例如
它们将被积累到一个缓冲区 缓冲区将定期刷新以进行批量写入
将数据写入许多目的地
第一个目的地是AWS特定的目的地，如将数据发送到亚马逊S3或发送到亚马逊红移以进行分析
或者发送到亚马逊开放搜索
你还可以选择直接将数据流发送到第三方合作伙伴目的地
如datadog
Splunk
New Relic和MongoDB
如果你的目的地不受支持
你可以使用HTTP端点集成直接将其发送到任何你想去的地方 现在数据流将写入这些数据
但你也可以选择只写入所有或已发送的失败数据到一个S3存储桶作为备份
亚马逊数据流曾经是称为
你可以看到数据流火，很长一段时间
但现在它被称为亚马逊数据流火，因为它能做的远远不止
它可以是一个完全管理的服务
再次支持红移
S3和亚马逊开放服务
第三方目的地如Plank和自定义HTTP端点
亚马逊数据流火曾经是称为
你可以看到数据流火，很长一段时间
但现在它被称为亚马逊数据流火，因为它能做的远远不止
它是一个完全管理的服务，支持红移，S3和亚马逊开放服务，第三方目的地如Plank和自定义HTTP端点
如果你希望编写自己的集成
您还拥有自动扩展功能
它是完全无服务器的
您只需为服务内使用的付费
它被称为接近实时服务
这在考试中需要你注意
接近实时通常指向亚马逊数据
Fire homes 为什么
因为你在火炬中有一个缓冲区
并且缓冲区可以禁用
但它通常被使用并且基于大小或时间具有功能
所以它会积累
然后过一段时间它会被刷新到你的目的地
因此因为它需要一点时间
它是在输入数据类型上接近实时的服务
它支持CSV
JSON Parquet
Avro 文本或二进制数据
并且从火炬中
您可以将数据转换为Parquet或RC等数据格式
或者使用gzip或snappy进行压缩
但如果您想要执行自定义转换或数据转换
例如 您可以使用bus Lambda
例如 如果您想将数据从CSV转换为JSON格式
然后再发送到亚马逊S3
所以它通常很有帮助将ksee数据流和亚马逊数据火炬进行比较
所以这里是 你可以看到
数据流是流数据收集服务
您通常需要编写自己的产生者和消费者代码
它将是实时的
并且有两种模式，预配置和按需模式，数据可以存储至多一年
并且您拥有亚马逊数据火炬的重放能力
它是将流数据加载到目标目的地的一种方式
例如亚马逊Redshift
Open search等 它是完全管理的
它是接近实时的 它具有自动扩展功能
没有存储数据和没有重放能力
好的
这就是这节课的内容 我希望你喜欢它 结束 我会在下一堂课再见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/020_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p20 17. Lab 1.1 - Amazon Data Firehose.ai-zh.srt

```
# 🎬 020_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p20 17. Lab 1.1 - Amazon Data Firehose

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 那么我们开始练习使用数据火喉为我们的课程
那么我们创建一个交付流
在这里我们有一个图表，你可以看到火喉是如何工作的
如果你需要复习
但现在你应该知道现在来源方面
我们有三个选项
我们有kc数据流
亚马逊或直接放入
因为我们没有流或msk
我们的输出将更加经济
在目的地方面
我们有这些目的地可供我们使用
但我们将要使用的一个是亚马逊s3
因为它便宜，并且从数据工程的角度来看，我们很容易处理
现在交付流的名称是自动生成
所以我们将保持不变
我们有不同的选择来转换和转换记录
我们可以使用lambda转换源记录
如果我们启用它
我们指定一个lambda函数
并且这个lambda函数会在记录到达时转换记录
想法是你可以用它做任何数据转换，这可能非常有用
另一个是将记录格式转换为
所以你可以启用这个
并且输出可以是apache parquet或apache或sc
所以在考试中你需要了解它们
如果你启用了这两个中的任何一个
你需要引用一个glue区域
一个glue数据库和一个glue表以便glue执行此转换
好的
我将跳过关于clever logs的部分 现在对于s3桶
我已经创建了一个名为aws机器学习课程的桶
stefan v3使用默认设置
那就是我将要使用的
对于新行分隔符
我们可以配置流在每个记录之间添加一个新行
发送到亚马逊是免费的
所以让我们启用它
它将有助于可读性
现在您想要动态分区
您是否想根据分区键进行分区
我们可以启用它
但这将需要我们查看记录
所以我们现在不会启用它
现在对于s3桶前缀
这是我们将定义
如何使用自定义前缀分区我们的数据
所以在实验室一中，我可以使用数据火喉
请确保复制并粘贴这行到你的用户界面中
我们的想法是，我们拥有数据流的名称
然后我们添加一年
这将对应于时间戳的年份格式
然后我们有一个月
然后我们有一个日期 然后我们有一个小时
这将允许数据轻松地被解析并分区到glue服务中
对于s3存储桶的错误输出前缀再次
我们将复制并粘贴这行到buffer中
所以缓冲大小可以在1兆字节到120兆字节之间
八兆字节
缓冲间隔可以在0秒到900秒之间
我们将其设置为10秒
我们可以获得免费的压缩和加密
我们可以将数据记录压缩为任何格式
例如gzip
Snappy Zip或Hadoop兼容的Snappy
我们还可以加密数据记录
要么使用三桶的加密设置，要么强制使用像kms的东西
所以我们将保留所有默认设置
最后我们可以看一下服务器端加密
我们可以查看云观察错误日志以及服务访问
这将为kinesis firehose服务创建一个iam角色
这将允许它将数据发送到亚马逊的三个
如果我们准备好了
让我们创建这个交付流
在这里我们将使用这个demo数据进行测试
这将实际发送一些数据
嗯 比如将股票代码输入到我们的s3桶中
所以我们开始发送演示数据
我们需要让这一直运行至少10秒
以便批处理和交付到我们的目标桶中
所以我们来这里刷新这一页
如你所见，我现在有一个对象
我的股票演示在这里是年份
这里是月份，日期，小时
我们这里有一个数据文件可以下载并在文本编辑器中打开
正如我们所见
我们有两条记录
所以每条记录占一行 多亏了新的行分隔符
每条记录都是相邻的文档
所以我们有更改的数量
价格 股票代号和行业
所有这些数据都将由firehose生成并发送到亚马逊的s3
这很好，这正是我们所需要的
如果我刷新现在
如你所见 我会得到更多的数据文件上传到亚马逊是免费的
因为肯尼剧院的主持人发送记录的时间更多已经过去了
所以我将停止发送演示数据
我们将重用这个，每当我们进入这个动手项目的下一阶段
但没有必要继续将数据上传到亚马逊S3
我们还注意到在这里我们有正确的分区
所以我们有年份
我们有月份
这一天和这一刻
如果我们继续运行这个，稍后
也许我们不会停留在我们的15
但我们的16和如此等等
这将非常有助于根据分区策略导航我们的数据
好的 这就是这次实践的全部内容
我们已经使用了 可以消防水龙
我们将数据发送到亚马逊是免费的
已验证 一切正常工作
所以这很好 我希望你喜欢 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/021_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p21 18. Amazon Managed Service for Apache Flink.ai-zh.srt

```
# 🎬 021_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p21 18. Amazon Managed Service for Apache Flink

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈亚马逊管理的Apache Flink服务
它以前被称为Kinesis数据分析Apache Flink
但自从那时起，它已被重命名为仅管理Apache Flink服务
所以Flink
什么是Flink
它是一个框架，通常使用Java
SQL或Scala语言
因此，它是一个用于实时处理数据流的框架
所以你可以在右边看到亚马逊管理的Apache Flink服务
然后你可以从Kinesis数据流中读取数据
甚至从Amazon MSK中读取数据
这是Apache Kafka
这也是一个用于实时数据流的服务
但它不是AWS服务
它是用于称为Apache Kafka的管理服务
所以，
多亏了亚马逊管理的Apache Flink服务
你可以在AWS的托管集群上运行任何Apache Flink应用程序
这意味着A将为您提供
计算资源 将提供对计算和自动缩放的访问
AWS将管理您的应用程序备份
这些将被实现为检查点和快照
然后你可以使用任何Apache Flink支持的编程功能来转换您的数据
所以你对流数据进行任何类型的转换都是自由的
正如你所知道的
Flink可以读取Yes
可以看到数据流 但它无法从Amazon Data Firehose中读取数据
这可能是一个考试技巧
好的 这就是你应该知道的关于亚马逊管理的Apache Flink的所有内容
记住它仅用于流处理，用于处理数据流
这就是本讲座的全部内容 希望你喜欢 我将在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/022_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p22 19. Lab 1.2 - Managed Service for Apache Flink (Formely Kinesis Data Analytics).ai-zh.srt

```
# 🎬 022_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p22 19. Lab 1.2 - Managed Service for Apache Flink (Formely Kinesis Data Analytics)

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么让我们来看看管理的Apache Flink
管理的Apache Flink正在取代Kinesis数据分析
现在所有数据流应用程序都直接使用Flink框架完成
因此，我们可以选择开始
无论是流应用程序还是工作室笔记本
第一种是实际运行应用程序
第二种是做交互式查询
所以我现在要做一个流应用程序
请注意，这并不包含在冰箱中
如您所见，有一个can you是处理单元，大约每小时要12美分
加应用存储
加应用备份
以及加任何您正在创建的
所以只是 您知道
如果您只是想知道Flink如何工作而不花费任何金钱
就与我一起看看而不进行实际操作
好的 所以让我们创建一个流处理应用程序
在这里我们有选项要么从头开始创建它
使用一步一步的过程
并且你真的需要写一些代码
或者使用一个蓝图，蓝图会使用云形成来部署一个
你能看到吗
你能看到数据流
然后一个亚马逊管理的Apache Flink应用程序
将数据发送到一个亚马逊免费桶
这样我们就可以见证服务是如何工作的
这更适用于你自己的自定义应用程序
你可以选择引擎版本
然后你选择代码等等
所以现在使用蓝图
因为它要容易得多，我将使用应用程序名称
我将使用演示
然后Fling版本将由UI设置
同样，KC流源将被为我创建
以及S3存储桶目的地
所以让我们部署这个蓝图，幕后
如果我点击这里
这将我带到云形成
正如你所看到的，这个Flink Bootstrap演示堆的创建正在进行中
部署大约需要10分钟
所以让我暂停，直到我们完成
现在，这个演示已经部署完成
如果你查看云形成
已经部署了两个堆栈，其中一些包含我们的Flink应用
其他包含数据源
其他一些包含数据流
所以，一切都按照预期部署完成
所以我们有一个流进入一个flink应用程序
然后这个flink应用程序不会进入亚马逊免费
所以这是我们可以检查的
所以您的flink应用程序的目标代码就在这里
所以如果我点击这个
我将数据传输到亚马逊S3
所以这是我的桶
在我的桶中
如您所见 这里有这个JAR文件
而这个JAR文件就是我们被提供的
这是我们的Flink应用
所以它是以JAR格式存在的
大约有两百兆字节
如果你要在Apache Flink上部署自己的流处理应用
那么你需要创建自己的JAR文件
这就是Flink会使用的
Flink的管理服务会使用它来实际运行你的Flink应用
尽管如此，我们这里可以访问输出数据
所以这就是我的对象
我的数据输出
如你所见，这里有
我有我数据的不同部分
所以它被指定了一个特定的时间戳
如果我点击这些文件之一
例如这个，我想看看里面是什么
我可以点击对象操作
然后使用s3选择查询
所以我们可以快速查询这里这个数据文件
格式是apache parquet
然后输出设置将使用json格式
然后我们可以在我们的文件内部查看数据
所以我们点击运行SQL查询
正如你所看到的，一些数据正在被发送到这个文件中
因此我们有事件时间
我们有报价和价格
这就是我们将要进入这些文件的潮汐
所以自由选择是看东西的好方法
并且查看你的三个桶中的一些文件
所以，归根结底，我的程序现在正在运行
所以我们有了它，还有一些指标
我们可以查看 我们还有快照来获取
备份
如果我们想恢复应用程序的状态配置
所有这些都是由蓝图配置的
所以我们真的不必查看它
但这太高级了，我们看不懂
但我们可以配置很多东西
当然标签和版本
如果你想随着时间的推移部署我们Flink应用的多个版本
所以所有的演示代码都在GitHub上
以便您可以查看这份蓝图并了解它是如何制作的
如果你真的感兴趣使用这个服务
然后我们可以访问Flink仪表板
所以这是Flink仪表板，我将点击它并实际打开我的弹出窗口
所以这是Flink仪表板，实际上是Apache Flink仪表板
您可以访问到正在运行的作业
所以在我的Flink集群中
目前这里有一个作业名称，它已经运行了4分钟
我们有四个作为其中的一部分
我们可以看到我们从数据源读取
并且我们将数据发送到SFTP文件同步
所以将其发送到Amazon S3
我们可以看到
例如，这个接收了10,000条记录并将其发送到Amazon S3
所以这一切都是Apache Flink特定的
但如果您习惯于使用Apache Flink
那么您将在这里找到仪表板以及您需要了解的一切
所以这非常方便
因为它为您提供了这个仪表板
因此您只需要提供您的JAR文件
然后管理Apache Flink将运行您的Apache Flink作业
如果您想要的话，您可以停止您的工作，以便它停止运行
并且您也可以配置它并更改配置以进入更多详细信息
但在这个级别，我们只是想看看管理Apache Flink是如何工作的等等
所以如果我们留下所有东西运行
那么它将花费我们一些钱
即使我们停止它
因为我们有一个Amazon
数据流 所以让我看看它是在哪里创建的
所以如果您进入
Amazon数据流，我们可以看到有一个数据流
Flink演示流
所以这是一个创建的流，它将花费您钱
无论什么 所以
我不想要它
所以现在我将要做的就是操作
然后在我们完成停止这个之后
我将删除它
这现在已经完成
这实际上将删除所相关的数据和Cloud Formation堆栈
所以让我们删除它
然后您还需要进入Cloud Formation并实际删除这个第一个堆栈
在这里 Flink演示堆栈并等待它完成
然后在删除您的第二个堆栈之后
就是这样 那么现在你已经对Apache Flink有了一个介绍 我希望你喜欢它 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/023_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p23 20. Managed Service for Apache Flink and Lambda.ai-zh.srt

```
# 🎬 023_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p23 20. Managed Service for Apache Flink and Lambda

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                因此，我们可以将kinesis数据分析与lambda函数配对
它不仅仅可以直接发送到流
你也可以将其发送到lambda
这给你更多的灵活性用于后处理
我可以将行聚集在一起
我可以将其转换为另一种格式
我可以进一步转换数据并丰富它
我甚至可以加密它 如果我想
我也可以与其他服务和目的地交谈
这使得我不仅可以与如s3和redshift等事物交谈
我也可以与dynamo
db aurora s sqs
cloudwatch 无论lambda可以与什么交谈
这在很大程度上是当今的一切
然而，热管理服务为apache flink
这有着与aws的长期历史
最初这被称为kinesis数据分析为java
然后他们将其更名为kinesis数据分析为apache flink
这就是现在的名称，尽管在底层
kinesis数据分析总是使用apache flink
他们只是在变得更加透明
它正在做的事情 而不是将其呈现为某种独特的工具
他们正在说 我们给你的只是一个为aws集成的开源工具的管理环境
这是大多数aws这些天的趋势
并且不再仅限于java 现在也支持python和scala
顺便说一下，flink只是处理数据流的框架
这就是为什么它一直是kinesis数据分析的底层技术
他们只是不再隐瞒这一点
基本上，apache flink管理服务集成了flink与aws
而不是像早期看到的那样使用sql 你可以从头开发自己的整个
你的整个flink应用程序，并将其加载到apache flink管理服务中
所以，基本上，你从s3加载你的flink应用程序
然后说，嘿，apache flink管理服务
这是我的flink应用程序在s3中，去加载并托管它
你开发了一个flink应用程序，用于处理你的流数据
你加载该应用程序到s3中
然后说，嘿，apache flink管理服务 这是我的flink应用程序在s3中，去加载并托管它
你开发了一个flink应用程序，用于处理你的流数据
你加载该应用程序到s3中
然后说，嘿，apache flink管理服务
这是我的flink应用程序在s3中，去加载并托管它
你开发了一个flink应用程序，用于处理你的流数据
你加载该应用程序到s3中
管理那个服务
这样我就不用
除了用于处理流数据的数据流API之外
还有一个表API用于SQL访问
这就是我为什么认为用于SQL的数据分析可能会被淘汰
因为你可以用Apache Flink做同样的事情
使用那个表API
而不是使用数据流API
这个概念看起来怎么样
哦，是的 它是无服务器的
显然嗯 因为它是被管理的
那无需多说
所以 我们有一个叫做 Flink 的来源
在 AWS 的背景下
那可能是一个 Kinesis 数据流进来
或者它也可能是 Amazon 管理 Apache Kafka 的流进来
如果你有kafka正在进入
作为一个被输送到Apache Flink管理服务的源
我们的实际Flink应用在做某事
在这种情况下，使用数据流API来操作kinesis或msk
再次通过s三加载的实际应用
一旦你的应用程序完成了它需要完成的任务
它可以继续与其他亚马逊服务交谈
这些是flink同步
这可能是三个Kinesis数据流
或者使用Kinesis Data Firehose来输出那个转换的流数据
一些常见的使用案例
流式ETL
你知道 我们已经讨论过了
如果你想做那个数据提取
实时转换和加载
数据接收时
一个Flink应用程序可以为你做到这一点
你也可以用它来生成连续的指标
所以如果我想监控一个数据流
并实时统计事情的进展情况
这可能是它的另一种应用
响应式分析
基本上是同一个想法
所以你知道 如果我想要实时分析谁访问我的网站 或者这可能是实现这一目标的一种方式
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/024_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p24 21. Kinesis Video Streams.ai-zh.srt

```
# 🎬 024_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p24 21. Kinesis Video Streams

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                最后，对于堪萨斯家族中的肯尼苏斯
有肯的视频流
所以我们在这里有k视频流，我们需要生产者
视频流只是发送视频的
生产者是你所期望的
这将是一个监控摄像头
一个佩戴式摄像头
D计划，如弗兰克将向你展示的智能手机摄像头
音频流
图像 雷达数据
或 rtsp 摄像头
这是一种用于发送图像的协议
所以这些都是可以放入 kinesis 视频流的东西
然后惯例是每个视频流一个生产者
如果你有一千个摄像头
那么你会有一千个视频流
有视频播放功能
你可以向你的应用程序和用户展示实时流
消费者可以是很多东西
实际上 它可以是建立你自己的
所以使用mx nets或tensorflow作为机器算法框架
或者你可以是sagemaker和弗兰克会向你展示这个和亚马逊识别视频
弗兰克也会向你展示这个
你可以将数据存储在一个kinesis视频流中，从一小时到十年
所以有很多数据保留的可能性
这显然是你想要做的事情，如果你有安全要求
例如，安全摄像头
你一定想长时间保留安全摄像头的录像，以防出现任何问题
长时间
好的 更深入地探讨一下关于如何使用视频流架构
这来源于一个机器学习博客，该博客介绍了视频流
然后它将实时被消费
由运行在目标上的Docker容器中的应用程序消费
但那也可能是e C Two
这只是一个例子
该应用程序会将流消费的进度保存到 dynamodb 中
因为它是一个消费者
所以它需要确保如果 Docker 容器停止
那么它可以恢复并回到它停止时的消费点
然后所有由应用程序自己解码的帧将被发送到 sagemaker
用于基于公平性的机器学习
现在我们还没有深入研究 sagemaker
但 Frank 将为你做到这一点
但想法是它是一个机器学习服务
你可以从中得到一些结果
使用这些结果
我们将所有推理结果发布到一个我们创建的kinesis数据流中
这可以帮助流被一个lambda函数消费
例如 以实时方式获取通知
因此，采用这种架构
我们能够实时将机器学习算法应用于视频流
并将此转换为实际可操作的数据发送到数据流中
这样，我们的其他应用程序可以消费该流并执行所需操作
例如 通知
这将非常有帮助
例如 如果你想在你的房子里检测到盗贼
或者有人平时不在你的房子里
这可能是您想在aws s中实现的架构
这就是肯尼的视频流
我认为这很简单，进入考试
只需考虑视频流 我将在下次讲座中见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/025_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p25 22. Kinesis ML Summary.ai-zh.srt

```
# 🎬 025_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p25 22. Kinesis ML Summary

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以这是关于kinesis的长章节
但这里有一个快速总结
当你想要创建实时机器学习应用时，你需要使用kinesis数据流
好的 例如
将数据流与sagemaker服务进行实时评估
或者如果你想使用
你可以看到数据火炬
当你想要近实时大量数据时，这将被使用
将其放入s三
然后可能稍后再对算法进行一些训练来训练你的机器学习模型
然后你可以看到数据分析是如果你想做实时的etl提取
转换或加载 或者你想做实时的ml算法
我们已经看到了随机切割森林和数据流中的热点
所以你可以看到数据分析是当我们不想写很多代码时
我们想使用sql
并且我们希望对我们的流应用一些快速简单的算法来做一些分析
最后你可以看到视频流我们拥有一个实时视频流
我们将应用机器学习算法例如检测车牌号
检测面部等以实时创建机器学习应用对抗这些视频流
好的 所以希望
这现在很有道理 进入考试 我会在下次讲座见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/026_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p26 23. Glue Data Catalog & Crawlers.ai-zh.srt

```
# 🎬 026_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p26 23. Glue Data Catalog & Crawlers

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么现在我们来谈谈glue数据目录和glue爬虫
那么glue数据目录是您账户中所有表的元数据仓库
所以它会有所有机构的自动推断
所有这些模式都会被版本化
所以想法是您会有glue数据目录并索引
例如 您在亚马逊s3中的所有数据集
真的很酷的是现在
这个元数据仓库与athena或reo spectrum集成
以进行模式和数据发现
所以想法是，你可以使用Glue数据目录中的schema
并将它们用于你最喜欢的数据仓库或你喜欢的无服务器SQL查询工具
然后我们也会在这节课中看到Glue爬虫
可以帮助你构建Glue数据目录
爬虫会遍历你所有数据表、数据库
以及你的S3桶，并找出其中的数据
所以让我们来谈谈这些爬虫和细节
它们会例如在S3中运行
并推断出所有分区的schema
它对JSON有效
Parquet Csv
关系型数据存储 所以有很多不同的数据类型
并且适用于不同的数据源
所以你可以在亚马逊S3上使用爬虫
亚马逊红移 亚马逊RDS
等等 爬虫可以在预定的日程上运行，或者按需运行
你需要一个IAM角色，将其赋予爬取器以访问数据
存储或凭据
例如 如果它要访问红移或RDS
因此Glue也有一个分区的概念
因此Glue 爬取器将根据您在S3中如何组织数据来提取分区
因此您需要提前考虑如何组织数据
因为基于这一点将定义分区并优化查询
所以 例如，你从一个设备发送数据
这是审查过的数据 它每小时发送一次
你是在询问主要按日期范围查询吗？
那么你首先想知道过去一小时发生了什么数据吗？
适用于我所有设备
如果你这样认为，你可能想组织你的桶。
例如，你有这一年
设备ID之前的月份和日期
但如果你主要是通过设备进行查询
所以你想让用户能够查找他们的设备
然后在时间上进行查找
你可能想要组织你的as免费桶，首先查找设备
然后最后查找年份
月份和日期
所以这给你提供了两种不同的s三存储方案
当你在之后运行查询时，这一点非常重要
好的 所以让我们进入下一讲
我将创建我们的第一个glue数据目录和我们的第一个glue爬虫
以查看在s三中的数据集 所以下一讲见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/027_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p27 24. Lab 1.3 - Glue Data Catalog.ai-zh.srt

```
# 🎬 027_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p27 24. Lab 1.3 - Glue Data Catalog

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们在胶水数据目录中添加一个表格
所以我要打开胶水
然后在左侧的数据目录下
我将找到爬虫
我们要创建一个爬虫
以从我们的桶中提取数据并将其转换为胶水的表格
让我们创建一个爬虫
我将这个命名为demo爬虫
那么我们的数据是否已经映射到胶水的表格
不，接下来我们添加一个数据源
它将是亚马逊s3
不需要特定的网络连接
它在这个账户中
我发现我的机器学习课程桶就在这里
你可以选择桶本身
好的
选择完美 所以现在我们有该桶中所有的对象，这将被爬虫爬取
我们将爬取所有子文件夹
我们将添加一个s3数据源
所以现在我们的爬虫将查看s3桶中的所有内容
这完美
接下来
我们将点击这里
我们需要选择一个iam角色
所以我们需要创建一个iam角色
我将这个命名为demo胶水
爬虫s3，这个角色将具有必要的权限来实际
当然查看亚马逊s3中的内容
如果你看一下iam角色和权限这里
我们看看事实
有一个胶水服务角色权限
允许你做很多事情在胶水c2，iam和s3
当然我们这里还有一个策略
允许我在我的机器学习课程桶中获取和放置对象
这完美
现在我们知道胶水具有正确的权限
我将点击下一步，现在输出配置
我们需要选择一个数据库
我将创建一个数据库
我将其命名为机器学习
这将是它的位置
所以现在这是我的胶水数据目录中的数据库
我刷新一下
我可以选择机器学习课程数据库
对不起，现在关于表名前缀
我不需要输入任何东西
没有最大阈值
没有高级选项
爬取计划将是按需
但我们可以让它定期运行
例如每小时到每月或自定义
所以让我们回顾并创建这个爬虫
所以是的 一切都看起来很好
我们在亚马逊S3上做
我们爬取一切
我们有一个服务角色
目标数据库是机器学习
所以让我们创建这个爬虫
现在它已经被正确创建，我们需要实际运行它
因为它是按需的 所以让我们运行爬虫
现在我们需要等一会儿
所以我的爬虫现在已经完成
如你所见，有一个表变化和一个分区变化
所以添加了表和分区
所以让我们看看
让我们进入我们的数据库机器学习
我点击它
正如你所看到的 我有一张被检测到的桌子
所以我们知道这张桌子的位置
我们知道它的分类是json格式
所以我可以点击它
我们可以看到这张表格的一些信息
看起来我们有数据列名
所以变化 价格
股票代码和行业
然后我们有一些关于分区的信息
我们有一个年份
月份 日期和时间
这非常有用
这是分区一、二、三等等
这是从我的数据中直接推断出来的
我们有年份等于2024
月份等于0
两个 两天等于零点六等等
所以这就是如何推导出来的，太棒了
所以，在这一讲中，我们已经练习了使用爬虫
我们爬取了三个桶
我们创建了数据库和表，自动创建
这就是在aws glue中拥有目录的基本方法
我希望你喜欢它 我会在下一讲见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/028_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p28 25. Glue ETL.ai-zh.srt

```
# 🎬 028_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p28 25. Glue ETL

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 所以最后有glue ETL和glue ETL代表提取
转换和加载
这肯定允许你转换数据
清洗数据和丰富数据在你进行分析或训练机器学习模型之前
所以想法是使用glue ETL
你可以生成ETL代码，要么是Python要么是Scala
你可以直接修改代码
或者你可以提供你自己的Spark或Pi Spark脚本
glue ETL的目标可以是S3
它可以是JDBC用于RDS和Redshift
或者它可以是glue数据目录
例如 glue ETL的真正酷之处在于它是完全管理的
它是成本有效的
你只支付消耗的资源
所以如果你有一些Spark工作
你可以让你的所有工作在无服务器Spark平台上运行
你不需要担心为你的Spark集群进行配置
你不需要担心如何运行你的工作
glue ETL会为你做这些事情
所以我认为在这方面它是一个相当酷的框架和服务
现在你可以使用glue调度程序来安排工作
你也可以使用glue触发器来自动化工作运行
基于正在发生的事情
那么在考试中
你需要记住glue ETL为你提供了一些转换
转换可以打包，例如删除字段或删除空字段
我想名字很明显
它删除字段 或者我们将删除空字段
然后过滤
例如如果你想要指定一个函数来过滤记录
连接以丰富数据映射
以添加字段 删除字段或执行操作查找
然后有一件事或你记住
因为这是一个机器学习考试我们在谈论
有称为机器学习转换的东西
并且目前只有一个
它被称为朋友匹配机器学习
并且这个可以用来识别重复项或匹配记录在你的数据集
即使记录不完全匹配
这是一个非常酷的事情
因为使用fine matches学习转换
你可以在glue ETL中做一些重复
这是glue ETL提供的一部分
你可以最终做一些格式转换
你可以尝试将CSV转换为JSON
Jason Avro Parquet
Or c Xml 等等
您可以使用任何 Apache Spark 转换
例如 K 意味着算法 所以让我们进入 Glue ETL UI 看看事情是如何工作的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/029_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p29 26. Lab 1.4 - Glue ETL.ai-zh.srt

```
# 🎬 029_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p29 26. Lab 1.4 - Glue ETL

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么让我们用胶水etl试试
想法是我们想把这张表
从json格式转换为parquet格式
为此，点击数据集成和etl etl工作
然后我们将使用可视化etl
这是我们将视觉上定义我们的etl作业的地方
首先列出作业名称
所以将ticker demo从json转换为parquet
好的
我们将添加点
数据来自哪里
正如你所看到的，我们有很多不同的选择
这可能是kinesis kafka关系数据库
这是glue数据目录和许多不同的不同来源
但对于我们来说，我们将使用亚马逊s3
然后我们需要添加一个节点
这将是一个转换
我们有很多不同的选择
例如更改模式
我们有一个连接
我们有sql查询
我们可以检测敏感数据
填充缺失值 汇总
正确常量转换等等
我们可以实际做的很多事情来转换我们的数据
这是一款很好的etl工具
但我们想使用的是更改模式
更改模式将允许我们将这些数据连接起来，然后将其更改为json
最后我们需要的是一个目标
目标是亚马逊s3
这是我们将发送我们的数据的地方
首先让我们将这些东西链接在一起
点击数据源底部的圆点
并将其链接到转换更改模式
再次链接它
所以现在这三件事是链接的
我们做步骤一
步骤二，然后步骤三
对于步骤一，我们需要选择s3源类型
因为我们已经爬取了我们的s3位置
我们做了一个目录 我们可以使用一个表
我们选择机器学习数据库和表我们选择这个
这个机器学习课程
好的 所以现在我们配置好了
我们有这个正确配置
然后我们有这一个更改模式
所以父节点是亚马逊s3
我们将获取模式
正如你所看到的，所有内容都已被检测到
所以我们有变化
价格
股票代码
行业 所以我们有两种数据类型是双精度
其余的是字符串
所以我们可以更改如果你愿意
但这是完美的，就是这样
所以我们要保存这个
然后最后我们这里有数据目标
所以它将是亚马逊
S三 在这里我们可以更改格式
所以我们想要它是parquet
我们可以添加或不压缩或snappy
作为do gps或未压缩
所以让我们使用snappy
最后一个
我们需要一个目标位置
所以我将进入我的桶
我将创建一个文件夹
我将其命名为ticker_underscore_demo在这个核心parquet中
这是我们的目标表将位于的地方
所以我将回到我的爬虫
我将浏览s3
让我们寻找机器学习课程
是的 点击它
然后点击股票演示parquet旁边的点，然后点击选择
这将是我们工作的目标位置
从数据目录的角度来看
我们可以选择不更新数据目录或创建数据目录中的表
并自动更新模式并添加新的分区
所以这完美 让我们选择第二个选项
对于数据库，我们将选择机器学习
表名将是股票
下划线演示分数
Parquet Okay
所以现在一切都看起来不错
让我们保存这个
所以我们有一个问题，关于工作详情
让我们点击工作详情
正如你所见，名称已经正确设置
但我们缺少一个iam角色
因此我们需要为这个工作创建一个iam角色
所以让我们创建一个
那么我们进入
我是
我将在滚筒上创建一个角色
这将是一个八服务
这个服务将很好
用例将是允许胶水ETL在我们 behalf 调用服务
所以我们需要两个政策
所以第一个是亚马逊是免费的
全权访问允许访问亚马逊免费读取和写入文件
第二个将是胶水服务角色
允许我们的工作访问胶水
所以我们肯定过度配置我们的角色
但至少我们知道它将足够好我们的工作
这是一个胶水ETL转换为parquet角色
我将只保存这个
这是一个信任政策
这是一个权限摘要
这是正确的
让我们创建这个角色
所以我的角色已经被创建
现在我回到这里
我可以刷新这个并找到我的胶水ETL转换为parquet工作优秀
这是一个火花工作
我有最新的胶水版本
我用python三
我用g one x类型的工人
我将选择两个工人最多尝试优化成本
其他一切看起来不错
是我们的高级设置
让我们保存这个
正如你所看到的，现在已经保存
现在没有更多的问题
这是正确的
我们所需要做的就是实际上运行我们的工作
虽然一切都被配置
点击运行
现在工作已成功启动
所以这一切都是通过视觉完成的
点击运行详情以获取有关此特定运行的信息
当然运行这会花费你一些钱
因为我们正在运行特定的工人类型，这些工人类型在这里
但这对我们练习胶水是必要的
所以让我们等它完成
我的工作现在已经成功
让我们验证它
所以 它持续了大约一分钟二十秒
首先，让我们进入我们的表
正如你所看到的，是的
我有一个表在这里
一个创建的条形图parquet演示
分类是parquet
我们可以查看模式
所以看起来一切正常
并且它在我的s三号桶中
所以如果我去我的三号桶并刷新这个
如你所见
我们有很多parquet文件
它们用snappy压缩
所以它们都在这里
我可以做的事情是
我可以点击这些文件中的一个
我可以做对象操作和s三选择
我选择格式为apache parquet
这是我们的glue作业为我们做的
输出设置为json然后我将运行此sql查询
让我们运行并滚动
正如你所见是的
我们有一些数据从中输出为json格式
并且它已成功返回
这意味着我们现在查询的数据
这个条形图parquet演示确实在apache parquet格式中
否则这将不会起作用
因此我们的glue etl作业是成功的
确实按照我们预期的运行
确实创建了一个名为条形图parquet的表在parquet格式中
这确实展示了glue etl的力量
就是这样 我希望你喜欢它 我将在下一节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/030_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p30 27. Glue Data Brew.ai-zh.srt

```
# 🎬 030_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p30 27. Glue Data Brew

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以这里有一个关于aws glue数据酿造服务的简短介绍
这是一个服务，允许您在不编写任何代码的情况下清洁和规范化数据
您将获得一个用户界面
正如我们在不久的实操中会看到的那样，从这个用户界面
您可以清洁和规范化您的数据
这将减少机器学习和分析所需的数据准备时间高达80%
这很好
您数据的来源可以是亚马逊 免费
红移 奥罗拉
或者Glue数据目录中的任何表
已经有超过250种现成的转换可以帮助您自动化任务
其中包括
例如 过滤异常值
数据转换和纠正无效值
这就是关于简短介绍的所有内容 在接下来的讲座中，我将为您演示一些数据酿造的实操
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/031_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p31 28. Lab 1.5 - Glue Data Brew.ai-zh.srt

```
# 🎬 031_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p31 28. Lab 1.5 - Glue Data Brew

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 让我们看看数据酿造
因此，制作胶水是一种服务
这将使我们能够真正规范化和清理我们的数据
因此，让我们创建一个项目并看看它
所以我会把这个叫做演示项目
并且这是在创建一个新的食谱，而食谱是一份列表
一系列的数据清理步骤
好的 所以我们要创建这个演示产品的食谱
然后我们需要选择一个数据集
目前我们在brew中没有任何数据集
所以我们可以创建一个新的数据集
我将其命名为ticker_demo
总共
这些数据集在哪里找到
我们有不同的连接
当然我们可以这样做
可以直接从亚马逊获取，免费
也可以从redshift获取 Jdbc
它可能正在使用 数据目录连接
这就是我们将要使用的
所以我们将使用此当前账户
现在正在加载我的glue数据库
这花了一些时间
但我已经有了我的数据库
我现在选择我的数据库
好的 所以我们将从这张表中读取数据
我们需要权限
所以需要一个有权限连接到您的数据的角色
我们需要创建一个新的
我是角色 我们设置所以演示
所以它将是 它是glue服务角色演示
让我们创建这个项目
现在布将打开
然后我们会根据我们做的事情来定价
基于我们做的事情 但我们会尝试做一些非常简单的事情
只是为了向你展示数据酿造的力量
所以第一件事是我们可以用丰富的统计来探索数据
所以我们会对一些数字做一些统计
我们也可以清理和归一化数据
使用一些转换
所以我们会看看如何也许可以从数据中提取一些东西
然后我们可以尝试食谱或使用更多的转换步骤
好的，然后我们做完了
我们可以对所有传入的数据运行这些任务来实际转换它
所以这也是一种etl的方式
当然在aws上
我正在得到一个错误
这是因为我的表没有正确设置
所以让我们编辑数据集
我将使用ticker demo parquet文件来连接
这将会更好
让我们完成这个
然后更新我的数据集
所以现在我们有一个完美的数据集
让我们回到数据酿造并进入我们的项目
点击项目
我们将查看在parquet格式下的数据
这是我们的数据酿造ui
如您所见
我们可以看到所有列和一些样本行
目前我们正在采样两行
我们有多个列
我们有变化
价格 股票符号
部门
年和月
天和小时
所以这非常有用
在这里对于变化
例如我们可以查看列统计信息
我们可以查看有效值
无效值
缺失值
我们可以查看值分布
这对你有一些数据并想查看这非常有用
例如获取信息关于最小值
最大值 平均值
标准差
等等独特的值
所以对于数据探索非常好
同样对于价格这里对于股票符号
这很好因为我们得到一些15个独特的
嗯值10个唯一
两个一行总数
例如qa z有我数据的14%
所以我们可以运行这些数据集的直接统计
这已经是一个很好的价值
所有列和唯一值的图表
所以这非常有用
但我们也可以例如更改数据格式
那么我们来处理这个
在这里我可以
例如 嗯，查找缺失数据或查找无效数据
或删除重复项
查找异常值
这非常有用
所以，如果您想清理数据集
查找可能没有意义的行
这非常好
这样将数据转换为表格很容易
你可以进行多种不同的转换
你可以合并
你可以应用函数
你可以应用条件
你可以嵌套一个反转分组
我的意思是，通常这些都需要用代码来做，非常复杂
但在数据转换器中，这些都变得非常简单
多亏了数据转换器
例如，我们可以应用函数
一个数学函数
这里我们希望
例如 取整，也就是大于等于一个数的最大整数
如果我们这样做
好的 我们将应用这个价格
我们将推导出一个新的
一个新的价格 它将只由一个整数组成
所以这只是一个例子
所以我们有一个价格下限列正在创建
让我们应用这个，我们会得到一个预览
现在，正如你在这个价格下限列中看到的那样
我们从14.89开始
现在我们在14和201.05
52到201.05，依此类推
所以我们有效地转换了那列
这可能是我们的数据清理步骤之一
所以这相当空
因此，我们对这个数据集进行的每一种转换
都将添加到一个称为食谱的东西中
所以，食谱是一系列步骤
这是我创建这个列的第一步，使用数学函数
因此，我们进行的所有转换都将在这里的食谱中概述
我们可以 当然，根据我们需要来编辑或删除它们
但这非常有用
因此，如果我们想清理
标准化我们的数据
这将主要由一个食谱组成
但这一切都是通过视觉完成的
这非常有力
当我们完成时
我们可以创建一个工作
这个工作将被分配实际应用转换的任务
应用转换
我们可以定义这个工作
好的 我们可以说我们想要输出到哪里
亚马逊s3完美
嗯
我们可以实际上选择一个
我们可以选择一个格式使其简单
我们可以选择一个格式
我们有csv Parquet
Avro等等
让我们选择一个csv
我们将使用逗号分隔符
不需要压缩
然后我将选择一个输出
所以我将展示这个take her demo
做了一杯啤酒
我将使用这个完美
所以所有我的数据都将由这个工作发送到那里
再次
我们需要有一个服务角色，它有权访问我的数据
好了
我们已经选择了服务角色
现在让我们创建这个工作并运行它
所以现在我们可以看到有一个工作在进行中
我可以点击工作详情以获取其运行状态
所以让我们等一会儿让它完成
好的 所以现在我的java已经成功
我得到了一个输出
所以我可以点击它并被直接传输到亚马逊is free
所以在这里我们有一个文件夹
我们有多个csv文件
所以让我们取其中一个
我将执行对象操作查询与s3
选择格式是csv
它是逗号分隔的
没有压缩
现在运行sql查询
如您所见，我们得到了一些关于我们数据的信息
所以我们要更改价格
但也要更改价格下限
也许这将更容易看到
如果我用json格式，实际上并不容易
让我们选择json，然后按tab
也许看得更清楚 好的
我们开始了 我们这里有价格下限
正如你所见，我们值14
二百零一和四
这个列在我的食谱中被推断为酿造
并在这个etl工作中创建
所以这就是这节课的内容
我们已经运行了我们的数据酿造工作
实际上我们在其上创建了一个食谱来转换列
我希望你喜欢它 我会在下节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/032_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p32 29. Lab 1.6 - Athena.ai-zh.srt

```
# 🎬 032_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p32 29. Lab 1.6 - Athena

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么让我们练习使用亚马逊Athena来立即查询我们的数据
在亚马逊S3中
那么让我们探索查询编辑器
它说在我们运行第一个查询之前
我们需要在亚马逊S3中设置查询结果位置
那么让我们编辑我们的设置
我们将进入我们的存储桶在这里
名称将是这个所以S3机器学习存储桶
斜杠Athena结果
好的 这是我们在Athena中存储所有资源的地方
我不需要做任何事情
那么让我们保存这个 现在我们可以开始了所以在Athena编辑器中
你可以做的是查看AI数据目录
所以Glue数据目录
然后查看我们的机器学习数据库
在这里你可以查看
例如，Ticker Demo Parquet
这个表将被
我们查询了
所以我们可以预览我们的表
实际上这将运行一个选择星从机器学习Ticker Demo Parquet
如你所见 数据正在被读取到Athena
我们得到了变化的信息
价格 股票符号
部门等等
所有这些都在工作
我怀疑因为我的表在Glue中没有完美设置
使用Athena查询它将不会工作
但是让我们看看 所以让我们预览这个表
实际上这也在起作用
所以这是正确的JSON
当然我们得到了相同的数据
但来自不同的表
这是我机器学习的表
所以所有这些都在表明
Athena可以用来在表上运行SQL查询
我们可以使用SQL语言在这里进行更有趣的表查询
至少我们知道我们可以预览数据并轻松操作它
使用Athena
好的 这就是本讲座的内容
我希望你喜欢它很短
但是Frank将详细解释亚马逊Athena 我将在下一个讲座中见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/033_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p33 30. Lab 1 - Cleanup.ai-zh.srt

```
# 🎬 033_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p33 30. Lab 1 - Cleanup

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以你可以清理你的环境
但现在并没有花费你任何金钱
但你可以清理亚马逊雅典娜
但这不花费你任何金钱
所以你不需要做任何事情
真的 嗯
你可以清理你的S3桶，通过删除你S3桶内的所有数据
然后删除你的桶
你也可以删除你的数据库
以确保你不会留下任何痕迹
但再次 工作只是在需求时运行
所以不一定是两
嗯 你可以删除你的
我的角色 当然
然后你需要进入你的Glue界面，在这里你可以
如果你想要删除你的数据库和表格
以及删除你的爬虫
以确保它不会爬取亚马逊S3
但从持续的角度来看
我们已经删除了与Kinesis相关的所有事情
数据流和Flink
这将花费你最多的金钱
所以你应该没问题
在下一讲中，我们将进行更高层次的服务概述，因为我们的手头工作
因为这里是考试要求你仅仅知道概念，而不是需要进行手头实践
所以这就是这节课的内容
我希望你喜欢它 我将在下一节课见到你 结束
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/034_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p34 31. AWS Data Stores in Machine Learning.ai-zh.srt

```
# 🎬 034_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p34 31. AWS Data Stores in Machine Learning

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么在进入机器学习考试时
你将有一些关于数据存储的问题
但它们非常 非常高层次
因此我们不会深入探讨所有这些
只是为了给你一个关于这些的一般概述
所以，它们是 红移是一种数据仓库技术，你需要预先配置它
你可以进行基于SQL的分析
因此它最适合用作OLAP或在线分析处理
所以，每当你需要运行一些大规模并行的SQL查询
以进行一些分析时
那么红移是一个不错的选择
你需要从S3中加载数据到红移中，以便其工作
或者你可以使用称为红移光谱的东西来直接查询S3中的数据
你不需要加载数据
好的 但是红移需要在先进行配置
它是一个完整的大数据库
然后你将在其上运行你的SQL分析
然后我们有RDS或Aurora
这种类似
它们是关系数据库
好的，你也有SQL语言
但这次它是用于OLTP或在线事务处理
区别在于红移是基于列的
所以数据按列组织
而RDS和Aurora是基于行的
数据按行组织
因此这个名字为OLTP
你需要预先配置服务器
RDS可能被使用
例如 如果你想要存储关于你模型的一些数据用于导出
但它不会用于直接进行机器学习
好的 红移将用于分析，而RDS仅用于存储行级别的数据
接下来
我们有DynamoDB DynamoDB是一种NoSQL数据存储
所以NoSQL意味着不是SQL
每当你看到NoSQL时，你需要寻找这个词
你想到它是一个无服务器
所以你不需要预先配置任何服务器实例
你需要只说你需要多少读写容量，以便它能工作
这非常有用，如果你想要存储
例如，一个机器学习模型
并且所有这些都需要由你的应用程序提供服务
好的 所以你不会在DynamoDB上进行任何机器学习
但是你的模型输出
例如 可能被存储到 dynamodb 中
然后我们有三种
这是一个肯定的数据存储
我们已经深入研究过 你可以进行对象存储
它是无服务器 你得到无限的存储
它与大多数 aws 服务集成
它是你在 aws 中进行数据处理的中心
最后，开放搜索
它以前被称为 elasticsearch
它通常用于数据的索引和搜索
这就是为什么它被称为开放搜索
然后也用于点击流分析
但这里没有直接嵌入的开放搜索机器学习
最后，elasticcache
这不是真正的机器学习技术
但它可能会出现在你的问题中
它有一个缓存机制
它并不是用于机器学习的
但是每当你看到 elasticcache 时，就想到缓存
如果没有缓存数据的理由
我缓存数据意味着确保数据可以容易且快速地访问
如果它是热的 嗯
那么想想 elastickeotherwise
否则就不要考虑它 这就是你需要知道的关于你的 aws 机器学习数据存储的所有内容
我将快速浏览这些
因为你不需要深入了解它们
我给你的介绍足以让你通过考试
好的 就是这样 下次再见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/035_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p35 32. AWS Data Pipelines.ai-zh.srt

```
# 🎬 035_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p35 32. AWS Data Pipelines

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们来谈谈aws数据管道
数据管道是一种服务，用于将数据从一个地方移动到另一个地方
所以它是一个etl服务
在考试中，你需要了解它的高层架构
好的 数据管道的流行目的地包括s three rds
Dynamodb Redshift和emr
您使用数据管道来管理任务依赖性
所以它只是一个调度器
实际的ETL不会在数据管道中发生
它会在E发生
C 由数据管道管理的两个实例
它有一些能力
因为它是一个调度器，用于在失败时进行重试和通知
数据源可能在本地
例如 它也非常可用
所以你没有理由让它失败
或者如果你它失败了
那么它会切换到另一个实例
好的 所以这只是一个非常
非常高层次的 但我们来看看一个具体的例子，为什么我们会使用数据管道
好的 所以假设你有一个rds数据库，它包含数据集
并且你想对这个数据集进行一些机器学习
所以你想把它移动到一个s3存储桶中，以便你可以使用
例如 Sagemaker和Frank将讨论Sagemaker
但我们如何将数据从RDS移动到S3
在那里我们将启动一个ADSP（自动数据流水线）
它将创建一个
EC two 实例或多个实例将由数据流水线管理
这些EC two
实例 将被分配从RDS移动数据到S3存储桶的任务
但这不仅仅适用于RDS
它也适用于 例如，对于DynamoDB进行相同的过程
因此，数据管道在这里协调并移动数据从RDS到S3
一直到S3
所以，你可能现在会有个问题，那就是
数据管道和Glue的区别是什么
嗯 Glue具有ETL功能
你在运行基于scala或python的apache spark代码
并专注于etl
你不必担心管理和配置资源
数据目录有助于您将数据提供给其他服务，如athena和redshift spectrum
但对于数据管道，它是一个编排服务
它不会实际上为您运行这些内容
您对环境、运行代码的计算资源以及代码本身有更多控制权
它允许您访问e
C
二实例或emr实例
因为所有资源都创建在您自己账户内 而glue
所有资源都属于
因此，很难获得访问权限
例如，您自己的数据库rds
所以它们有点不同
它们都是etl服务
但glue非常专注于apache
Spark
专注于etl
进行一些转换 而数据管道为您提供更多
控制是一个编排服务，运行在e
C
二或我们的实例中，属于您自己的账户
好的 这就是大致情况
所以，希望这对您有帮助 在下次讲座中见 我希望这有所帮助 我会在下次讲座中见到您
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/036_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p36 33. AWS Batch.ai-zh.srt

```
# 🎬 036_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p36 33. AWS Batch

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                你需要了解的下一个服务在较高层面上是aws批处理
aws批处理允许你运行批处理作业，正如其名所示
它们基于docker镜像构建
因此，使用批处理服务你不需要手动配置实例
实例的动态配置将由批处理服务完成
无论是e C 2实例还是spot实例
批处理服务将根据作业的量确定最佳实例数量和类型
并且您不需要管理任何集群的要求
这是一个完全无服务器的服务
就像胶水一样
您只需为底层支付费用
这两台实例是由批处理本身创建的
您可以使用CloudWatch Events安排批处理作业
以便您可以在预定时间运行
或者您可以使用Step Functions编排批处理作业
在下一节课中我们将会看到
但是您可能会有一个问题，即批处理和Glue之间的区别是什么
再次粘合 再重复一次
它是Apache Spark代码
只使用Scala或Python
它专注于ETL
你不需要担心配置、管理资源
现在有批处理的数据目录
它有点不同 它是任何计算任务的
无论工作
好的 这不仅适用于etl
这适用于任何批处理导向的
只要你有一个适合批处理的docker镜像
你可以在批处理中进行
例如 适合批处理的工作
将是清理任务
然后是一个空的桶 所以你在免费桶上运行一个脚本来清理它
你定期运行这个批处理作业
而在glue中
它真是关于处理数据
转换它 然后将其放在其他地方
所以资源在你的账户中创建
并由批处理管理
所以你在使用e C 账户中的两个实例
所以你可以在账户中访问所有这些内容
如果你需要进行数据转换，实际上需要ETL
那么Glue可能是一个更好的选择
但如果你的工作与非ETL相关
但它仍然是批处理导向的
那么AWS批处理会更好
这就是关于这个服务的所有内容 希望这对你有帮助 那么下次讲座再见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/037_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p37 34. AWS DMS - Database Migration Services.ai-zh.srt

```
# 🎬 037_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p37 34. AWS DMS - Database Migration Services

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们再来一次，对另一个服务的高级概述
这就是数据库迁移服务
这是一种快速安全的将数据库迁移到aws的方法
它具有弹性和自我恢复功能，源数据库在迁移过程中保持可用
并且它是连续的
它支持某些种类的同构迁移
例如 从本地的Oracle数据库迁移到aws的Oracle数据库
但也支持某些异构迁移
例如 如果你有一个本地的sql服务器，并且你迁移到一个
这是一个持续的数据复制，使用cdc或更改数据捕获
你必须创建一个e
C 两个实例来执行复制任务
它的工作方式是源数据库在某处
可能在账户上或者在本地
你有e C 两个实例
他们将运行数据库迁移服务，所以DMS
然后它会知道如何将数据从源数据库移动到目标数据库
所以它是一个非常简单的服务
正如名字所表明的 它是数据库迁移服务，只有数据库迁移服务
所以当我们使用GMS时
当我们再次使用Glue时，Glue
我们已经知道这一点 但这是Spark上的ETL
在使用Glue ETL时，您不需要配置或管理资源
数据目录帮助您使数据源对其他服务可用
而DMS是连续数据复制
因此，Glue是批处理定向的
好的 顺便说一下，您最多可以设置Glue的5分钟时间
ETL
而GMS绝对是实时的
它是实时的 DMS不执行任何类型的数据转换
它实际上将一个数据库的数据复制到另一个数据库
所以，如果你有etl需求
那么你需要在之后进行处理
一旦数据进入aws
你可以使用glue来转换数据
这是非常常见的
例如，使用dms将本地数据库复制到云中
然后使用glue etl从该数据库获取数据
并在将其放在其他地方之前对其进行etl处理
例如，将其放入s3中
因此，这两项服务是互补的
但是记住，dms是用于数据库迁移的，而glue是用于纯ETL的
好的 嗯 这就是这节课的内容 我希望你喜欢它 我会在下节课见到你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/038_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p38 35. AWS Step Functions.ai-zh.srt

```
# 🎬 038_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p38 35. AWS Step Functions

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                因此，步骤函数在这里负责编排和设计工作流程
它为您提供直观的可视化
在下一页中，我们将看到一些
它为您提供高级的错误处理和重试机制，超出代码之外
想法是您将定义一个工作流程
例如 这样做然后那样做
然后在失败时重试
然后这样做如果那项工作
然后这样做如果那项没工作
等等 因此，所有编排都由步骤函数管理
并且感谢步骤函数服务
我们可以获得工作流程的历史审计
例如，如果您有一个非常复杂的ETL
需要协调20个不同的服务
您可以知道每个服务如何反应
发生了什么
并且可以获得一些关于事情如何工作的审计 以及如果他们没有工作一些调试
因此您可以等待任意长的时间
并且步骤函数本身
工作流程有自己的最大执行时间为一年
因此可以是一个非常非常长的工作流程
所以这些都是非常基于文本的
但是让我来给您展示一些具体的可视化示例
因此我们可以使用步骤函数来训练机器学习模型
例如，在这里我们使用左侧的代码
并且您完全不需要了解代码
但是右侧您将获得一个任何人都可以理解的可视化
因此开始是一个数据集被生成 也许使用lambda函数
然后进入下一步
这是训练模型
并且它使用从sagemaker来的应用程序
并且您将训练模型
然后在最后它将模型保存到s3存储桶中
并且可能对模型进行批处理转换
因此所有这一切的编排都是由步骤函数完成的
每个实际步骤将调用不同的服务
例如lambda s3等
但是这里是步骤函数在这里定义工作流程的高层次
关于事情应该如何工作
因此步骤函数的整个编排服务的力量是关于什么
让我们看看另一个示例
例如
调整机器学习模型
因此这里我们使用lambda生成训练数据集 然后使用sagemaker训练模型
然后将模型保存到s3存储桶中
然后我们使用xboost进行超参数训练，然后提取模型路径
然后我们再做一次
我们保存这个超参数读取的模型
然后我们提取模型名称
然后我们进行批量转换
你可以想象，你可以使用步骤函数进行许多不同机器学习流程的编排
最后，如果你有一个批处理任务
这是一个很酷的事情
因为它有一个成功和失败的路径
我们提交并提交一个aws批处理任务
它将执行某事，可能是glue，也可能是批处理，也可能是你想要的任何东西
如果是成功的 会通知成功
如果是失败的
会修改失败
但你可以用步骤函数进行非常复杂的操作
这只是一个高层次的例子，来解释给你看事情是如何工作的 所以进入考试
记住，任何时间你需要编排一些事情
或者确保一件事发生，然后另一件事发生，然后另一件事发生
然后步骤函数是这个完美的候选人
好的
嗯
我希望这对你有帮助，我会在下次讲座见到你
Okay Well I hope that was helpful And i will see you in the next lecture
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/039_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p39 36. Full Data Engineering Pipelines.ai-zh.srt

```
# 🎬 039_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p39 36. Full Data Engineering Pipelines

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的 所以，这只是一个非常大且冗长的
我们讲座的目的是总结我们所看到的所有服务
并试图理解它们如何相互配合。现在这可能对你来说相当沉重
但我希望你现在理解我所说的一切
这只是复习
但如果你还不理解每个服务如何相互关联
这将为你提供一些启示
让我们谈谈实时层
首先我们有生产者将数据写入Kinesis数据流
例如 我们可以将其与Kinesis数据分析连接起来
以进行实时数据分析
我们可以使用Lambda读取该流并在实时中进行响应
或者我们可以将新的目的流直接发送到Kinesis数据流
或者我们可以将其发送到
数据喷射器
如果您想在某处摄取数据
因此，如果数据进入Kinesis数据流
也许我们在AWS上会有一个应用程序
C两个数据流，一个用于读取数据流，另一个用于进行一些分析，以及一些机器学习
它可以肯定地与亚马逊sagemaker服务进行通信，以进行实时机器学习
如果它在 你能看到数据火炬
它可以转换为orc格式，发送到亚马逊s3
然后加载到红移中
或者它可以以json格式将数据发送到亚马逊elasticsearch服务
例如
我们也不一定非要产生两个kinesis数据流
我们也可以生产到kinesis数据火烈鸟
从那里 例如
将其发送到pocket格式到亚马逊s3
正如我们所知道的
我们可以连接亚马逊
你可以看到数据火烈鸟到亚马逊kinesis数据分析
正如我们在之前的讲座中看到的动手操作
所以这就展示了我们可以有很多不同的组合
可以看到数据流 可以看到数据分析
流数据等等
但希望这能让你理解
并且你正在尝试 你开始最终理解所有这些服务之间的区别以及这个实时层
接近实时层如何一起工作
好的 接下来我们将进入kinesis视频层
所以我们有视频生产者
例如它可以是一个摄像头
但也是深层次的植物
一个设备
我们将被发送到kinesis视频流
记住每个设备一个流
也许我们会有识别
这是一个机器学习服务，弗兰克会介绍你，从那个视频流中读取
并产生一个充满数据的kinesis数据流
我们从这里了解我们已经见过
我们可以有亚马逊c2
你能看到数据火炬或kinesis数据分析
只是读取那个流
并且做我们需要做的事情，反之亦然
我们可以仅仅有亚马逊e
c 两个实例 读取canes
使用视频流
然后与sagemaker交谈以产生另一个
你可以看到数据流那可能被读取
例如由lambda读取以在实时中对通知做出反应
例如当视频看起来有什么问题时
好的 现在我们有了批处理层
批处理层是关于处理数据并转换它
等等 所以假设你的数据位于本地mysql中
我们首先使用数据库迁移服务将其发送到rds
这是一个一对一映射
没有数据转换
它只是发送并复制数据从本地到rds
然后我们想要分析这个数据
所以我们会使用数据管道从rds获取数据
并将数据从rds发送到amazon s3存储桶，例如以csv格式
我们可以用dynamodb做同样的事情，通过数据管道
并将数据插入到json格式中的amazon s3存储桶中
然后我们需要将数据转换为
嗯 与etl一起进入我们喜欢的格式
所以我们可以使用glue etl来做一些etl
并在最后将其转换为parquet格式存储到amazon s3中
例如 假设我们的工作产生了许多不同的文件
我们可以使用批处理任务定期清理你的存储桶
你的estuary存储桶
所以这是一个完美的用例用于批处理
因为批处理转换数据
它将定期清理一些文件在你的amazon s3存储桶中
好的 那么我们如何协调所有这些事情
嗯 步骤函数对于确保管道能够响应并跟踪所有不同服务的执行将非常有用
响应 并跟踪整个不同服务的所有执行
现在我们有数据存储在许多不同的地方
将非常有用使用AWS Glue数据目录来了解数据在哪里
以及其模式
因此 我们将在DynamoDB、RDS、S3等部署爬虫以创建该数据目录并使其与模式保持最新
以便进行批处理
这将是批处理层
最后还有分析层
所以数据存储在亚马逊s3中，我们希望进行分析
因此我们可以使用emr，弗兰克会向你介绍emr
这是hadoop sparkhive
我们可以使用红移或红移光谱进行数据仓库操作
或者我们可以创建一个数据目录，将s3中的所有数据进行索引
并且会创建此模式
因此我们可以使用amazon athena以无服务器方式进行分析
以分析s3中的数据
然后我们分析了数据
也许我们想可视化它
所以我们将使用quicksite，弗兰克也会向你介绍
在redshift或athena上进行可视化
这就是整个数据工程模块的内容
我希望你喜欢它 我希望你学到了很多
每当我提到机器学习或一个你没有见过的服务时
例如 emr或quick site
不要担心 弗兰克会向你介绍一切
我就把你交给他了
这就是我在这门课程上的全部内容 希望你喜欢 我会再见到你，弗兰克也会再见到你，下次讲座见
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/040_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p40 37. Random things you need to know AWS DataSync and MQTT.ai-zh.srt

```
# 🎬 040_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p40 37. Random things you need to know AWS DataSync and MQTT

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以我简要地要触及几个主题，它们与机器学习没有直接关系，但是我们从参加考试的学生那里得知，
但是，如果我们在之前的认证中没有遇到过这些主题，你可能会感到困惑
嗯，如果你之前没有接触过这些主题，你可能会感到困惑
一个是AWS数据同步
这是一个用于将数据从本地系统迁移到云中的AWS S3的系统
或者您想在AWS中使用的任何其他服务
从宏观角度来看，
它只是坐在你本地存储和AWS之间
在我们的用例中，可能是S3，用于将您组织内部的数据传输到AWS云中
S在机器学习的上下文中
这可能是
在将大量训练数据从公司内部传输到s3的上下文中
在那里我们可以使用像sagemaker之类的东西来训练机器学习模型
基于这些数据
它的工作原理是，您在自己的围墙内部署一个数据同步代理
作为一个能够连接到您的内部存储的虚拟机
使用NFS协议或B或HDFS
如果您有一个在那里运行的Hadoop集群
然后您有一个AWS
直接连接或互联网为您提供了一个管道，连接您的内部组织与aws自身
直接连接只是与您的组织之间的高速专用连接
或者您也可以通过互联网，无论哪种方式
它将会加密，数据将在每个方向上进行验证
因此您无需担心数据在途中被损坏或被盗
在aws内部
这就是数据同步本身所处的位置
它将路由数据到您希望去的地方
它可以是s3 它可以是弹性文件系统
这可能是fsx在训练机器学习模型中的再次出现
这可能会传到s3
所以如果你在考试中看到数据同步
他们只是在谈论从公司内部获取数据到aws s然后传到s3
在那里你可以使用sagemaker等工具对其进行处理
在考试中你可能还会看到mqtt t
这与aws或机器学习无关
它只是一个物联网协议
所以物联网(IoT)的想法是到处都有传感器
它们一直在向某个中央仓库提供大量数据
在我们这个案例中 我们可能会使用这些数据来训练机器学习模型
所以MQTT只是一个在物联网世界中使用的标准消息协议
它具有所有必要的功能，以确保数据能够可靠地到达
它需要到达的地方
你知道的 诸如此类
所以只需将其视为一种将大量传感器数据传输到机器学习模型的方式
AWS连接是AWS IoT设备
SDK可以使用MQTT协议连接到设备
所以如果你看到mqtt t，只需想到物联网，物联网传感器
这就是上下文，问题试图通过谈论mqtt t给你 这就是全部 就是它
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/041_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p41 01. Section Intro Data Analysis.ai-zh.srt

```
# 🎬 041_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p41 01. Section Intro Data Analysis

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在接下来的部分中
我将带你进入探索性数据分析的世界
数据工程使你的数据到达它需要的地方
但你用于机器学习的算法往往需要你的数据处于特定的格式
并且不包含任何缺失的数据
我们将深入研究特征工程的世界
这是一个重要的学科，但不常被教授
我将教你如何通过各种插补技术处理缺失数据
如何处理你数据中的外点
以及如何将你的数据转换为机器学习算法
期望的格式
在开始使用数据训练算法之前，理解你的数据也很重要
这就是探索性数据分析的全部内容
我们将涵盖工具如scikit-learn
Athena Quick site mapreduce和Apache Spark
它们可以提供你对数据的洞察
即使你的数据来自一个庞大的数据湖
我们还将涵盖一些数据科学基础知识
这样你就可以理解像数据分布
趋势和季节性的概念
这些都是你在考试中被期望知道的东西
当我们完成时 我们有一个有趣的动手练习
我们将在Elastic MapReduce上为维基百科构建一个搜索引擎
这需要我们的数据需要进行大量的预处理 让我们开始学习如何在训练之前理解并准备我们的数据
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/043_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p43 03. Example Preparing Data for Machine Learning in a Jupyter Notebook.ai-zh.srt

```
# 🎬 043_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p43 03. Example Preparing Data for Machine Learning in a Jupyter Notebook

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                如果你刚接触这一切
通常你会在这样一个环境中进行探索性数据分析
在一个叫做jupyter笔记本的地方
jupyter笔记本实际上在你的网页浏览器中运行
它通过你python环境的服务器进行通信
在这个例子中
anaconda 这在数据科学和机器学习世界中是一个非常受欢迎的选择
jupyter笔记本允许你将代码与你自己的笔记和注释混合在一起
这样别人可以理解你看到了什么
这可能也是对你自己的一个提醒
这些代码块实际上会被运行
你只需在代码块内按shift+enter 就可以运行这些代码
你可以在笔记本中设置一整个流水线或步骤
当你迭代时可以重新运行它们
你可以运行分析、准备、清洗数据的整个流水线
在数据上训练机器学习模型
然后将模型部署以进行预测
我们将在整个课程中看到它以使其更真实
我们将查看一个实际的jupyter笔记本
它将创建一个机器学习模型
用于预测乳腺X光结果的严重程度
所以这是一个机器学习的实际世界例子
所以这里有一个使用jupyter笔记本和python的实际世界例子
以及pandas库和numpy和scikit
学习准备和理解数据
并从中创建实际工作的机器学习模型
我们将使用的
这里是来自uci仓库的乳腺肿块数据集
这是一个找到公共领域数据集进行实验的好方法
我们将尝试创建一个神经网络
该网络将预测给定乳腺X光片中肿块的良性或恶性
基于肿块的属性
这是一个非常有用的例子
我们再次不会详细探讨实际的Python代码
因为考试目的
这里只需要了解高层次的发生了什么
在考试中，你不需要阅读、查看或理解Python代码
所以我们不会深入探讨
所以，总体来看 我们在这里要做什么
好的，我们将从导入我们被给予的数据开始
通常，我们被给予的原始数据并不特别有用
正如我们所见，首先我们需要看看自己面对的是什么
我们将导入pandas库
我们将调用它的read_csv方法
对于我们这里的原始数据文件
它是以逗号分隔的格式，先进行查看，看看我们有什么
我们正在处理的是什么
当我们这样做时
我们可以在这里看到一些事情
首先，这份数据文件中没有有意义的列名
因此，我们没有为数据本身部分的列命名
如果我们真想理解正在发生的事情
在这里，我们正在评估和玩弄我们的数据
我们将不得不手动引入那些列名
另一个奇怪的事情是，这里有很多问号而不是数字
而且，事实证明，这代表了缺失的数据
现在 一个问号不是Python期望缺失数据点的样子
因此，我们也必须处理这一点
并且还要决定如何处理那些缺失的数据点
所以我们将只是丢弃它们
尝试用其他东西替换它们
我们将怎么做呢
让我们从这里开始
我们将首先使用pandas重新读取那个csv文件
这次我们将说问号代表缺失的值
一个值
并且我们将传递一个明确的列名列表
这是伴随这个数据集的文档中提供的
我们再次查看结果
好的 这稍微容易处理一些
现在 我们实际上有了有用的列名，我们已经添加到我们的数据框中
而不是问号
我们有没有值的数量
这是Python的数值方式，说缺失数据
无意义的数据
好的 现在我们实际上有我们的数据在一个格式中
我们可以理解它并处理它
我们需要理解我们需要对数据进行的清理和预处理
我们的模型只有那么好，输入的数据
让我们从调用pandas的数据框来获取一些关于那里的高级信息开始
通过查看计数
我们可以看到它们并不相同
因此，这里有很多缺失的数据
例如
密度列似乎缺少了很多数据
与严重性列相比 那么我们如何处理所有那些缺失的数据
嗯
我们将稍后再讨论
在这一节中 有很多方法可以插补缺失数据
我们将在后续章节中讨论 有很多方法可以插补缺失数据
并使用有意义的替代品替换缺失的数据
但最简单的做法就是删除包含缺失数据的行
完全不处理这个问题
如果你有数据足够多，可以做到这一点
这样做不会引入任何偏见
那么这可能是一个合理的做法
至少在你算法的早期迭代阶段
我们需要确保我们不会引入任何类型的偏见
通过删除这些缺失的行
所以让我们可视化一下那些包含缺失数据的行看起来像什么
这就是这条线在这里要做的事情
它只是提取所有有is null的行
这些行在列中包含一个空值
只是粗略看一下
我的意思是有更复杂的方法来实际做这件事
但只是看看
我们真的不能看到年龄或其他东西的任何明显模式
看起来缺失的数据在不同的类型的人中分布得很均匀
因此，既然我们可以感到放心，就可以删除这些数据
不过，删除数据从来都不是最优的选择
通常可以通过插补方法更好地处理这个问题
但我们稍后会讲到
但现在我们只是简单地在数据框中删除缺失值
就是说任何包含缺失数据的行
直接删除它们
让我们去做这个并再次描述它
现在我们可以看到每个列都有830行数据
所以再也没有缺失数据了
如果我们想要的话
我们可以实际上比较这些的平均值和标准差
在'前后'对比中看看这种影响有多大
好的 现在我们在将数据输入scikit之前
我们需要对其进行建模
我们需要将其转换回numpy数组
这正是values方法所做的
我们只是取我们的pandas数据框
将其转换回numpy数组
就是这样 让我们看看
我们可以在这里看到，转换到numpy数组的特征数据框看起来像这样
接下来我们需要处理的是数据归一化
所以你可以看到这里有很大的数据范围
例如，年龄
它将是一个比
比密度的形状或质量更大
如果使用这个数据
那个时代的权重对结果影响会比其他因素大得多
这不是一个合理的事情，没有意义
还有年龄
嗯 你知道，它们是围绕
我不知道 30到40岁
所以，我们需要考虑到这个偏差
我们需要标准化状态
这就是关键，确保所有数据列的中心值是平均值，并且缩放到相同的范围
这样它们具有相同的权重
好的
为了做到这一点，我们可以使用scikit lear的预处理模型 它有一个叫做标准缩放的好用工具，正好可以做到这一点
我们只需在整个数组上使用标准缩放器调用fit transform
然后查看结果数组
你可以看到，现在事情
你知道，在正负一左右
更多一些
它有这种正态分布 所以它并不是真正受限于那个
但它们都围绕零中心，并且大致在同一范围内
这是很重要的
所以我们使用pandas来理解我们的数据，处理数据中缺失值
我们也可以使用pandas来处理数据样本，如果我们需要
如果我们有数据处理问题
在我们实验的时候 然后我们将数据导出到一个numpy数组
并使用scikit learn的预处理模块将数据缩放到一致的范围内
现在我们的数据已经准备好并清理完毕
我们可以将其喂给一个实际的机器学习模型
我们不会深入探讨这个
但我们使用tensorflow的keras api创建一个神经网络
它将从这个数据中学习
并创建一个神经网络
它可以预测一个它没有见过的质量
可能是良性的还是恶性的
让我们运行它
别担心
我们会在课程建模部分详细讲解
这很有趣 现在我们有了这个模型
我们可以将其包装成一个scikit learn估计器
就像我们在幻灯片中看到的那样
keras有一个方法可以将一个tensorflow神经网络变成一个scikit learn模型
所以我们使用keras分类器来做到这一点
基本上它会创建一个神经网络，具有多种参数
现在，我们已经定义了这个模型
我们可以将其包装成一个scikit learn估计器
基本上它会创建一个神经网络，具有多种参数
我们将其称为估计值
然后我们可以使用scikit-learn的交叉验证分数函数来实际评估它
并训练它 所以交叉验证分数所做的是
它实际上会随机将数据集分为训练和测试数据集多次
因此它会多次使用训练组件
训练数据集
训练模型
训练神经网络并使用我们保留的测试数据评估结果模型在从未见过的数据上预测标签的效果
预测从未见过的数据上的标签
在这个情况下 我们将实际进行10次
并计算每次将数据分为训练和测试集的结果的平均值
所以我们可以开始运行
所以基本上它将把我们的数据分成10次训练和测试集
训练10个不同的神经网络并评估所有10个网络的结果
显然这将花费一些时间
所以我们会等它完成
我们已经完成并得到了大约80%的准确率
总的来说，对于这个特定的数据集并不是很好
但这也不是坏事
我们会更多地讨论这准确性的真正含义
以及如何在后续的课程中解释这一点
但再次 这里我们想让你理解的是高级内容就是jupyter笔记本是如何工作的
它们是如何被使用的 所以你可以看到我们有这种块的python代码
我们可以一步一步地运行
它实际上是在与一个真实的python环境进行通信
在后端运行
我们可以在笔记本中穿插一些笔记和备注，以便提醒我们自己
这里发生了什么，以及我们做这些事情的原因
无论是为了自己还是为了他人
这就是笔记本的全部意义
我们使用pandas库来实际探索我们的数据
稍微清理一下
处理缺失值
然后将其导出到实际模型中
使用这个示例
TensorFlow和Keras
使用scikit
在使用scikit Learn之前对数据进行预处理
使用标准缩放模块对数据进行归一化处理
我们使用了jupyter笔记本
我们使用pandas来可视化和清理我们的数据
我们使用scikit learn来实际对数据进行缩放和预处理
然后使用实际的深度神经网络来创建模型并评估其结果
这一切都在一个笔记本中进行，非常高级
这就是在这里发生的事情
这是机器学习世界中非常普遍的模式 希望这有助于使其都变得真实
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/044_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p44 04. Types of Data.ai-zh.srt

```
# 🎬 044_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p44 04. Types of Data

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                对于新接触数据科学的人来说
让我们从数据科学的基础知识开始
在考试中你一定需要这些基本概念
让我们从讨论你可能遇到的不同类型的数据开始
这是非常基础的内容
但我们会逐步过渡到更复杂的内容
了解你处理的数据类型很重要
因为不同的技术可能根据你处理的数据类型有不同的细微差别
所以有数据的几种不同形式
如果你愿意，我喜欢冰淇淋
所以我们在这里与一张图片联系起来
主要的数据类型有数值型
分类和顺序数据
再次 你处理的数据类型决定了你可能使用的算法类型
以创建机器学习模型
所以你需要记住你处理的数据类型
在你尝试分析它之前
让我们从数值型数据开始
这可能是最常见的数据类型
这基本上代表了一些可以量化的东西，你可以测量
一些例子是每个人的身高
页面加载时间
股票价格
变化的东西
你可以测量的东西 有很多可能性的东西 现在，基本上有两种数值数据
所以，数据的一种
一种口味
如果你愿意 这里有离散数据
它是基于整数的
所以 例如 这可能是某种事件的数量
例如 一个客户在一年内购买了多少次
嗯 那只能是一个离散值
他们买了一样东西
或者他们买了两样东西 或者他们买了三样东西
但他们不可能买了两点
两 五样东西 或者三又三/四分之一件东西
它是离散值，对它有整数限制
另一种类型的数值数据是连续数据
这是有无限可能性的物品
你可以进入分数
所以回到人们身高的例子
人的身高有无限的可能
你可能五英尺十英寸
三 七 六 两英尺五英寸高
或者随便 或者像网站结账时间这样的东西
这可能是一个巨大的可能性范围
比如十点 七
六 两 五秒
或者一天内落下的雨量
这些都是连续数据的例子
那里可能有无限的精度
潜在地 所以这是一个连续数据集
所以总结一下，数值数据是你可以用数字量化测量的东西
它可以是离散的，基于整数的
比如某种事件计数
它也可以是连续的，你有无限的精度可以使用那个数据
我们将要讨论的第二种类型的数据是类别数据
这是没有任何固有的数值意义的数据
你不能直接比较一个类别到另一个类别
比如性别
是或否的问题
种族 居住州
产品类别 政治党派
诸如此类
你可以给这些类别分配数字
并且通常你会这样做 但那些数字没有固有的意义
所以 例如 我可以说德克萨斯州的面积大于佛罗里达州的面积
但我不能说德克萨斯州大于佛罗里达州
我的意思，它们只是类别
它们没有真正的数值可量化的意义，我可以拿来相互比较
它们只是我们分类不同事物的方式
我可能会给每个州分配一个数字
我的意思，我可以说佛罗里达州是第三州，德克萨斯州是第四州
但那里的三和四之间没有真正的关系
数字的实际顺序并不重要
这就是我对类别数据的理解
它们是任意的
这只是一种简写，用来更紧凑地表示那些类别
再次 分类数据没有任何内在的数值意义
这只是你选择用来分割一个数据集的方式
基于类别
最后一类是顺序数据
它是数量和分类数据的一种混合
一个常见的例子是电影的星级评价或音乐的星级评价
或者类似的 在这种情况下我们有分类数据
因为它可以是一到五星
其中一可能代表差，五可能代表优秀
但在这种情况下，数字有数学的意义
你可以将这些值相互比较
我们知道五意味着它比一好
在这种情况下，我们有不同的分类，它们实际上有数值的关系
我可以说一星少于五星
我可以说两星少于三星
四星大于两星
以衡量它们的质量
所以它处于中间
你知道 你也可以这样想
如果你认为它实际上是星星的数量作为离散的数值数据
所以这里有一种模糊的界限，介于顺序和离散数值数据之间
在很多情况下，你可以实际上以同样的方式处理它
这就是全部 这就是三种类型的数据
数值 分类 顺序
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/045_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p45 05. Data Distributions.ai-zh.srt

```
# 🎬 045_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p45 05. Data Distributions

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们深入数据分布
这描述了你的数据落在特定范围内的可能性，本质上
当你进行探索性数据分析时，这非常重要
这也是你很可能在考试中看到的内容
让我们从正态分布开始
正态分布是一个例子
这是一个概率密度函数的一个例子
这是我们之前见过的正态分布曲线
从概念上很容易尝试将这一想法视为给定值发生的概率
但当你谈论连续数据时，这种说法有点误导
因为在连续数据分布中有无限可能的实际数据点
可能会有零或零点零零一
或零点零零零零零一
所以非常具体值发生的实际概率非常小
甚至无限小
所以概率密度函数真正说的是给定范围内值发生的概率
这就是你思考这些事情的方式
例如，在这个正态分布中，均值和一个标准差之间的范围
有34.1%的机会
有1%的机会 结果是一个值落在那个范围内的
你可以根据需要收紧或扩展它
然后找出实际的值
但这就是思考给定值范围内概率密度函数的方式
它给你一个找到该范围发生的概率的方法
你可以看到，当你接近均值一个标准差时
你很有可能得到那里，如果你加起来是三
四和三四 无论结果是什么
这是落在均值一个标准差范围内的概率
但是当你在这里在两个和三个标准差之间时
嗯 我们只剩下不到4%的组合，加上正负两侧
当你在三个标准差之外时
那么我们实际上不到1%
所以 这只是一种可视化的方式
谈论给定数据点的概率
这种特定的分布是正态分布，出现的很多
它基本上就是一个以零为中心的钟形曲线
这代表你数据集的均值
你可以将这个分解成各种标准差，分布在这些点上
再次 概率分布函数给你一个数据点的概率
落在某个给定范围内的给定值
正态函数只是一个概率密度函数的例子
让我们现在来谈谈更多的
当你处理离散数据时
关于有无限可能值的那个小细节消失了
事情变得有点不同
现在我们谈论的是一个概率质量函数
如果你处理的是离散数据
我们谈论概率质量函数
例如 我们可以绘制连续数据的正态概率密度函数在这个黑曲线上
但如果我们将其量化为一个离散数据集
就像我们做直方图那样
我们可以说数字3发生了一定的次数
真的 我们也可以说数字3有超过30%的几率发生
所以概率质量函数是我们可视化离散数据发生的概率的一种方式
它看起来像直方图，因为它本质上就是一个直方图
这是一个术语
区别 概率密度函数是一条实线
它描述了连续数据范围内值的概率
概率质量函数是数据集中给定离散值发生的概率
概率质量函数的一个例子是泊松分布
这是一个基于所谓的泊松实验的特定离散分布
泊松实验被定义为一系列以成功或失败告终的事件
在时间和距离上平均成功的次数是已知的
例如 你可能知道一个房地产公司平均每天卖出一定数量的房屋
泊松分布会告诉你卖出给定数量房屋的可能性
在第二天
在这个图中
Lambda将代表预期的房屋销售数量
如果lambda是1
我们正在查看黄色线
我们可以看到大约有三
七分之一的机会在任意一天销售零或一套房子
如果我们知道总体平均数是一
但如果平均每天我们销售十套房子
我们正在查看蓝色线
现在 这开始看起来更像正态分布
随着我们有更多的可能值小于十来工作
重要的是，泊松分布处理离散数据
我们不能销售二点五
在一天中四套房子
所以我们没有足够的点数来处理一个漂亮的
平滑的正态分布与任何值
当我们接近lambda值为零
分布开始看起来更像指数
所以再次 当你听到关于泊松分布的
记住我们必须谈论离散数据
其他例子可能是
你一天收到的信件数量
或呼叫中心收到的电话数量
这些都是离散事件
你没有半个电话或半封信
在这些问题中，只有整型值才有意义
另一个离散概率分布是伯努利分布
这仅描述在Yes或No问题序列中的成功次数
例如
抛硬币正反面可以被描述为二项分布
这只是另一个离散概率分布，你只能有二项式0或1
你知道 正或负正反面
某种结果
还有一种称为伯努利分布的东西
你可能想要知道
它是二项分布的特殊情况
它只有一个试验n等于1
你可以将二项分布视为伯努利分布的总和
伯努利分布是一个单次试验
而二项分布可能由多次试验组成
就是这样
所以总结一下，我们谈到了基本的数据分布，包括正态分布，泊松分布，二项分布和伯努利分布
记住，只有正态分布是连续数据
其余的则是离散数据
记得，只有正态分布是连续数据 其余的则是离散数据
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/046_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p46 06. Time Series Trends and Seasonality.ai-zh.srt

```
# 🎬 046_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p46 06. Time Series Trends and Seasonality

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈与时间序列分析相关的一些基本知识
所有时间序列都像它听起来的那样，是一系列随时间变化的数据点
它们通常都是离散的样本
在时间的某个特定点采集
这就是时间序列的全部
例如 时间序列中可以有趋势
这是海平面全球平均水平的实际图表 从1870年到2008年
随时间变化
而且嗯 是的
这不好看，伙计们 但这是另一个话题
你们可以看到这里有一个明显的总体趋势在上升
从1870年到2010年，海平面在上升
但每年都有波动
但较长时间内的大趋势非常清楚
它在上升，向右移动
这就是趋势的全部内容
如果你后退一步，看你整个时间序列
它是否看起来朝一个方向或另一个方向趋势
这就是趋势
时间序列也可以表现出季节性
所以 例如 如果我们看看肺炎和流感的发病率
我们可以看到它肯定有季节性成分
它倾向于在某些月份达到峰值
你知道 夏天不这么糟糕
所以我们有一个季节性
由这些黑色线条表示的常规波动
从月到月
季节性可以叠加在趋势上
基本上构成一个时间序列
也可能有趋势
但在这种情况下看起来相当平坦
我们主要观察到的季节性信号
我们在这个时间序列中看到的大多数波动都可以用
基本上这一年是什么时候
这个月是 这周是
你可以两者都有 当然
嗯 例如
你可以取一个原始数据集，这是一个时间序列，就像这里看到的
提取其季节性成分
所以你实际上可以数字形成出什么是季节性的部分
如果你从原始数据中减去季节性，你是
你剩下的是趋势 基本上现在我们看的数据实际上是实际的维基百科编辑
所以原始数据在上面
我们提取了季节性趋势
如果你减去那个季节性
我们得到去除了月与月变化的总体趋势
所以这是季节性和趋势一起工作的一个例子
所以这是一个具有季节性的数据集
事实证明，无论什么原因
人们会在某些月份比在其他月份更频繁地编辑维基百科页面
但这里也存在一个更大的趋势
所以显然在2006年事情开始爆炸式增长
他们大约在27年左右达到了顶峰
近年来，它们一直在下降并趋于平稳
这就是我们描述的趋势
季节性是月与月之间的变化
还有噪声是时间序列的一个组成部分
我们不仅拥有季节性和趋势，还有噪音
这也是一些随机噪音，仅仅
你知道 无法解释
否则，一些变化本质上是随机的
你无法做任何事情
有几种方法来建模这个
所以，一个是加性模型
所以，如果你的季节性变化是恒定的
你知道，总体的交通情况并不重要
基本上，从一个季节到另一个季节看到的变化是相同的
然后你可能会使用一个加性模型
你可以将整个时间序列描述为季节性的和长期趋势的加和再加上噪声
这就是你的整体时间序列
正如我们之前所看到的，你可以取一个时间序列
减去季节性
你就会剩下趋势加上噪声，对吧
所有数学上都是成立的
但有时季节性会随着趋势而变化
随着你的数据的规模增加或减少
那个季节性振幅也增加或减少
在这种情况下，您想使用乘法模型
或者您可以说时间序列是季节性、趋势和噪声的乘积
这取决于数据的性质
那里没有像硬性和快速的规则
一种模型或另一种模型可能在描述您的数据方面更好
您知道那里存在一个时间序列分析的世界
但对于考试的目的，对探索性数据分析 这就是您需要了解的部分
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/047_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p47 07. Introduction to Amazon Athena.ai-zh.srt

```
# 🎬 047_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p47 07. Introduction to Amazon Athena

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                因此，我们讨论了一些分析并准备数据的一般方法
你可能在AWS内外使用它
但是让我们谈谈一些你可能使用的特定AWS服务
在探索你的数据过程中，过于明显地发挥了在考试中的巨大作用。
我们比这深入得多
当我们在为aws认证数据分析或大数据考试做准备时
我们需要对它有一个更高层次的理解，以便于机器学习考试。
所以这两项认证考试之间存在一些重叠
我们就需要为了这个目的而稍微提高一个层次
让我们从亚马逊雅典娜开始
这提供了一种无服务器的交互查询你s three数据湖的方式
无论它是什么
所以没有必要将数据加载到数据库中，使用athena
它只是保持其原始形式在s three中
所以你可以有一个数据湖的csv文件
或者你有也支持json
Orc Parquet和avro格式
它只是看看它
分析它并允许你对其执行sql查询
基本上在内部
它由一个开源的东西叫presto驱动
记住与athena的主要事情是a
它让你能做sql查询
在s3数据湖中结构化或不结构化或半结构化数据
并且b它是无服务器的
所以你不需要管理任何服务器
你只需使用athena
给它一个查询 告诉它去哪里查找数据
而且它就发生了
AWS会找出实际运行这些服务器的地方
为你完成这一切
你不需要管理这些使用案例
嗯 也许你想对你的网页日志数据进行一些非常规的查询
你可以将原始的网页日志存放在S3中某处
并用Athena查询这些数据
然后 看看其中的趋势
你可能想查询一些正在s three中预处理的数据
在你将其加载到红移（Redshift）或其他数据仓库之前
你可能想分析来自云轨迹或云前端的日志。
或者你的网络虚拟专用云（VPC）或弹性负载均衡器日志
他们坐在S三
使用 athena 仅在那个上运行 SQL 查询
你也可以在我们的工作室中将athena与jupiter和zeppelin集成
笔记本很不错
你也可以直接在你的笔记本中运行查询
这对于在笔记本中分析和理解你的数据非常有用
它还与快速站点集成
这是aws s的数据统计工具，我们将很快讨论
它也可以与几乎所有其他可视化工具集成
通过odbc和jdbc协议
了解athena与aws glue的关系很重要
所以 如果你使用glue
为你的s three数据湖添加结构并从中提取
你知道哪些列可能与其中的数据相关
athena可以利用这一点
因此，glue数据目录可以创建一个跨各种服务的元数据仓库，包括athena
它可以爬取s three中的数据，提取其结构
并提取该方案的结构
athena可以使用该方案来实际执行sql查询
并从sql角度为列命名
因此，glue的全面管理ETL功能可以用于转换数据或将其转换为列格式
以优化athena查询的成本并提高其性能
一个典型的管道可能看起来像这样 你有数据存放在s three中
你有一个glue爬虫来提取数据的实际含义及其结构
athena则位于其上方以对其进行查询
你可以将这些athena查询发送到quicksite中进行可视化
athena的计费模型为按需计费
这些细节在考试中可能不太重要
但值得了解
嗯
它很便宜
你知道的
每TB扫描5美元 它根据查询扫描的数据量向您收费
基本上 你可以通过压缩数据来降低成本 因此，如果你想让athena保持便宜，你应该压缩数据
此外，将数据转换为列格式可以节省大量费用
因为它允许athena仅读取处理查询所需的列 因此，使用列格式，如orc和parquet与athena结合使用是一个好主意
显然，glue和s three也会有自己的收费
从安全角度来看
它使用所有典型的访问控制
iam acl s three桶策略也会发挥作用
还有iam策略为athena或quick site提供方便
当然，athena访问s three时需要考虑安全问题
当然，glue和s three也会有自己的收费
从安全角度来看
它使用所有典型的访问控制
iam acl s three桶策略也会发挥作用
还有iam策略为athena或quick site提供方便
当然，athena访问s three时需要考虑安全问题
当然，glue和s three也会有自己的收费
从安全角度来看 你可以对来自athena查询的结果进行加密
如果你想要在一个s3存储目录中进行加密
你有多种加密s3数据的方式
可供你使用
包括ssc
s3 ssc kms或csse kms
跨账户访问和athena使用s3存储桶策略也是可能的
对于在传输中的数据
我们始终使用tls进行加密
以加密s3和athena之间的数据传输
一些你不想使用athena的事情包括高度格式化的报告
或可视化 这正是quick sights的作用
我们将在下一节讨论
它也不适合进行etl
这正是glue etl的作用
所以athena只适用于做ad hoc查询
嗯 使用sql对你的s3数据湖进行查询 就是这样
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/048_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p48 08. Overview of Amazon Quicksight.ai-zh.srt

```
# 🎬 048_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p48 08. Overview of Amazon Quicksight

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来花点时间谈谈亚马逊快速站点
尽管快速站点从技术上讲更多是一个数据分析工具
但它明确包含在机器学习考试中
所以我们要花些时间来讨论这个
它是一个数据分析和可视化工具
而数据分析和可视化是你需要为考试了解的事情
那么快速站点是什么
嗯 它的官方描述是它是一个快速的
简单易用基于云的商务分析服务听起来很棒
但它实际上并没有告诉你太多
它从根本上来说
它只是一个应用程序，让你能够非常快速且轻松地创建仪表板
和图表和图表和报告
基于你存储在任何地方的数据
它在这方面做得非常快
Quickside 有一点不同之处
它更针对最终用户
针对商务人士
所以它并不是针对开发者的
把它看作是一个应用程序，允许组织中的所有员工构建可视化
进行即席分析
从数据中获得商业洞察，并且可以在任何时间、任何设备上完成
无论他们有浏览器还是移动设备
这并不是说你需要成为开发者并处理aws
仪表板 或API
或命令行 或任何类似的东西
这更多地是一个用于可视化数据的高级应用接口
那几乎被存储在任何地方
并且由于那种性质
当然这是一个无服务器应用
你不需要考虑底层实现
你在这里只是一个应用程序的用户，用于创建各种图表和图形
它可以连接到许多不同的数据源，既包括亚马逊内部的，也包括外部的。
因此它可以连接到红移
显然 那就是一个明显的应用领域
你可能想可视化由亚马逊托管的数据仓库
你也可以将其放在aurora或rds之上
我们将更广泛地看到
你可以将其放在任何数据库上，athena
这也是一个非常常见的用例
所以你在s3中有一个数据湖
你想通过glue将athena堆叠在该数据湖之上
也许 你可以使用quick side来查询athena并可视化该数据湖中的数据
此外，ec 两个托管的数据库
你可以运行自己的数据库
E C 两个 只要它使用标准语言
你可以与之交谈
基于文件的 你可以直接与S3数据进行交谈，存储在数据湖中
你也可以与Excel数据交谈，数据存储在逗号分隔的值中
或者制表符分隔的值格式
或者任何 嗯，多种日志格式
也有准备数据的方法
在分析或可视化之前
在可视化之前可以更改一些内容
例如 假设你选择一个CSV文件作为数据集
并且你想将列名更改为更美观的东西
你可以这样做 更改字段名
可以添加计算字段
在顶部使用SQL查询
更改数据类型 它也可以在AWS上工作
物联网 分析
互联网事物和数据准备
如我们所说，它允许一些有限的ETL
因此，你可以在Quick Sight过程中进行一些转换
Too 它如何工作
称为Spice
一种强制缩写，用于超级快速的并行内存计算引擎
这是加速这些查询的底层引擎
通常
你使用Quick Side在非常大的数据存储、数据仓库或数据湖上
它必须快速获取你想要的可视化
Spice就是这样做的
它使用的技巧包括内存中的列存储和机器码生成
它使用这些来加速在非常大的数据库上的交互式查询
内存计算限制为每个用户10GB的Spice内存
当然，它高度可用且耐用
就像AWS中的所有内容一样
它也高度可扩展
一次可以支持数万个用户
你可能使用Quick Side的一些事情
它旨在于交互式、即兴的数据探索和可视化
所以他们使它容易创建图表和图形，探索数据视觉上
想法是如果容易这样做
你可以交互式地进行操作
并尝试深入挖掘和探索，实时切片和处理你的数据
以从中获得一些见解
你也可以用它来创建仪表板和关键绩效指标报告
并且我们将看到它还有一个新的按页报告功能
以使其更加坚固
你可以分析并可视化存储在S3中的服务器日志中的数据
你可能在本地拥有的数据库
任何标准的AWS数据库源，如RDS
Redshift
Athena和S3 SaaS应用程序
你可以将其与Salesforce连接
并且任何支持JDBC或ODBC的程序都可以使用
将这更直接地与机器学习连接
它确实有一个新功能叫做机器学习洞察，有几个酷功能
一个是叫做由机器学习驱动的异常检测，底层使用的是亚马逊非常喜欢的随机切割森林算法
所以你可以用它来尝试检测销售数据中的异常
以及你网站上的所有异常和延迟
无论它是什么
如果你需要识别数据中可能存在的异常并去除它们
它还有一个由机器学习驱动的预测功能
这也使用随机切割森林
以便自动去除数据中的异常
并进行预测
你可以使用它在考虑季节性的同时进行预测
并且自动排除异常值
以及自动填充缺失值
再次 这是为非技术用户设计的
所以他们可以非常快速且轻松地说嘿
快速站点 基于这个数据做一个预测
快速站点通过机器学习洞察
然后它会做所有有关如何按摩和准备数据的脏活累活
以给你提供最好的预测
在机器学习洞察中还有一个叫做自动叙事的东西
允许你构建嵌入叙事的丰富仪表板
以用平实的语言讲述你的数据故事
这是一种利用AI的方式
为你创建仪表板
你可以使用它生成的仪表板或者在此基础上进行构建
当然它并不总是第一次就能做对
如果你想自定义它生成的内容
你也可以这样做
至于QuickSight的直接机器学习功能
Go 就是这样
如果考试问你QuickSight的其他机器学习特性或指标
它们并不存在
这是一个陷阱问题
所以记住，Quick Site相对新的功能叫做Quick Site Q，这是一个很好的功能
这是另一个机器学习相关的东西
但这是一个 这是一个附加功能
它是由机器学习驱动的
它使用自然语言处理回答商业问题
使用Quick Site Q
你不必尝试编写SQL查询
或者你知道的
你不必尝试弄清楚如何将字段拖放到你的仪表板中
以及如何手动构建图表
你可以说诸如
佛罗里达州最畅销的商品是什么
这是一个针对特定地区的附加功能
然而，正如我们将在后面讨论的那样，这会产生额外的费用
然而，它并不能直接使用
正如你可以想象的那样，有人需要告诉Quick IQ
在这个特定的例子中
哪一列代表销售数据
是的 它不能自己神奇地找出来
所以首先
有个人培训如何使用它
这是使用Quick IQ的必要条件
他们不会直接给你使用，然后希望你能搞定
他们会给你一些个人培训，教你如何整理你的数据，准备你的数据
以及如何使用Quick IQ以获得最佳结果
它可能不像你希望的那样直观
你可能需要设置与数据集相关的主题，以便让Quick Site Q工作
显然，你需要确保你的数据集和字段是NLP友好的
如果你的销售数据列名为q七十三二五五
这不会帮助算法理解
这代表销售数据
所以你需要确保你的字段和数据集是用英语或其他自然语言命名的
以便使其NLP友好
你还需要明确告诉它如何处理日期
这需要在前期定义好
Quick Site Q的亮点是自然语言处理
在Quick Site上回答商业问题
另一个新功能是在2022年底推出的Quick Site分页报告
这些是为管理层设计的高度格式化的多页报告
这是一个例子
在右边可能跨越多页
你可以在现有的Quick Site仪表板上构建它
所以你可以创建一套Quick Site仪表板
将它们放在一起，使其看起来不错，然后为管理层打印一份常规报告
这将在2022年11月推出
你不想使用Quick Site做什么呢
以前，你不想用它来创建高度格式化的固定报告
但分页报告的引入使得这一点不再成立
我正在指出这一点 然而
因为有时考试无法及时更新
以我们希望的速度跟上最新的变化
如果你在2020年代末参加考试
你知道在2023年初
2023年中期
我不知道他们是否已经考虑到了分页报告
所以你可能想将这个放在你脑海中的后面
如果你看到关于高度格式化的快速视图中的自定义报告的特定问题
你不能那样做 但现在你可以这样做
它不设计用于ETL
正如我们所说，有一些有限的ETL能力
但是做你知道的
核心ETL操作
你想要使用Glue来做
那就是Glue的作用
Quick Site可以做一些转换
但是真正你想要使用Glue来做那些繁重的工作
关于安全的快速说明
它为你的账户提供了多因素认证（MFA）
显然，就像几乎所有事情一样，VPC连接是可能的
因此，你可以将Amazon的QuickSite IP地址范围添加到你的数据库实例中
安全组规则以启用流量进入你的VPC和数据库实例
行级安全（LS）可以启用QuickSite数据集所有者以行粒度控制对数据的访问
基于与数据交互的用户相关的权限
因此，有了LS
QuickSite用户只需要管理一个单一的数据集
并应用适当的行级数据集角色
企业版的快速站点的另一个相对新特性是列级安全
所以在企业版中
你也可以限制用户访问数据的特定字段
而不仅仅是特定的行
私有VPC访问和快速站点也是可能的
这是由弹性网络接口实现的
或ENI，用于在VPC中与数据源进行安全私有通信
它还允许您使用AWS Direct Connect创建安全的
与本地资源之间的私有链接
处理这个问题有几种方式
就像我们提到的，这是一种人们使用的应用程序
它是一种网站
基本上一个网络应用 你需要控制人们如何访问那个以及如何在quickside中查看底层数据
也因为你需要按用户收费
所以你不想让全世界都访问你的仪表板
那样会很快变得非常昂贵
因为你是根据用户收费的
用户通过iam定义
就像nes中的大部分事情一样
或者通过电子邮件订阅过程
你可以这样做
你可以快速站点电子邮件用户成为快速站点域的一部分
然后你可以决定删除用户如果你愿意
以及如何处理孤儿资源
当这种情况发生时
最后也有样本基单一点登录能力
再次基于iam
并且只在企业版中
也有与活动目录的集成
所有内容都有多因素身份验证 当然 因为那才是现在的标准
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/049_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p49 09. Types of Visualizations, and When to Use Them.ai-zh.srt

```
# 🎬 049_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p49 09. Types of Visualizations, and When to Use Them

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们更深入地了解快速站点可视化的外观
他们喜欢在考试中问这样的问题：给定这个数据集，你想要从中获取什么
哪种可视化方式最合适
所以让我们看看它们看起来是什么样的，快速小仪表板
我们之前讲过几次了
这就是它们实际上看起来的样子
这只是分析的快照
它是只读的
一旦你创建了它，它就只能读取
它是为了 你知道的
让其他人查看并获取事物的快照
一旦创建，你可以将其与具有快速站点访问权限的其他用户共享 但他们无法编辑或更改过滤器
所以它是一个很好的方式，将一堆视图组装起来，供其他人消费
快速站点提供了多种可视化类型
相当多 实际上有一个自动图表
它允许你根据数据的特性选择最合适的可视化
而不是你选择
可视化类型是根据数据和关系的有效方式选择，以最佳方式揭示数据
有时它做对了
有时你想在此基础上构建
用于比较和分布的工具是条形图
所以直方图之类的
条形图是一个很好的选择
如果你在寻找随时间的变化
那就是线图
如果你在寻找随时间的趋势
你需要考虑线图和区域线图
或者可能是区域线图
如果你在寻找相关性
那就是散点图
散点图会给你两个d数据的具体图解
所以你可以看到原始数据并推断出你想要的东西
这对于查看相关性很有用
因为那些散点倾向于围绕一条线或某种模式分布
我可以告诉你在两个轴之间有相关性
正确 热图也用于对二维数据进行颜色编码
所以 例如 如果你想在x轴上绘制餐厅并在y轴上绘制食品评论员
你可以在每个单元格中包含每个评论员对每个餐厅的评分
并且根据评分进行颜色编码
如果一个个体餐厅在所有评论中都能得到好的评价
它会在热力图中以热色列显示
所以我们稍后会看一下那个例子
为了更好地进行聚合
我们有饼图和树图
树图你可能不太熟悉
它们使用嵌套矩形显示分层数据
我们很快就会看到一个例子
对于表格数据我们有pivot表
可以用来汇总数据
并测量两个维度的交点的值
你可以应用到统计函数
所以如果你想处理多维数据
并应用到统计函数
以前有一个叫做故事的特性
但它已经过时不再使用
一些更新的视觉类型
他们近年来引入的
我们有kpis
我们有地理空间图表
或者只是地图的一个花哨名字
带有覆盖物
就像右边的这个
而不是花太多时间谈论这些
现在我会稍后给你看例子
让我们深入研究柱状图
所以再次 它们用于比较和分布
如果你需要查看数据的分布
或比较不同时间段的数据
柱状图是你想要的 它们可以是垂直的
也可以是水平的
如果你知道数据分析
我相信你知道柱状图
线图你也应该熟悉
我希望它们是用于可视化随时间的变化
所以如果你看到一个问题
询问你使用什么样的视觉化
来查看随时间的变化或趋势 好机会
答案是线图
你也可以有面积线图
或堆叠面积线图
散点图这是一个例子
用于显示相关性
我们在这个例子中显示了原始数据点
对于喷发持续时间和喷发之间的等待时间
来自黄石国家公园的老忠实间歇泉
美国
你可以在这里看到，只要粗略一看，就有这两个小集群
你知道这告诉你关于数据的性质
这里也有一种线性关系的样子
如果你想看看等待时间与爆发之间的联系
散点图可以是一个好的开始，来理解这种相关性
我们可以从这个散点图中看出，是的
爆发之间的时间越长，爆发持续的时间就越长
我们可以从这个散点图中看出，是的
爆发之间的时间越长，爆发持续的时间就越长
这有道理对吧
你知道随着时间的推移，事情会逐渐积累
它必须释放所有积压的压力
这是一个热图的例子
具体来说 这是一个DNA微阵列数据
反映了几种条件下的基因表达值
再次，您有两维数据
其中两维网格中的每个单元格都进一步增强
以表示该单元格值的颜色
对的，这允许你可视化什么
你知道 所谓的冷或热
基于该网格内的低高值
对相关饼图有用
我想你们所有人都知道什么是饼图，对吧
所以 这个特定的例子在这里显示了英语母语人口的数量，看看这个来自维基百科
你可以看到大多数都在美国
紧随其后的是英国 紧随其后的加拿大
然后我们有澳大利亚，所有其他内容都被打包在那边的其他切片中
所以如果你在寻找聚合
饼图可能是一个值得仔细看的选择
相对而言是甜甜圈图表
嗯 这些图表更专门用于比较维度中的项目的值
所以这种图表的最佳选择
最佳用途是显示一个总金额的百分比
甜甜圈图表中的每个扇区都代表维度中的一个值
楔形的大小代表了所选指标的价值中该项目所占的比例，相对于尺寸的孔。
与尺寸的孔相比。
甜甜圈图表只有在精确度不重要时才是最佳的。
维度中有几个项目。
这个特定的例子。
一些行星的访问。
你知道我们向这里的每个行星发送了多少机器人太空探测器。
我认为天王星实际上并没有被访问太多。
所以我不确定这是真实的数据。
但你明白了。
仪表图也是相对较新的功能
嗯 就像它听起来的那样
它是以仪表盘的风格进行可视化
就像你汽车上的仪表盘
它的目的是显示你所测量的东西的量
比如油箱中的燃料
或者带宽使用情况
或者类似东西比较测量值
更具体地说
树图你可能以前没见过
它是用于层次聚合
所以它是饼图的一种亲戚
但显示每个切片中的内部层次结构
如果你愿意的话，在这个矩形格式中
所以例如
那个上部顶部的棕色方块
看起来代表金属对吧
比如金属出口什么的
对了，我们可以看到具体类型的金属出口的细分
当你想显示你想要的信息层次结构
或者当你试图以层次结构方式显示聚合时
树图可能是那个工具pivot tables
如果你 嗯 如果你是个excel高手
你知道pivot tables是什么
这里的想法是你从表格数据开始
你想要以任意方式对数据进行聚合，并以表格格式显示
例如
你可以在顶部有原始销售数据
并按区域和发货日期进行旋转，以查看按区域随时间销售的情况
所以pivot tables只是以任意方式对表格数据进行聚合的方式
另一个相对较新的功能是kpis
这些用于比较关键值与其目标值
相当直接 对吧，所以只是说嘿
这里有一些指标 我关心的
这里有什么 这里有多少
相对于我们希望它达到的地方
这就是kpis
然后我们有地理空间图表
就像听起来的那样，一张地图
基本上在地图上叠加数据
通常这些圆圈的大小代表该特定位置的数量
对吧，我们这里有一些按州划分的人口
每个州都有一个圆圈
那个圆的大小代表该州的人口，相当直接
但如果你需要根据地理空间信息可视化数据
比如它在视觉上的地理位置
地理空间图表显然是那个选择
最后我们有词云
它们显示数据集中的单词或短语的频率
所以如果你有一本书或什么东西或者一些文本数据
词云将显示不同单词或短语在文本中出现的频率
每个单词或短语的大小表示它在数据中出现的频率
嗯 例如
我们可以有一个周中目的地的数据集
我们可以挖掘目的地字段中的文本，并用词云展示它
你可以看到亚特兰大乔治亚州是排名第一的
休斯顿德克萨斯州达拉斯沃斯堡芝加哥
都差不多排在那里
也很高
这些很可能是机场枢纽，没什么巧合的
但是，我不会再深入探讨太多
虽然我会在诸如...
嗯 一个例子，我曾用过的是查看威廉·莎士比亚的全部作品
并查看其中哪些单词和短语被最常用
这是我在其他课程中用过的方法
所以词云
在Quick Side中可用的最后一种可视化类型
所以回去再复习这些
你知道你应该为那些问题做好准备，比如
基于这种分析
这种数据
哪种可视化是有意义的 知道菜单是什么以及他们应该做什么是好的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/050_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p50 10. Elastic MapReduce (EMR) and Hadoop Overview.ai-zh.srt

```
# 🎬 050_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p50 10. Elastic MapReduce (EMR) and Hadoop Overview

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来，让我们深入探索弹性地图reduce（Elastic MapReduce，简称EMR）的世界
简称
它是一个在AWS上运行的管理Hadoop框架
EC two实例 和实例
这是一个有点混淆的名称
因为MapReduce本身有点过时，它是Hadoop的一部分
而且EMR不仅仅是一个Hadoop集群
是的 它确实包括Hadoop，MapReduce也在其中
但你更经常使用的那些建立在Hadoop之上的技术，比如Spark、HBase、Presto、Flink或Hive
这些技术 EMR还预装了许多其他东西
甚至可能还有一个叫做EMR笔记本的东西
它类似于在EMR集群上运行的Jupyter笔记本
并且它与AWS服务有多个集成点
这对机器学习领域很重要
因为如果你有一个需要被准备、归一化或缩放的大规模数据集
或者你知道的，
在你将其输入算法之前需要进行处理 EMR提供了一个将数据处理负载分布在整个计算机集群上的方法
对于大规模数据集
在集群中并行处理数据
一个EMR集群是EC two实例的集合
每个EC two实例被称为一个节点
每个节点在集群中都有一个角色
称为节点类型 首先，主节点管理集群
通过运行软件组件来协调数据的分发和任务 在集群中的其他节点之间进行处理
它跟踪任务的状态并监控集群的健康状况
每个集群都有一个主节点
你也可以创建一个只有一个主节点的单节点集群
如果你只需要在单个机器上运行的足够多的处理能力
主节点有时也被称为领导节点
我们还有核心节点
这些是运行任务并在Hadoop分布式文件系统（HDFS）上存储数据的节点
或者简称HDFS
多节点集群至少有一个核心节点
然后我们有任务节点
这些是只运行任务但不在HDFS上存储数据的节点
这些是可选的
你不需要它们
但它们是使用spot实例的好方法
因为它们可以根据需要引入和移除集群
当你需要的时候 因为它们可以根据需要引入和移除集群
当你需要的时候
因为它们实际上不会占用你集群的存储空间
因为它们实际上不会与hdfs通信
Hadoop分布式文件系统
因此，你的集群不需要在任务节点上存储任何永久文件
所以它们只用于计算
因此，如果你有大量数据需要处理的突发任务
并且你真的需要它，你可以将任务节点引入你的集群中
并在完成后将其移除
然后你的集群将继续快乐地运行，即使你已经将其移除
有几种使用Earmar集群的方法
在长期运行的集群中有一个临时实例
配置的临时集群将在所有步骤完成后自动终止
所有定义在集群上运行的步骤完成后
你可能运行的步骤包括加载输入数据
处理数据
存储输出结果
然后关闭整个集群
如果你有一个预定义的任务序列，你想要你的集群执行
你可以在任务完成后自动终止集群来节省费用
你也可以创建长期运行的集群
你想直接与上面的应用程序进行交互
完成后手动终止它
这更适合
比如 临时查询
或者对数据集进行实验，你不知道在前期要做什么
而且你没有一个可重复的序列，只想一遍又一遍地运行它
在这种情况下
你想要一个长期运行的集群
并在完成后手动终止它
当你启动一个集群时
你可以选择安装框架和应用程序以满足你的数据处理需求
在那时
一旦你通过任何方式启动集群
你可以直接通过主节点连接到它
通过e C Two并从那里通过终端运行作业
或者你也可以通过控制台提交有序步骤
如果你可以通过aws控制台提前定义那些
无论哪种方式都可以
我们讨论了emr和aws之间的集成点
显然这些都是重要的
例如 它使用EC two
当然 与组成您集群节点的实际底层实例一起工作
它可以与亚马逊vpc一起工作，让您的集群在一个虚拟网络中
您可以将输入数据和输出数据存储在亚马逊s3而不是hdfs中
如果您愿意
你也可以使用亚马逊云观察来监控你集群的性能
并在你集群的每个节点上配置警报
IAM可以用来配置你对集群的权限
云轨迹 将为你集群上的服务请求创建审计轨迹
最后 AWS
数据管道可以用来安排和启动你集群
那些正在运行预定义一系列步骤的集群
让我们再多谈谈EMR上的存储
现在Hadoop的默认存储解决方案是HDFS
这是Hadoop系统
所以我们可以使用HDFS
这是一个Hadoop的分布式可扩展文件系统
它将存储的数据分布在你集群中的每个实例上
并在不同的实例上存储多个数据副本，以确保不会丢失数据
如果单个实例失败
HDFS中的每个文件都作为块存储，默认情况下分布在Hadoop集群中
HDFS中的块大小为128兆字节
一旦你终止你的集群，这种存储就是临时的
那些存储在本地节点上的数据会随之一起
这就是不使用hdfs的原因
然而，这将会快得多
因为不需要通过互联网访问数据
数据处理节点上的数据都是本地的
hadoop内部有很多智能功能
它会尝试优化这些事情
使得处理数据的代码运行在数据存储的相同节点上
所以从性能角度来看，hdfs非常好
hdfs在性能方面非常好
但是，当你关闭你的集群时，它有一个缺点
那个数据会消失
那不是好事
然而 我们有允许你使用s三的erf
仿佛它是个hdfs文件系统
结果发现它仍然相当快
在emrfs中也有一个可选的始终如一的视图，它是为了s三一致性
实际上，你可以使用 DynamoDB 来跟踪跨 MRfs 的一致性
所以，与emr相关的一个关键点是，您可以使用s3代替hdf
那就是关键点
你也可以 当然，如果你想要的话，也可以使用本地文件系统来处理临时文件
但这不会进行分发
所以这只对
例如 在主节点上
当你试图基本准备数据并使其到达目的地时
嗯 我们也可以使用弹性块存储来支持hdfs
所以ebs和hdfs之间有某种关系
emr按小时收费
再加上任何e C Two的隐藏费用
它承诺会
自动为你准备新节点，如果一个核心节点失败
所以你不必担心
至少 你也可以在运行时添加或删除任务节点
就像我们讨论过的
你可以使用spot实例来添加或移除容量
而不影响hdfs文件系统的底层存储
你也可以在运行集群中调整核心节点 你可以为容量规划这样做
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/051_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p51 11. Apache Spark on EMR.ai-zh.srt

```
# 🎬 051_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p51 11. Apache Spark on EMR

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么hadoop到底是什么
它由几个不同的部分组成，我们将讨论这些部分
你emr集群上的其他内容通常建立在这些模块之上
同样 当我们谈论hadoop本身时
我们通常谈论hdfs文件系统
yarn和mapreduce
所有这些背后的基础 所有这些是称为hadoop核心或hadoop common的东西
那是所有这些模块运行所需的库和实用程序
所以它提供了文件系统和操作系统的层级
它需要抽象自己与操作系统之间的东西
以及启动Hadoop本身所需的所有Java归档文件和脚本
在底层之上是HDFS
这是Hadoop分布式文件系统
这是Hadoop的又一个分布式可扩展文件系统
它把存储的数据分布在集群中的实例上
多个数据副本存储在不同的实例上，以确保数据不会丢失
如果单个实例失败
在集群终止时，数据将丢失
然而 但在mapreduce处理过程中缓存中间结果很有用
或者对于随机性较强的工作负载
我 O
在hdfs之上
我们有hadoop yarn
Yarn代表另一个资源谈判者
它是在hadoop2.0中引入的组件
用于集中管理多个数据处理框架的集群资源
这让我们能够使用除了MapReduce之外的其他东西
正如我们很快就会看到的
那么MapReduce是什么
MapReduce是一种软件框架，用于轻松地编写处理大量数据的并行应用程序
在大型商用硬件集群上以可靠且容错的方式运行
这追溯到了谷歌在早期大数据处理中的一些想法
MapReduce由您在代码中编写的映射函数和减少函数组成
一个映射函数
将数据映射到一组键值对 到键值对集
称为中间结果
映射函数通常做这样的事情
转换你的数据或重新格式化或提取你需要的数据
这就是它与探索性数据分析相关的原因
然后我们有减少函数，这些函数结合那些中间结果并应用了额外的算法
并产生最终输出
通常，映射器是转换你的数据并准备它的事情
减少器是聚合数据
并浓缩成你想要的最终答案
然而，这些年
Apache Spark已经基本上取代了MapReduce
多亏了YARN的存在
Spark实际上可以运行在你Hadoop集群之上
并使用Hadoop提供的底层资源调度器和文件系统
但提供了比MapReduce更快的替代方案
所以Spark
可以在你的EMR集群中可选择安装
是一个开源的分布式处理系统，常用于大数据工作负载
现在是非常热门的
它使用内存缓存而不是MapReduce
优化查询执行以实现对任何大小数据的快速分析查询
因此，它使用了称为有向无环图（DAG）的东西
这是它速度的主要技巧
与MapReduce相比 它可以更聪明地处理处理中的依赖关系以及如何更有效地安排这些
Spark提供了Java的API
Scala Python和R
并且它支持跨多个工作负载的代码重用
例如批处理和交互式查询
实时数据分析，机器学习和图处理
它有许多不同的应用场景，我们将讨论流处理
机器学习 交互式SQL
但是，Spark通常不被用于OLTP或批处理作业
它更多用于数据转换，数据进来时
Spark内部是如何工作的
Spark应用程序在集群上作为一个独立的进程集运行
它们由主程序的Spark Context对象协调
这被称为驱动程序
这就是你编写的代码，使你的Spark作业运行起来
Spark上下文连接到不同的集群管理器，这些管理器会在应用程序之间分配资源
所以 例如 Yarn或Spark有自己的内置版本，你也可以使用
如果你不在Hadoop集群上
一旦连接，Spark将在集群节点上获取执行进程
执行进程是运行计算并存储应用程序数据的进程
应用程序代码发送到执行进程
在最后一步
火花上下文将发送任务给执行进行运行
Spark本身有许多不同的部分
就像Hadoop所做的那样，底层的一切都是Spark核心
它作为平台的基础
它负责内存管理、故障恢复等事情
它为Java、Scala提供API
Python 和R 在最低级别
它使用一个叫做弹性分布式数据集（RDD）的东西
或者简称为RDD
它代表了一个逻辑上的数据集合，这些数据分布在不同的计算节点上
正如我们将看到的 在Spark SQL之上有一个层
它是一个分布式查询引擎，提供低延迟的交互式查询
查询速度比MapReduce快100倍
它包括一个基于成本的优化器
支持列式存储 以及用于快速查询的代码生成
并且它支持来自jdbc的各种数据源
ODBC（Open Database Connectivity） JSON和HDFS
Hive orc或parquet文件
如果你想，它也支持使用HiveQL查询Hive表。
但是 关于火花的真正重要是事情
SQL 就是它在 Python 中暴露了一个称为数据帧的东西
或者一个Scala数据集
这正在取代较低级别的位置。
当今的Spark分布式数据集具有弹性
因此现代的Spark代码倾向于以与您在Pandas的数据框或关系数据库的表
相同的方式与数据进行交互
您可以向Spark集群发出SQL命令
在底层 它会找出如何将此转换为分布式查询
它在整个集群上执行
非常有用
我们也有Spark流
这是一个实时解决方案
利用Spark Core快速调度能力进行流数据分析
数据以小批次方式摄入并进行分析
同一批处理分析代码可以应用于这些小批次
这很酷，因为你可以用于批处理编写的相同代码
应用于Spark Streaming的实时流处理
支持从Twitter摄取数据
Kafka Flume
HDFS和ZeroMQ
正如我们将看到的，它还可以与AWS Kinesis集成
我们也有 ml lib，这是Spark上的一个机器学习库
显然这与机器学习考试有关
我们会更深入地讨论这个问题
当然 以及它能做什么
最后我们有GraphX
这是一个基于Spark构建的Drift分布式图处理框架
我们不是在谈论QuickSite这样的图表和图形
我们在谈论的是计算机科学
这里的图 所以
你知道 例如 一个社交网络的人图
这更多是一个数据结构问题
它提供了 ETL和探索性分析
和迭代图计算
以使用户能够交互式地构建和转换大规模图数据结构
所以让我们更多地谈谈ml lib
它提供了几种不同的机器学习算法
但特别的是，它们都以一种方式实现
那就是分布式和可扩展的
并非所有机器学习算法都适合并行处理
它们中的许多都需要重新想象
为了将那个配置分布在整个计算机集群上
因此使用spark ml lib
它有一套特定的功能，可以用于分类任务
它提供了逻辑回归
哪一个是 你知道这是一个伟大的替代方案
朴素贝叶斯
它也可以做回归，跨集群决策树
它内置了一个基于交替最小二乘法的推荐引擎
它内置了一个用于主题建模的K均值聚类实现
如果你想从文本信息中提取主题，它具有LDA
在不监督的情况下，从文档中
它还有多种通用机器学习有用的工作流程实用程序
例如管道和特征转换以及持久性
它还具有SVD的分布式实现
主成分分析和统计函数也很好
我们将在后续的课程中更详细地讨论这些含义
如果你在建模部分对这些内容不熟悉
但是再次强调 这里的特殊之处是它可以在集群上运行
许多这些算法在其默认状态下不能在集群上运行
如果你要在 例如 scikit learn 上运行它们
例如 因此使用 spark ml
Lib 这允许您处理大量数据集，并在整个集群上实际训练机器学习模型
正如我们在课程后面看到的那样
您甚至可以将Spark包含在SageMaker中
Spark Streaming也值得更多的讨论
通常，Spark应用程序在其代码中使用一个数据集来引用您的数据
在Spark Streaming中，这个表就像数据库表一样处理
该表只是随着新数据块在实时接收而不断增长
您可以使用时间窗口查询该数据
例如
你可以查看数据流中过去的一小时数据
就像查询数据库一样
基本上这就是从高层次上
Spark中的结构化流是如何工作的
它模型化流入的流数据
基本上作为一个无界的数据库表，你可以随时查询
Spark流处理与Kinesis集成
你可以有一个Kinesis生产者将数据发布到一个Kinesis数据流中
在KCL中有一种方法可以实现一个Spark数据集
基于你的数据流，你可以像查询任何其他Spark数据集一样查询
另一个关键点在spark中是zeppelin
这基本上是一个笔记本
它允许你在浏览器中的笔记本环境中交互式地运行spark代码
因此你可以对你的spark数据执行sql查询
使用sparksql
你也可以查询你的结果并用图表和图形可视化它们
使用matplotlib和seaborne等工具
这使得spark感觉更像一个数据科学工具 并允许你以数据科学家熟悉的格式预处理你的数据
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/052_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p52 12. EMR Notebooks, Security, and Instance Types.ai-zh.srt

```
# 🎬 052_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p52 12. EMR Notebooks, Security, and Instance Types

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                Emr 建立在此基础上
然而 使用 emr 笔记本
它与 zeppelin 的概念相似
但与 aws 本身有更多集成
因此，笔记本会备份到 s3 中
你可以在笔记本中实际创建整个集群
你可以启动一个 emr 笔记本
启动一个 emr 集群
并从中开始向该集群发送任务
所有这些都在 emr 笔记本中进行
它们安全地托管在 vpc 中
并且只能通过 aws 控制台访问
使用 mr 笔记本可以做的一些事情是构建 apache
Spark 应用并在集群上运行查询
它们支持 python
Pi Spark
Sql Spark R 和 scala
并且预装了 anaconda 流行的开源图形库
以帮助你原型化代码并可视化结果
以及使用 spark 数据框进行探索性数据分析
它们都不在 emr 集群本身中托管
并且笔记本文件备份到 s3 中
这就是你可以从 emr 笔记本启动集群的方式
笔记本本身不在你的集群中
结果，emr 笔记本也允许你组织中的多个用户创建他们自己的笔记本
并将它们附加到共享的多租户 emr 集群
并协同使用 apache spark 进行实验
笔记本对 emr 客户免费提供
让我们谈谈 emr 的安全性
所以与大多数 aws 服务一样
iam 是你的主要工具
iam 可以使用策略授予或拒绝权限
并确定用户可以与 amazon emr 以及可能与之关联的其他 aws 资源执行的操作
你也可以将那些 iam 策略与标记结合使用，以按集群控制访问
Kerberos 是一种通过秘密密钥加密提供强身份验证的另一种方式
它确保密码和其他凭据不以明文形式通过网络发送
不再需要担心
emr 5 及以上版本支持 kerberos
我们现在已经到了 6
所以那已经不是问题了 你还可以使用 ssh（安全外壳协议）
显然这是一种安全的连接到集群实例的命令行方式
它还提供了一种隧道方式以查看 web 界面
那就是 kerberos
应用程序可能被主节点托管
您可以使用kerberos或e c
客户端认证的两对密钥
那里 我可以角色可以被使用
您可以拥有emr服务角色
实例配置文件角色
服务链接角色
无论它是什么，这将控制emr如何能够访问其他aws服务
emr中的每个集群都必须有一个服务角色和一个角色
E c 两个实例配置
现在 附加到那些角色的策略将为集群提供权限，以便于用户与其他aws服务进行交互
代表用户在aws服务之间进行交互
如果你的集群使用自动缩放，那么你还需要一个自动缩放角色
并且服务链接角色可以被使用
如果你的emr服务失去了清理EC two资源的能力
你可能在其他情况下使用iam角色
emr fs请求亚马逊时使用iam角色
S three So if you have emr file system on top of s three
You can use iam rules to control whether the cluster users can access files within emr
Based on users groups
Or the location of the data within s three
Also If you're integrating emr with lake formation for automatically creating your clusters through lake formation
You can specify security configuration as a little json snippet
And give that to lake formation
To use to set up the security for your emr cluster while it's being created
最后，现在有与Apache Ranger的本地集成
在EMR中，Ranger是一个用于Hadoop数据安全的开源工具
如果你想使用它，你可以这样做
它集成了Hive
因此，它可以为您的Hive数据元存储提供安全保护
还可以与EMR中的Hive实例进行交互
你如何选择实例类型呢
主节点或主节点真的不需要太多的处理能力
它只是用来路由事物
因此，您可以使用M4 Large
如果你的集群节点数少于50个
如果你的集群节点数多于50个
你可能想要考虑稍微强大一点的
比如x large
你的核心和任务节点就是你真正需要考虑实例类型的地方
深入思考 你知道什么最适合你在集群上实际执行的任务
这里并没有一个绝对的正确答案
一个通用的解决方案可能是m4 large
像这样的东西是一种比较好的通用解决方案
如果你在做一些对CPU要求很高的事情
但无法通过GPU加速
如果你处于一种情况
不过 如果你的集群在等待外部依赖时
比如网络爬虫之类的
也许你可以选择稍微轻量级的
比如t2 medium
我的意思是这会对你的集群成本产生很大影响
对吧 所以不要过度
不要拥有你不需要的容量
如果你确实需要更多的性能
如果m4 large不够
你可以升级到x large或其他
当然 如果你有计算密集型应用
你可能想要选择CPU优化的实例类型
如果你有数据库或内存缓存应用
你可能想要选择高内存实例
或者你可能有网络消耗很大的应用
比如NLP或跨集群的机器学习应用
对于使用集群计算实例的人
特别是如果你在做AI，使用神经网络
你可能需要一个加速计算实例
比如GPU实例
那就是g3、g4和p2、p3类型
记住还有spot实例可用
这些是任务节点的好选择
你只在核心或主节点上使用spot实例进行测试
或者在非常注重成本的情况下
这种情况下你可能会丢失数据
因为spot实例可能会消失
但是记住，任务节点可以根据需要启动和停止
所以在任务节点上使用spot实例是一个很好的省钱方式
如果你需要根据动态调整容量 如果你需要调整你的容量动态
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/053_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p53 13. Feature Engineering and the Curse of Dimensionality.ai-zh.srt

```
# 🎬 053_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p53 13. Feature Engineering and the Curse of Dimensionality

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们深入机器学习中的特征工程世界
特征工程到底是什么
嗯，基本上，它是将你对数据的了解应用到
某种程度上修剪掉你使用的特征
或者创建新的特征，或者转换你已经有的特征
我提到的特征是什么意思
它们是你的训练数据的属性
就是你用来训练你模型的东西
让我们来看一个例子
所以 假设我们试图预测人们的收入
根据人们的各种属性
在这种情况下，你的特征可能是一个人的年龄
他们的身高 他们的体重
他们的地址 嗯
他们开的车
任何数量的东西，对吧
其中一些东西可能会对你的预测有关键影响
有些它们不会
所以 特征工程的过程在某种程度上就是选择哪些特征对于什么来说是重要的
我正在尝试预测并选择那些特征
明智地
很多时候，你也需要以某种方式转换那些特征
你知道 也许原始数据对你正在使用的特定模型没有用处
也许某些事情需要以某种方式归一化或缩放
或者以某种特定的方式编码
经常你在现实世界中会遇到数据缺失的情况
通常你没有每个数据点的完整数据
你处理这些问题的方式会极大地影响最终模型的质量
你拥有的
有时你也想从现有的特征创建新的特征
也许你现有的数据的数值趋势
你拥有的特征的数值趋势可以通过取其对数更好地表示
或者取其平方或者类似
或者你可能更倾向于将几个特征数学上组合成一个
以减少你的维度
这就是特征工程的全部
你不能只有你所有的数据
并把它扔进这个大机器学习搅拌机，期望有好的结果
另一端 这真的是机器学习的艺术
这是你专业知识应用的地方
实际上得到好的结果
这不仅是一个机械过程，你遵循这些步骤
把所有的数据你扔进这个算法，看看你能做出什么预测
这就是好机器学习实践者和坏者的区别
能够实际进行特征工程的人
是在职场中最成功和最有价值的人
当然
这不是通常教的东西
这主要是通过经验学习的很多东西
实际上在现实世界中进行机器学习实践
这也是为什么这个考试实际上关注很多
机器学习认证考试试图筛选出真正有这方面经验的人
而不是没有经验的人
他们真的在努力
给真正知道他们在做什么的人
以及在这个考试中拥有真实世界经验的人一个很大的优势
我会尽力在这里教你
这不是通常覆盖的内容
但是有一些通用的最佳实践
如果你知道它们 它们肯定会帮助你通过这个考试
特征工程为什么在首先重要
嗯 这是关于维度诅咒
我们指的是什么 嗯
如我所说 你不能把所有特征都扔到机器里并期望好事会发生
太多的特征实际上可能会非常成问题
有几个不同的原因
首先它会导致数据稀疏
再次回到试图在个人属性上训练模型的例子
一个人的属性有几百个
你可以想出正确的
如我们所说 年龄
身高体重 你开的车
你赚多少钱 你住在哪里
你知道的
你去过的大学
清单可以继续下去
你可以想象每个人作为所有这些特征的维度空间的向量
好的 请跟我来
想象 例如
我们只有一个特征，即人的年龄
你可以通过一个单一的年龄轴代表一个人
从零到一百或什么
现在我们添加一个维度
比如他们的身高我们添加一个维度
另一个轴我们向量指向那里编码
他们的年龄在一个轴上，他们的身高在另一个轴上，正确
现在我们有两个维度的向量，再加一个维度
比如说 他们赚多少钱
现在我们有三个维度的向量
一个维度是他们的年龄
一个维度是他们的身高
一个维度是他们赚多少钱
当我们不断增加更多的维度
我们可以工作的空间就在不断爆炸
对嗯
这就是我们所说的连体维度
所以你拥有的特征越多
我们可以找到解决方案的空间就越大
拥有一个大的空间来尝试找到正确的解决方案
只会让找到最优解变得更加困难
所以你拥有的特征越多
你的数据在这个解决方案空间中就变得越稀疏
找到最佳解决方案就越困难
所以你最好把那些特征简化到最重要的那些
这将给你提供较少稀疏的数据，使其更容易找到正确的解决方案
从性能角度来看
想象一下，尝试为所有这些特征创建神经网络，无论以何种方式编码
这将需要一个巨大的神经网络
底部非常宽，可能也非常深
以实际找到这么多特征之间的关系
这将非常难以收敛
任何结果
这就是为什么我们需要减少特征的数量
以便神经网络可以更好地学习
所以机器学习的成功很大一部分不在于选择算法，不在于你知道
不在于清洗你的数据 而在于你最初使用的数据
这就是特征工程所涉及的内容
很大程度上取决于领域知识和你的基本常识
关于什么会起作用，什么不会
以及如何改进你的模式
只是尝试不同的东西，看看什么有效，什么无效
什么有帮助，什么有损害
所以这很大程度上就是来回折腾
这个功能能帮助吗
不 好的 我们不会使用它
这个功能能帮助吗 不
好的 试试别的
现在你不必总是猜测才公平
有一些更原则性的方法来做降维
所以其中一个叫做主成分分析（PCA）
我们会在课程建模部分详细讨论
但PCA是一种方法，可以将所有多维数据
你所有的特征
提炼成一个更小的特征集合
提炼成一个更少的维度
它试图以尽可能保留信息的方式做到这一点
我的意思是
如果你有足够的计算能力，实际上在大量
特征集上使用PCA 这是从实际特征中提炼出真正重要特征的一种更合理的方法
你最终得到的特征
不是你能贴上标签的东西
只是 你知道的 人工创建的特征，它捕捉到了
你最初特征的精髓
K均值聚类是另一种方法
这些方法都很好，因为它们都是无监督的
你不需要对它们进行任何训练
你可以直接将你的特征数据喂给这些算法
它会提炼出一个更小的特征集合，效果一样好
希望效果一样好
但是 更多的特征并不一定更好
至少我们称之为维度诅咒
而这就是我们要做特征工程的主要原因 这也是你在这个过程中要做的主要事情
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/054_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p54 14. Imputing Missing Data.ai-zh.srt

```
# 🎬 054_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p54 14. Imputing Missing Data

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                因此，特征工程的一大部分是缺失数据的插补
当你的数据中有
嗯 缺失数据元素时
这就是现实世界中发生的事情
对于每个观察值
可能会有一些缺失的数据点，这种情况很常见
嗯 一个简单的解决方案就是称为均值替换
这个想法是 如果你有一个行中缺失的特征或属性
只需用整个列的均值来替换它
记住我们在谈论的是列
而不是行 你想要取该特征所有其他观察值的均值
不是从该行的其他特征取均值
对吧 均值替换就是关于取该列的均值
并用该均值替换所有缺失值
这既快又简单
嗯 这些都是这种方法的优点
它也不会影响你整体数据集的均值或样本量
因为你只是用均值替换缺失数据
这不会影响整体数据集的意义
这可以是很好的
但是有一个细节需要注意，如果你的数据集中有很多异常值
这也是你在准备数据时需要处理的问题
你可能发现中位数比均值是更好的选择
假设你有一个数据集是关于很多人的
也许有一个列是收入
有些人不报告他们的收入，因为他们认为这是敏感信息
你知道的 你可能会发现你的数据集中充满了百万富翁或亿万富翁
如果你的数据中有很多异常值 如果你在这种情况下使用均值插补
你可能会得到一个过高或过低的
用于替换的值
如果你有异常值拉高了均值
你可能想要考虑使用中位数
而不是均值 那样会更少受这些异常值的影响
但一般来说
这不适合用于插补
首先 它只适用于列级别
如果你数据集中的其他特征之间有相关性
它不会捕捉到这些相关性
所以 你知道的，如果年龄和收入之间有关联
那种关系将被完全错过
所以你不能说一个10岁的孩子赚一年50,000美元
因为你的数据集的平均值
但这听起来并不合理
我的意思是，一个15岁的孩子还不会赚那么多钱
所以从这个角度来看，这是非常幼稚的方法
另一个问题是你不能用它来处理类别特征
你知道
你怎么做 如何
分类数据的平均值，听起来就不合理
虽然你可以使用出现最频繁的值
你知道，最常看到的类别会是一个合理的做法
在这种情况下 这有点像是均值替换
但整体上并不是同一回事
尽管这不会是一个很准确的方法
这是对缺失值进行填充的一种非常生硬的尝试
虽然它快速且简单
嗯 在实际操作中有一些优势
如果有人问你在认证考试上
数据插补的最佳方法是什么
均值替换
可能不是
也可能不是简单地删除缺失的行
尽管 正如我们所见
有时 如果你有数据足够多，这是一个合理的做法
这样，删除几行数据并不重要
你知道 如果你没有太多包含缺失数据的行
嗯 这听起来并不离谱
另一个方面是，你需要确保删除包含缺失数据的行
不会偏袒你的数据集
如果存在缺失数据的行与那些观察值的其他属性之间有实际关系
该怎么办
例如
让我们假设我们再次看收入
可能会有一种情况，收入非常高或非常低的人
更有可能不报告他们的收入
所以 通过移除或删除所有那些观察值
你将会移除很多收入非常高或非常低的人
这可能会对你最终得到的模型的准确性产生非常坏的影响
所以你需要确保如果你要删除数据
那么它不会以某种方式偏袒数据集
正确 所以这是一件非常快速和简单的事情
这可能是最快的也是最简单的事情
你可以在Python中一行代码中真正做到这一点
但这可能永远不会是最好的方法
再次 如果考试问你
填充缺失数据的最佳方式是什么
删除数据可能不是正确答案
几乎任何事情都会更好
也许你可以用类似的字段进行替换
正确 我是说，这也是一种简单的方法
例如，我可能有一个数据集
嗯 客户对电影的评论
如果我有一个评论摘要和一个完整的文本评论
把评论摘要复制到全文会更有意义
比如那些没有填写全文的人
所以任何都比直接丢弃数据要好
但在现实中
如果你只是想快速且粗糙地做点什么，就像
开始尝试一些数据
只是开始玩它
这可能是一个合理的事情
我只是不会为了生产目的把它留在原地
在生产出你可能真正想做的事情之一是使用机器学习本身
为了填充你的机器学习训练中缺失的数据
这是一种种元东西
有不同的方法可以做
这种方法叫做kn，代表k最近的邻居
如果你有任何机器学习的经验
你可能已经知道那是什么
基本想法是找到k
其中k是一些与你正在查看的行最相似的行的数量
这些行有缺失的数据
然后将这些最相似行的值平均起来
你可以想象每行之间有一种距离度量
可能是每行之间的归一化特征的欧几里得距离
或者类似的东西
如果你找到说
与缺失数据最相似的前十行
你可以从这些最相似的十行中取该特征的平均值
并用该值进行填充
这样可以利用数据集中其他特征之间的关系
这是一件好事
不过，这种方法有一个问题
那就是它假设你正在处理的是数值数据，而不是类别数据
对于类别数据，取平均值很困难
但可以使用汉明距离等技术来解决这个问题
但是 kn 通常更适合数值数据
而不是分类数据
如果你有分类数据
你可能更应该开发一个深度学习模型
神经网络在分类问题上表现很好
所以想法是实际上构建一个机器学习模型
来为你的机器学习模型填充数据
这有点像一个循环
这对分类数据效果很好
现在深度学习很难
然而 当然这很复杂
涉及大量的代码和调整
嗯
但很难超越结果
如果你有一个深度学习模型，试图预测缺失的特征基于你数据集中的其他特征
这将会做很多工作
大量的计算努力
但它会给你最好的结果
但是，这将会消耗大量的计算努力，但它会给你最好的结果
你也可以对你的数据集中的其他特征进行多重回归
这也是一个完全合理的事情
通过回归 你可以找到缺失特征与你数据集中的其他特征之间的线性或非线性关系
以及数据集中的其他特征之间的线性或非线性关系
在这些方向上有一个非常先进的技术叫做mice
它代表多重插补由链式方程组成的插补
它是这个领域的前沿技术
目前对于插补缺失数据的
所以你知道
如果你看到一个问题，问如何有效地处理缺失数据
其中一个答案是mice
有很大的可能性，这可能是一个合理的答案
实际上，我在考试中没有看到这样的问题
但如果我是出题者
我可能会把这个问题放上去
最后，处理缺失数据的最佳方式就是获取更多的数据
如果你有很多行数据存在缺失
也许你只需要更努力地获取更完整的数据
从人们那里获取更多的真实数据是困难的
这样你就可以不再担心那些缺失数据的行
你要小心如果你删除数据
你可能以某种方式偏袒你的数据集
但处理数据不足的最好方法是获取更多
有时候你必须回去找出数据来自哪里
并收集更多更好的质量数据
你系统输入的更好质量数据
你将得到更好的结果
虽然填补技术是掩盖问题的一种方式
当你没有数据并且无法获取更多时
总是一个好主意，获取更多的和更好的数据 如果你可以
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/055_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p55 15. Dealing with Unbalanced Data.ai-zh.srt

```
# 🎬 055_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p55 15. Dealing with Unbalanced Data

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在特征工程领域，另一个问题是处理不平衡数据
你指的是什么
那就是我们在正例和负例之间存在巨大的差异
在我们的训练数据中
一个常见的例子是在欺诈检测领域
实际的欺诈是非常罕见的
没错 因此，你的训练数据中大部分都是未欺诈的记录
这可能导致难以构建一个能够识别欺诈的模型
因为它只有很少的数据点可以学习
与所有非欺诈的数据点相比
所以模型很容易说
好的 嗯
由于欺诈实际上只发生在零点
一的时间
我就预测每次都不是欺诈
而且嘿 现在我的准确率棒极了
正确 所以如果你有一个不平衡的数据集
你可能会陷入这样的境地
你有一个机器学习模型
看起来准确率很高
但它每次都只是猜测
这不会有帮助对吧
所以在特征工程中有处理这个问题的方法
首先，不要让术语混淆你
这在一开始让我困惑了很久
当我说正例和反例
我不是在谈论好和坏
所以不要将正负与正负结果混淆
正仅仅意味着这
我正在测试的东西是发生了什么
所以这可能是欺诈
所以如果我如果我的模型试图检测欺诈
那么欺诈是正例
尽管欺诈是一件非常负面的事情
记住正只是您试图检测的东西无论什么
那对你来说是如此显而易见
因为如果你继续混淆积极和消极与
像道德判断
这与此无关
在这个语境中
这主要是一个神经网络存在的问题
所以你知道这是一个真正的问题
如果你有一个像这样不平衡的数据集
它可能不会学习正确的东西
我们必须处理这个问题 不知怎么的，处理它的一种方式就是
抽样是一种简单的解决方案
所以只要从你的少数类中取样
你知道 在这个欺诈的例子中
只取更多已知为欺诈的样本
然后复制它们一遍又一遍
你知道 制造克隆大军
如果你愿意的话，你的欺诈测试用例
你可以随机这样做
你可能会认为这不会有帮助
但在神经网络中它会起作用
这是一个非常简单的事情
你可以这样做 通过复制少数群体的其他样本来制造更多的少数群体案例
另一种方法是欠采样
而不是创建更多的少数群体案例
嗯 移除多数群体的
所以，在欺诈的情况下
我们会谈论去除一些非欺诈的案例
以平衡它多一点
然而 丢弃数据通常不是正确答案
我的意思是 你为什么想要这样做
你正在丢弃信息，对吧
所以，只有你在特别尝试避免你的训练中的缩放问题时，欠采样才有意义
如果
正确 也许你的数据量超过了你硬件的处理能力
如果你有太多数据无法精细处理
丢弃一些大多数情况下的数据可能是一个合理的做法
但更好的解决方案是获取更多的计算能力
然后在集群上扩展这个数据
欠采样通常不是最好的方法
比欠采样或过采样更好的方法是称为smote的方法
而且 这可能是你会看到的
它代表一种合成的少数群体过采样技术
一种创造性的缩写
它做的事情是，通过使用最近邻的方法，人工生成少数类新样本
就像我们之前讨论的，用knn进行缺失值填充一样
同样的想法，我们在少数类每个样本上运行k最近邻
然后，我们从这些knn结果中创建新的样本，通过这些邻近的平均值
而不是只是
你知道的 简单地将少数类其他测试用例复制
我们实际上是根据其他样本的平均值创建新的
并且以这种方式制造它们效果相当好
所以它既能生成新样本又能减少样本
多数多数类
这很好 这比仅仅通过复制来过度采样要好
因为它实际上是制造了一些基于现实的新数据点
所以记住
如果你处理的是不平衡的数据
SMOTE是一个很好的选择
一个更简单的方法是调整阈值
当你实际上在做推断时
并且将模型应用到你有的数据上
当你在做分类预测时
比如欺诈或不欺诈
你会有一个概率阈值，当你说
好的 这可能是欺诈
你知道 大多数机器学习模型并不直接输出欺诈或不欺诈
它实际上会给你一个欺诈或不欺诈的概率
并且你必须选择一个概率阈值，当你说
好的 这可能是欺诈
它值得一些调查
如果你有太多的假阳性
一种修复方法是只是增加那个阈值 是的
那样肯定会降低你的假阳性率 但这以更多的假阴性为代价
所以在你做这样的事情之前
你必须考虑那个阈值会产生的影响
所以
如果我提高我的阈值 这意味着我将会有更少的实际被标记为欺诈的事情
这可能意味着我会错过一些实际的欺诈交易
但我不会打扰我的客户那么多
说嘿
我认为这是欺诈 我关闭了你的信用卡
你可能实际上想要相反的效果
正确
也许我想要更宽松地标记欺诈 所以我我会降低那个阈值来实际上标记更多的欺诈案例
可能这是一个你更好猜错的情况
如果不是欺诈
反之亦然
所以你需要考虑假阳性和假阴性的成本 并且根据你的选择阈值
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/056_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p56 16. Handling Outliers.ai-zh.srt

```
# 🎬 056_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p56 16. Handling Outliers

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在特征工程领域，另一个问题是如何处理数据集中的异常值
你如何处理它们 你是如何处理它们的
你是如何识别它们的
在我们谈论异常值之前
我们需要一些数学背景
别担心 这不难
让我们从方差的概念开始
你如何测量方差
我们通常称之为sigma平方
你会很快知道为什么这么叫
但目前只需知道方差是均方差
要计算数据集的方差
首先计算其均值
假设我有一些数据，它可以代表任何东西
让我们说，某个小时排队的最大人数
第一个小时我观察到有1人在排队
然后4，5，4，8
好的
计算方差的第一步是找到均值
数据的平均值
我只需将它们相加
除以数据点的数量
结果是4.4
在这个例子中，平均排队人数为4.4
现在 下一步是找出每个数据点与均值的差异
我知道均值是4.4
第一个数据点是4.4，数据值为1
所以1-4.4是负数 3.4
4-4.4是负数
0.4
以此类推
所以，我得到了一些正负数，表示每个数据点与均值的差异
好的
但我想要的是一个单一的数字，代表整个数据集的方差
所以下一步是找出平方差异
我只需逐一计算这些与均值的原始差异的平方值 有几个原因
首先，我想要确保负值与正值一样重要
否则它们会相互抵消，这将是不好的
我也想要给出异常值更多的权重
这样放大了与均值差异较大的值的影响，同时要确保正负差异的可比性
all
while
still
making
sure
因为负数的平方是一个正数对吧
当你平方一个数字时，你总是得到一个正数
所以让我们看看当我这样做时发生了什么
所以负三
三点四的平方 那是一个正数
十一点六 负点四的平方是一个更小的数字，点一六
因为那更接近于平均值四点
只有点三六那里
但当我们上升到正离群值时
三点六最终会变成十二点九六
为了找到实际的方差值
我们只需要取所有数据点与均值的平方差的平均值
把所有那些平方差相加，然后除以五
我们拥有的数值数量
最后我们得到一个总方差为五点零四
这就是方差的全部
让我们继续讨论标准差
我们通常谈论标准差而不是方差
而标准差实际上是方差的平方根
所以我有一个方差五点零四
标准差是二点二四
所以你现在知道为什么我们称方差为sigma的平方
因为sigma本身代表标准差
如果我对sigma的平方取平方根
我得到sigma
最后例子中结果是二点二四 现在我们看到这是实际数据的直方图
我们正在研究的
我们看到数字四在我们的数据集中出现了两次
然后我们有一个一
一五和一八
现在
标准差通常用作一种方法 来思考如何识别你数据集中的离群值
如果我在均值四四的一倍标准差范围内
这被认为是正态分布中的一种典型值
但你可以看到在这个例子中
数字一和八实际上超出了那个范围
如果我取四点四加减二点二四
我们得出结论
一和八都落在标准差的范围之外
所以我们可以说数学上
一和八是离群值
我们不需要猜测，也不需要用眼睛看
我们可以给那个下定一个数学基础
现在仍然存在一种判断
关于你认为什么是离群值
在标准差上，它远离平均值的程度
所以你可以大致谈论一个数据点或异常值
通过它远离平均值的标准差
这就是你在现实生活中会看到的标准差用法
你可能定义你的异常值为远离平均值的一个标准差
或者远离平均值的两个标准差
这是你在做特征工程时需要作出的判断
所以有时候，在你识别出异常值后，适当地移除它们是合适的
有时候，这不合适，确保你做出这个决定是负责任的
所以例如 如果我在做协同过滤
并且我正在尝试提供电影推荐或者类似的东西
你可能有几个超级用户已经看过并评价了所有电影
他们可能会对每个人的推荐产生过大的影响
通常 你不想让几个人在你的机器学习模型中有那么大的影响力
这可能是一个例子
这可能是一个合理的事情来过滤掉异常值
并且通过他们实际放入系统中的评价数量来识别他们
或者可能是一个异常值是那些没有足够评分的人
或者你可能正在查看的
网络博客数据 其中异常值可能告诉你你的数据中有些地方出错了
首先 这可能是恶意流量
或者可能是机器人或其他应该被丢弃的代理
它们不表示你试图建模的实际人类
但是如果某人真地想知道美国的平均收入而不是中位数
他们特别想要平均值
你不应该只因为你不喜欢就把你国家里的所有亿万富翁都排除在外
因为你不喜欢他们
事实是，数十亿美元将推动平均数上升
即使它不会影响中位数
所以，不要通过排除离群值来扭曲你的数字
但是，如果它是与你最初试图建模的内容不一致，那么就要排除离群值
那么，我们是如何识别离群值的呢
嗯
记住我们的老朋友
标准差 我们前几页已经讲过了
它是用于检测异常值的非常有用的工具
并且找到数据集的标准差是非常严谨的
如果你看到一个超出一个或两个标准差的数据点
那么你就有一个异常值
记得我们之前讨论的箱线图
现在也有内置的方法来检测和可视化异常值
那些定义异常值为超出四分位距的1.5倍
四分位距
那么你选择多少倍
你基本上得用常识来判断
没有固定的规则来判断什么是离群值
你得看你的数据
大致的目测 它
看看分布情况
看看直方图
看看是否有明显的离群值
在理解它们之前
你只是把他们扔掉
也是在aws的背景下
aws s有自己的异常值算法，称为随机切割森林
并且它正在逐渐进入他们越来越多的服务中
你可以在quick site中找到随机切割森林
Kinesis 分析
Sagemaker和更多，如果你谈论的是异常值和异常值检测
亚马逊似乎对他们的随机切割森林算法感到非常自豪
所以有很大可能性
这是他们在考试中寻找的
如果他们在异常检测的背景下提到随机切割森林
让我们看一个关于收入不平等的例子
那么我们假设我们正在建模
嗯 收入分布
我们只是看看每个人每年赚多少钱
我们试图理解这整个数据集的含义
所以在这个简单的例子中再次
考试中不会给你Python代码
但这只是为了让你们感同身受
我们在这里设置了一个收入正态分布，中心点为27000美元
并在其中添加了一个亿万富翁
并绘制了该分布的直方图
你们可以看到，这一个亿万富翁严重扭曲了我们的分布
所以所有的人
所有的普通人
所有的非亿万富翁都被挤在一起，形成一个小线段，位于左边
我们的亿万富翁，那个数据点你甚至看不到，严重扭曲了我们的数据
极大地扭曲了我们的数据
如果我们计算这个的平均值
我们会得到一个非常离谱的数字
这里写了一个非常简单的异常值检测算法
它只计算整个数据集的中位数
计算标准差
如果任何值超出两个标准差之外
它就会被排除
如果我们调用这个排除异常值的函数
然后绘制直方图
通过排除异常值，我们得到了更有意义的数据
所以现在我们也有了更有意义的平均值
这更接近我们开始时的二万七千美元
这就是一个例子，让你思考
你的异常值来自哪里
你真的应该把那位亿万富翁排除在外吗
这对你试图实现的商业成果真的有什么影响 归根结底
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/058_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p58 18. Amazon SageMaker Ground Truth and Label Generation.ai-zh.srt

```
# 🎬 058_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p58 18. Amazon SageMaker Ground Truth and Label Generation

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们最后谈谈sagemaker的ground truth服务
这是从亚马逊推出的一个相对较新产品
那么它是什么呢
它基本上是一种利用人类来标记你的数据的方式
我在课程中很难决定把它放在哪里
最后我决定把它放在特征工程中
因为从根本上讲，它涉及到处理缺失数据
在这种情况下，它更多地是关于缺失标签和缺失特征，而不是经常
但你可以 你知道
实际上也可以将其应用于缺失特征
基本上，如果你有缺失数据
这是人类ground truth可以轻松推断的
让你将这些测试外包给实际的人类
为你填充这些缺失数据
这在图像分类的世界中是最常见的例子
例如 如果我在训练一个新的图像分类模型
为了训练它
有人必须去实际标记所有这些训练图像
它们实际上包含什么
这未必是电脑容易做的事情
例如 在这张照片中
我们看到有人将这张照片分类为篮球或足球比赛
有人必须去做所有这些工作
这是什么样的鸟
这张图片中的鸟在哪里
诸如此类
他们会去标记你的数据以供培训目的
如果你有大数据集
比如一堆图片并且你需要给它们打标签
有时人类是最好的方法来获取这些标签
ground truth就是管理这个过程的工具
但它不仅仅是管理标记数据的人类
ground truth与众不同的地方在于，它会在它接收到标签时创建自己的模式
所以当它接收到更多的人类标签时，这个模式会学习
随着时间的推移，它只会向人类发送它不确定的标签
随着时间的推移
随着它从人类标记者接收到越来越多的标签
它会创建自己的模式
并且只有模糊的案例才会被发送给人类标记者
所以这个模式会随着时间的推移而建立
越来越好
只有模型不确定的案例才会被发送给人类
这最终会减少标签工作的成本高达70%
这相当可观
这些人是谁
嗯 你可以选择不同的人来作为事实核查者
嗯 一种选择是使用亚马逊机械土耳其劳动力
这是一个全球范围内的庞大劳动力
他们会标记你的数据并做很多其他事情
你知道的 为你执行一些简单的任务，费用非常低
你也可以选择将其外包给你的内部团队
如果你处理的是非常敏感的数据，这可能对你来说很有意义
而且外面也有专业的标记公司，他们唯一的工作就是雇佣一些人来标记训练数据以维持生计
如果你想要一个在这方面更专业的人，你可以花更多的钱使用一个
还有其他技术，除了使用真人来生成训练标签，你还可以使用aws的图像识别服务
这是一个图像识别服务，我们在课程后面会详细讨论
基本上它是一个图像识别服务
如果你需要对图像进行分类，这可能是为了创建标签或者甚至是特征数据
那么这可能是为了创建标签或者甚至是特征数据
你可能只是那样
我在你的数据集中发现了一个特征
在这张图像识别中，有一个预训练的模型
对于大多数常见的对象类型，你可以直接使用
所以如果你只是使用
你知道
世界上物体的一般分类可能能够为你做到这一点
如果你处理的是文本信息
与图片相比，理解可能更有用
基本上那就是一个aws服务
你正在从事文本分析和主题建模
因此你可以自动为一篇给定的文档生成主题或情感
这可能是创建标签和甚至额外的训练特征的一种方式
这就是特征工程的另一个例子
你可能通过使用Comprehend来生成模型中的新特征
如果我有一篇文档
我可能会使用Comprehend来生成新特征
这些特征可能包括文档的主题或该文档所代表的情感
这可能对训练机器学习模型有用
这就是为什么它在特征工程的这一部分
你知道 你可以实际应用这些技术，利用现有服务甚至人类来生成更多信息
这些信息你的模式可以使用
任何预训练模型或无监督学习技术
可以帮助生成新的培训标签甚至新的训练特征在你的数据集
所以真相是很简单的
但如果对你来说仍然太难
有一种东西叫做真相加
有人可以为你做所有的事情
这是一个一键式解决方案
你基本上就是在亚马逊上雇佣一个人
来为你设置和管理整个项目
在文档中他们称其为我们的AWS专家团队
我不太清楚他们是从哪里来的
但这些人会管理工作流程并为你设置一切
还会管理你的标签团队
你所需要做的就是填写一个表格
说明你是谁以及你的目标
然后他们会联系你并试图讨论细节和价格
他们并没有公开披露价格
但我想象这肯定不便宜
因为他们会提供这种白手套服务
在他们设置一切时
当他们的标签团队开始产生结果时
你可以通过ground truth plus项目门户跟踪他们的进度
在那里他们会给你一些有用的图表
显示进度和结果
当他们完成时
你会从S3收到你的标签数据
但这里没什么好说的
基本上
这是将数据标签问题外包给他人的一种方式 ground truth plus 如果你愿意支付
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/059_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p59 19. Lab Preparing Data for TF-IDF with Spark and EMR Studio, Part 1.ai-zh.srt

```
# 🎬 059_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p59 19. Lab Preparing Data for TF-IDF with Spark and EMR Studio, Part 1

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                因此，在这个实验中，我们将练习如何大规模准备数据
在 Apache Spark 上
在一个集群中 我们将在准备数据以供tf使用的上下文中进行操作。
IDF算法
你可能不知道tf idf是什么
那么让我们从覆盖tf idf代表术语频率和逆文档频率开始
基本上它在搜索算法中被使用
基本上来说 这是找出一个给定术语对于一个给定文档的相关性的好方法。
所以，如果你能计算每个术语的tf-idf分数
在一个文档语料库中
你可以使用这个来确定哪个文档与该搜索术语最相关听起来很复杂
但实际上它听起来比它实际要简单得多术语频率
或tf只是衡量一个词在一个文档中出现的频率
这就是术语频率
就像它听起来的那样 一个在一个给定文档中出现频率很高的词很可能对这个文档的意义非常重要
例如 一个包含单词或短语的文档
机器学习很多可能是关于机器学习
文档频率只是该词在整个文档集中出现的频率
例如 也许你在查看整个维基百科
或者被搜索引擎索引的每一网页
文档频率会告诉你该词在所有文档中的多少
这往往告诉你一些常见词汇
比如a和the和一些在所有文档中都常见的词
但它们对特定文档并不特别相关
它们只是常见的词
通过将这两者结合起来
文档频率中的术语频率
你可以确定一个给定术语对一个给定文档的重要性
你可以通过取术语频率来测量相关性
并将其除以文档频率
事实证明，除法在数学上与乘法相乘相同
这就是为什么我们称其为tf-idf
在数学上，这与术语频率相乘
逆文档频率
我们只是取单词在文档中出现的频率
把它除以它出现的频率
这将给你一个衡量那个词对于给定文档的特殊性的指标
现在，在实际操作中 我们实际使用这个方法的时候有一些微妙之处
例如 我们使用实际逆文档频率的对数而不是原始值
这是因为在实际中，单词频率通常呈指数分布
所以通过取对数
我们最终得到了一个稍微更好的单词权重
根据它们的总体流行度
And there are some general limitations to tf idf
One is that we just treat everything as a bag of words
Every document is just a collection of words
And we don't really pay attention to the relationships between the words so much obviously
That's not always the case
And it turns out that parsing out those words is a good part of the work
As we'll soon see
Also What do you do about things like synonyms
And tenses of words and abbreviations and misspellings
We'll see in our example here
That we'll at least deal with capitalization by making everything lowercase
But there are more complicated things you could do to try to make this work better
Also doing this at scale becomes challenging
Mapping all these terms and documents together into numbers that tie together is kind of hard to do
And that's where apache spark is going to come in to help
We talked a little bit about the bag of words approach one way around
That limitation is to look at not just unigrams or individual words in a document
But also things like bigrams or more generally n grams
So look at words that occur together
For example If i had a document that just contained the phrase
I love certification
Exams I could just look at the unigrams and consider the terms to be the individual words
I love certification and exams
But i could also look at bigrams and consider those terms as well
That would be every grouping of two words that appears together
So for example the bigrams in the sentence are i love love certification and certification exam
That is every unique combination of two words that appear in that document
I could go further to trigrams
And those would be i love certification and love certification exams
I bring this up Because one of the sample questions that aws provides you for the certification exam
Gets into the guts of all this
So here's an example of what a tf idf matrix might look like
That's considering both unigrams and bigrams
Let's imagine that we have two documents in our entire corpus here
One document only contains the sentence i love certification exams
And the other document just contains the sentence i love puppies
This is what that matrix might look like
When we're actually computing tf idf for every individual term across every individual document
The documents appear on the rows of the matrix and each individual term appears on the columns
So since we're looking at both unigrams and bigrams
We start by taking each individual unique word that appears in this in this corpus as individual terms
So that consists of the words i love
Certification exams and puppies
Since the words i and love appear in both documents
Those aren't counted twice
Those are you know deduplicated there
我们只考虑矩阵中的单词
因此，一个术语只能在一个列中出现一次
然后我们也会看到大写字母
文档集中的唯一性大写字母是我爱，爱认证，爱小狗和认证考试
再次
因此它们将出现多次
tf-idf的任务是填充这个矩阵
并计算每个文档中每个术语的tf-idf值
再次，那些术语可以是单词或大写字母，例如
在这个例子中
这个矩阵的维度是2x9
我们有两行对应两个文档，九列对应九个唯一的术语
这些术语包括唯一的单词和大写字母
所以如何使用tf-idf
你可以使用它创建一个简单的搜索算法
你可以提前计算tf-idf
然后对给定搜索词按tf-idf分数排序文档 然后显示结果
所以我们现在来尝试一下
我们将使用tf-idf处理维基百科数据集
我们将使用aws，elastic mapreduce和apache spark构建自己的搜索引擎
我们的主要目标是提前处理数据
因为，这就是我们要讨论的 我们将在aws上使用elastic mapreduce和apache spark
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/060_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p60 20. Lab Preparing Data for TF-IDF with Spark and EMR Studio, Part 2.ai-zh.srt

```
# 🎬 060_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p60 20. Lab Preparing Data for TF-IDF with Spark and EMR Studio, Part 2

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                好的
所以对于这个活动 我们将重新创建维基百科
如果你愿意，我们将使用我们从真实维基百科中提取的一小部分数据
但是，我们不会使用一些深度神经网络来搜索数据，保持时尚
我们将使用传统的方法，使用词频-逆文档频率
取而代之
为了使其更符合本课程的内容
我们将在Elastic MapReduce上构建它
使用Apache Spark在EMR工作区运行
这将有一个emr笔记本
所有运行在emmr studio实例中
那么我们开始吧
我已经在我的aws控制台中分配了
这里有一个小注解
不要在你的根账户下这样做
如果你能避免 我在emr studio中遇到了一个特定的根账户的bug
假设它还在那里
最好用特定的iam用户登录
如果你可以转到你的设备主页并前往emr
如果你之前没有去过那里
那么我们继续使用emr启动一个小型笔记本实例
我们可以用它来尝试pi spark
实际上使用一些
嗯 一些真实的维基百科数据
现在要困难得多
以前你必须打开一个隧道
以查看直接在emr服务器上运行的笔记本
在某个集群中，最近
这要容易得多 你可以直接进入emr studio，点击创建studio
你可以看到它已经预填充了一个交互式工作负载
这正是我们所需要的 我们希望在一个笔记本环境中交互式地使用pi spark
所以这些默认设置将没问题
此外，它将使用emr无服务器应用
因此我们不必担心自己设置emr集群
无论 现在都是无服务器了
这样生活就简单多了
现在 确保您已登录为iam用户
而不是以您的主账户登录
这样做时
有很多技巧
这涉及到底层操作，以确保您的用户具有适当的权限
在iam中以正确的方式运行无服务器应用程序
这做不到
如果你是root用户
如果你被指定为root用户
注销 创建一个iam用户
如果你必须这样做，并使用该用户登录
我是用户而不是
好的 让我们创建一个工作室并启动工作区
就这么简单
让我们在这里改变一些事情
做一些事情 让我们感到有点有生产力
至少改变名称
所以工作室的名称默认是工作室和1分
让我们叫它别的什么
我不知道 tf-dash-idf
在现实世界中
这将是你组织或团队的共享环境
在实际意义上
这可能与你的工作组相关联的名称
你知道 你正在与之合作的人
将使用工作室的组织
记住emr工作室具有协作功能
因此多个人可以同时工作
所以在现实世界中这可能不是一个很好的名称
但对于我们做的小小玩具示例
使用tf-idf这是有意义的
我们将允许它创建自己的存储桶as3
我们将允许它创建自己的service roll
这将节省我们大量时间而不是与iam纠缠
我们还将设置一个工作区设置
让我们改变那个
我们将其命名为tf-idf
缺乏更好的想法
工作区基本上是笔记本的新名称
所以这将包括
您实际运行的jupyter笔记本的环境
这将附接到emr serverless应用程序上
所以这里有一种层次结构
所以在底部我们有一个emr serverless应用程序
实际上正在运行我们的代码并运行spark
在上面，那是由一个工作区包裹的
让我们与我们在emr serverless应用程序上交互的笔记本
在上面，我们有一个实际的工作室，包括这个工作区
这里没多少需要改变的
我将保留默认的应用程序名称
你不能改变这些
所以我将保留它们
它们就是这样 它是无服务器的
所以它有预初始化的容量
但它在理论上有能力根据需要增长并扩大其能力
如果你想有更多的控制
这里有一个自定义选项当你创建工作室时
这将给你对确切的
你想要使用的权限更精细的控制
你想要使用的确切最佳三个桶
你可以甚至创建自己的EC
从零开始创建两个集群
如果你不想使用emr serverless
那么这也可以
但这又是一大堆工作
你不需要现在处理 你也不需要为考试这样做
当然 这里只是一些概念
让我们继续创建那个工作室并发起一个工作空间
不应该花太多时间
因此，现在工作坊正在为这个工作室实例启动
这将涵盖我们的笔记本
所以我们只需等待它在新浏览器标签页中打开
再次，这不应该花太多时间
我们开始了
我们将上传我已经给你过的笔记本
现在 你可以点击这里选择pi spark
然后开始从零开始编写东西
但我不会强迫你这么做
因为考试并不希望你真的去写代码
所以，让我们只说这个小上传文件
点击此处并导航到你的课程材料
你应该找到一个tf idf ip n d文件
让我们继续上传那个
我们将覆盖与这个工作区一起提供的默认一个。
现在我们应该能够双击它并打开它
好吧 所以你可以看到我们有啊
这里选择的是派火花内核
但还没有集群被附加
这是因为它还在启动EMR的过程中
无服务器实例 所以我们给它一点时间
你可以看到它现在处于附加无服务器交互式应用程序的状态
而且那很快
它现在要我们选择一个内核，我们将使用pi spark
因为这是用于允许我们做spark的笔记本所编写的
而不仅仅是python
所以让我们选择pi spark
你可以看到它还在连接中
我们不能使用这个笔记本，直到它完成
我们只需要等待一会儿，直到它完成
不应该花太长时间
它说已经连接，非常酷，pi spark
我们现在应该可以开始了
我们需要处理的一件事是
确保我们的工作区可以访问我们的数据
我们需要将我们的数据放入我们有权限访问的s3桶中
因为我们没有为这个工作区创建自定义配置的麻烦
我们没有为特定的s3桶设置明确的权限
为了简化生活
我将把那些数据放入自动创建的桶中
作为快速开始的一部分
要找出那个桶的位置
你需要回到emr studio
选择我们刚刚创建的studio
然后点击工作存储
那就是它具有访问权限的地方
如果你希望添加另一个桶
你想要将东西放进去或者获取访问权限
然后你就必须处理所有事情
确保你的VPC能够访问外部互联网以访问S3服务
确保你的iam权限有正确的权限来访问那个桶等等
因此等等 但是再次
我们将作弊并使事情变得更容易，只需使用那个桶
我们知道有效 所以，我们现在在我们的桶里，我们只是点击上传
我们将去添加文件
如果你直接去你的课程材料
你应该能找到一个小点的Tsv文件
我们上传那里
让我们点击那里
从那里我们可以复制我们需要从脚本中访问的URI
让我们复制回来我们的笔记本
我们只是替换这里的字符串
用你刚刚复制的
让我们重启这个内核
只是为了清除我在最初运行时产生的所有其他东西来做那件事
我打算去内核并说
重启内核并清除所有输出并重启
那可能需要一点时间
你可以看到底部
它说 重启
给它一点时间来完成
这也有一个很好的副作用，那就是迫使无服务器应用
嗯 做需要做的事情来启动
老实说 这可能有点麻烦
有时候我发现了几步故障排除步骤
所以如果你到目前为止遇到了一些麻烦
如果你遇到了一些权限错误弹出
这可能是因为你是以root用户运行而不是以iam用户运行
它无法自动将您需要的权限附加到您的用户帐户
如果你是root用户
所以确保你是以iam用户运行
有时候这纯粹就不起作用
你知道我遇到过很多情况，我开始运行这段代码
它说正在运行
但什么也没发生
我一点反馈都没有，它立即就终止了
看起来启动这个无服务器集群有点不稳定
有时候你只能试几次，直到你得到一个实际上能工作的实例
我能
你知道我现在对aws的可靠性有点评论
但我会省掉这一点
你知道有时候你必须开始
我试了几次，得到了一个工作得还不错的实例
看起来像这样 嗯
内核终于重启了
这花了相当多的时间，令人惊讶
让我们确保它实际上能工作
所以我们将从维基百科读取一些原始数据开始
我们有这个子集-小tsv文件，我已经将其加载到S三中，以便您可以访问它
所以，我们所要做的就是使用Spark读取它
然后调用show来查看结果数据框中的内容
让我们运行第一个块，看看情况如何
看起来一切正常，这是个好兆头 执行已经开始启动Spark应用程序
如果您是第一次这样做，可能需要几分钟才能启动
所以我们再给它一点时间再次运行
如果你没有看到
不过 如果没有任何输出
你可能有一个坏的无服务器集群
所以重新开始，直到你得到一个能工作的
不幸的是 在那方面我没有更好的建议
不管怎样 这个正在运行
那么我们就等它完成吧
开始加载数据
这很酷
是的 我们回来了 他们没有花太多时间
但是很酷 嗯
实际上我们加载了那个数据集
这看起来怎么样
你可以看到我们没有任何列名
这是我们必须解决的第一个问题
说这些是什么 但显然这些都是某种标识符和一些文章标题
显然这个数据集从无政府主义开始，然后是自闭症和阿尔贝多
然后到a
然后是文章的日期和文章的文本本身
所以这是一个很有趣的数据集，显示前20行
我们需要首先给那些列起一个名字
这样我们可以更直观地引用它们
我们将加载我们刚刚从csv文件加载的原始数据
并将其加载到spark数据框中
并使用id，标题，时间和文档的特定列名
并显示结果
再次按下shift运行
现在我们已经加载了spark
它运行得更快
你可以看到我们有相同的数据
但是有人类可读的列名
那是进步
现在我们需要对我们的数据进行清理
所以首先让我们探索它
让我们在这里进行一些探索性数据分析
我们知道tf-idf无法处理空文档
所以首先确保我们没有任何空文档
spark的语法只是调用filter命令
在这里的操作是document字段是否为null
我们希望计算有多少个
从高层次来看，这条线说的是
在文章数据框中计算有多少文档有null值
答案是
只有一个
实际上有一个文档
所以我们检查了
我们必须去掉它
如果你深入挖掘
我现在不会做
因为我在意你的时间
但这是一个腐败的记录
所以我们只是将其删除
你知道有一个大问题
如何处理数据分析中缺失的数据
但当你谈论一行时
可能明智的做法就是删除它
所以我们将运行一个过滤器，只选择非空的文档
然后计算有多少是空的
只是为了确认在那之后没有留下任何空值
并确保不是空操作
我们做了我们想做的
并只给了我们一个新的数据框，名为cleaned articles，包含非空文档
让我们开始
它返回零
表明我们的新cleaned articles数据框中没有空文档
好 那是进步
接下来我们需要处理的是tf idf模型本身需要数字
而不是单词 所以它需要我们将这些文章进行分词，转换为数字形式
我们需要为每个数据集中的唯一单词创建一个映射
将其映射到唯一的数字
在我们能够运行任何有趣的算法之前，我们必须这样做
我们将开始对这些文章进行分词，将它们分割成单词
然后将它们存储在一个稀疏向量中
这是每个文章中单词的数值表示
我不会在这里深入研究代码
但我们将使用称为tokenizer的东西
以及pi spark机器学习库中的hashing tf
来处理这一点
首先我们对数据进行分词
这仅仅是找出哪些单词是唯一的
找出唯一单词的列表
然后 我们将调用这个哈希函数，将这些唯一单词与数值哈希值关联起来
并展示我们最终得到的结果
在这里的每个单元格中按下shift enter
随着我们前进
走spark
走 好的，我们有了
你可以在这里看到，对于每个文档id
我们有一个单词列表，后面跟着这些单词的数值表示
非常酷
看起来它起作用了 它还为我们计算了词频
通过计算每个文章中每个哈希词的出现频率
没有深入研究它返回的结构
这些信息确实包括了每个文章的词频
非常酷
然而，我们希望为每个文档中的每个术语计算tf-idf分数
所以我们只完成了一半
但这并不难
我们还可以使用Python中的spark机器学习库中的idf功能
这将完成这一部分
所以我们在这里创建一个idf模型
嗯 并输入上面的原始特征
最后创建一个新的特征列
其中包含每个单个特征的tf-idf分数
我们开始吧
然后我们会显示结果
好的 现在我们有一个特征列
以及一个包含tf idf分数的raw features列，而不仅仅是tf
所以让我们利用这些信息来做一个搜索
所以假设我们要搜索术语gettysburg
那么
这比你想象的在spark中要难
但这是实现它的一个方式
所以，我们又需要使用数字
而不是文字 所以我不能说
首先搜索葛底斯堡
我需要计算葛底斯堡的哈希值
和上面我们使用的结构一样
我需要计算上面使用的模型中的哈希值
以计算实际术语葛底斯堡的分数
这就是这段大块的代码所做的
基本上 它说的是我想创建一个只包含葛底斯堡一词的数据框
只包含这一个特征
我将使用相同的哈希tf进行转换
并对其进行转换
然后收集与该术语相关的实际哈希值
运行所有这些代码
我们应该得到一个数字
这个数字是116,775
一千七百七十五
所以显然，这是代表盖兹堡的魔法数字
我想强调的是，理解Spark的代码对于考试来说并不重要
这就是我为什么略过那里的原因
我不想让你的大脑被不需要的东西塞满
但重要的是，我们有盖兹堡的哈希值
现在我们需要做的就是提取盖兹堡的TF-IDF值
对于每个文档
之前我们提取了每个术语和每个文档的分数
但我们只关心盖兹堡
所以让我们用这个UDF函数从我们的数据框中提取它
基本上吸出这些并创建一个新的数据框
那只是关于葛底斯堡的
TF-IDF分数启动
现在我们有了我们的新数据框
那只是葛底斯堡的TF-IDF分数
如果你遇到错误 那就再试几次
如果你只是在单台超大实例上运行
这在某种程度上是边缘的
这里我们有足够的资源
有时候这会失败，你就得再试一次
Spark在这方面可能会有点非确定性
但我们现在几乎完成了
我们所有的文档都在这里，带有葛底斯堡的分数
与之关联 现在我们只需要按葛底斯堡的分数对它们进行排序
我们已经可以看到亚伯拉罕·林肯在那里突出了，对吧
让我们按葛底斯堡的分数来排序
并打印出结果
我们将每列截断到100个字符
只是为了确保我们能读它
这就是我们搜索词gettysburg的结果
我们得到了关于亚伯拉罕·林肯、阿伯纳·道布尔迪的文章
谁是内战人物
美国内战本身
德克萨斯州奥斯汀市
然后我们开始有点模糊
我们得到了亚述的结果
但是最终结果确实很有意义
所以TF-IDF似乎起作用了
但真正酷的是，我们是从零开始做的
所以我们只是从维基百科中获取了原始的数据集
我们清理了数据，创建了自己的tf idf模型
然后我们将这个模型应用到实际的搜索中，用于查找一个术语
并返回有意义的结果，所有都在emr上进行，使用emr workspace
运行一个emr笔记本
在由emr集群支持的emr studio实例中运行pi spark
所以非常酷，我们完成了
让我们清理我们的混乱 尽管
所以让我们继续关闭这个，回到emr studio
现在，这里有很多资源我们需要处理掉
为了确保我们不会在账单上遇到意外费用
你知道 老实说，aws在这方面变得越来越狡猾
最近 他们让它变得太容易离开
你不使用的东西在运行
在像sagemaker studio这样的工具中
你可能在这个月底发现自己手里拿着一张一千美元的钞票
如果你不小心
所以让我们变得偏执
让我们从访问这个工作区的底层emr服务器less应用程序开始
我们会在管理应用程序下找到它
在那里我们可以看到我们使用的交互式服务器less应用程序
让我们删除它
所以我会说
停止应用程序
等待它停止
在它完成之前无法删除
现在停止后
我们将删除它
好的，下一个 我们需要删除使用该无服务器应用程序的工作区
它在工作区部分
我们选择并删除
正在删除
最后，我们可以回到工作室本身
我被踢到了一个不同的界面
这没有显示给你
所以你必须回到主emr屏幕这里，然后回到工作室
那里是工作室
选择它并删除它
好的，现在 三还有一些剩余的东西，还有一个iam角色
你可能想删除它 只是为了清理东西
但这不会让你花太多钱
所以我不会详细说明
但请确保清理你的垃圾
与工作室本身和底层工作空间一起
无服务器EMR应用
为了确保你的账单上不会有任何意外
月底
这就是了
我们已经使用emr studio来创建一个交互式笔记本，我们用它来进行操作。
实际上实现了tf-idf，并从头开始做了一点维基百科搜索 如此酷炫
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/061_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p61 01. Section Intro Modeling.ai-zh.srt

```
# 🎬 061_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p61 01. Section Intro Modeling

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们将深入探讨这门课程的最复杂部分
这也是考试中权重最重的领域——建模
在这一部分，我们将最终进行机器学习，在我们收集
分析并准备我们的训练数据之后
深度学习已经接管了机器学习领域
我们将从深度神经网络的工作原理开始，进行一个快速课程
以及它们的一些常见变体
比如卷积神经网络和循环神经网络
然后我们将深入探讨这些网络的调优细节，这些细节并不总是被教授
我们如何识别并解决过拟合问题
我们如何决定我们的网络的拓扑结构和深度
我们如何优化它们的性能
这些现实世界的关注点在考试中受到了严格的测试
这让很多人感到惊讶
但你会为此做好准备
这仍然是一个aws考试
所以你可以期待被深度测试aws s的机器学习服务
这意味着我们将谈论很多关于aws sagemaker
它提供的广泛内置算法
以及使用sagemaker进行自动模型调优的方法
我们还将涵盖aws
我们将涵盖更高级别的服务
包括comprehend translate
Poly 转录
Lex Deep lens
和更多
这次考试也需要深入了解如何评估你的训练结果
何时查看准确性
精确率 召回率 或F1分数
它们是如何计算的 它们是如何解释的
我们将深入探讨所有这些
并确保你了解它们就像你的后背一样
我们将以实际的卷积神经网络为背景，结束这一节
进行一次实践操作
我们将评估其结果，使用各种正则化技术来改进它
并尝试不同超参数的效果
这样你就能感受到它们在现实中是如何工作的
最终，这就是你在这次认证中被测试的内容 让我们开始吧 这里有很多内容要覆盖
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/062_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p62 02. Introduction to Deep Learning.ai-zh.srt

```
# 🎬 062_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p62 02. Introduction to Deep Learning

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以我们在谈论机器学习时不能不谈论深度学习
我的意思是，这在该领域是最热门的
现在 你不需要在深度学习内部进行大量的深度
对于考试 但我们至少需要识别出那里存在的不同类型的神经网络
以及如何在aws中最佳部署它们
以便从大数据中学习
让我们深入探讨总体情况
这真的很令人惊叹
整个人工智能领域都是基于对我们大脑工作原理的理解
经过数百万年的进化
自然已经找到了一种让我们思考的方法
如果我们只是反向工程我们大脑的工作方式
我们可以获得一些关于如何让机器思考的见解
具体来说，在你的大脑中，特别是在你的大脑皮层中
那是你所有思维发生的地方
你有一堆神经元
它们是单个神经细胞
它们通过轴突和树突相互连接
你可以将它们视为连接线
如果愿意，它们连接不同的轴突
单个神经元会向所连接的神经元发送信号
当足够多的输入信号激活单个神经元时
这是一个非常简单的机制
你有一个神经元，有许多输入信号进入它
如果这些输入信号达到一定阈值
它将向它连接的神经元发送一组信号
但是当你开始有许多
许多这些神经元以许多 许多不同的方式相互连接，并且每个连接的强度都不同
事情变得非常复杂 这是一个完美的涌现行为示例
你有一个非常简单的概念
一个非常简单的模型
但当你将它们堆叠在一起时
你可以创建出复杂的行为，从而产生学习行为
这实际上可行
这不仅在你大脑中可行，也在我们的计算机中可行
想想你的大脑规模
你有数十亿个神经元
每个都有数千个连接
这就是创造人类思维所需的
这是我们在深度学习和人工智能领域仍然只能梦想的规模
但这是同一基本概念，你只是有一堆神经元
但在深度学习和人工智能领域，这是我们仍然只能梦想的规模
但它是同一基本概念，你只是有一堆神经元
但在深度学习和人工智能领域，这是我们仍然只能梦想的规模
但它是同一基本概念，你只是有一堆神经元
一堆相互连接的组件，每个单独的行为非常简单
但是一旦你拥有很多这样的组件，并以复杂的方式将它们连接在一起
你可以创建非常复杂的思想和甚至意识
你大脑的可塑性主要是调整
这些连接的去向
以及每个连接的强度
这就是所有魔法发生的地方
此外 如果我们深入研究你大脑的生物学
你可以看到你的皮质中
神经元似乎排列成堆或皮质柱，这些柱在并行处理信息
所以 例如
在你的视觉皮质中
你看到的不同部分可能被并行处理
由不同的柱或皮质柱的神经元处理
每个柱都是由大约一百个神经元组成的小柱组成的
许多小柱然后组织成更大的超柱
在你的皮质中有大约一千万个这些小柱
所以再次强调，它们真的会迅速增加
巧合的是 这与你电脑的三维视频卡工作架构相似
它包含许多非常简单
非常小的处理单元，负责计算
屏幕上小像素组的计算
它正好是一个非常有用的建筑，以模仿你的大脑工作方式
所以它是一种幸运的巧合
你最喜欢的视频游戏背后的研究，正好是使人工智能成为可能的同一项技术
在宏大的规模和低成本上
同样的gpu
在你用来玩视频游戏的同一张视频卡上
也可以用于进行深度学习和创建人工神经网络
想想如果我们实际上制造专门用于模拟人工神经网络的芯片
那会有多好
嗯
有些人正在现在就设计这样的芯片
在你看到这段视频的时候，它们甚至可能成为现实 我想谷歌正在开发一个
深度神经网络是如何工作的呢
我们已经把那些受到你大脑生物学启发的想法，翻译成了人工神经元
老实说，现在这些天
人工智能领域的研究和开发
已经从生物学基础中脱离，我们正在改进我们创造的人工神经元
现在，人工神经元仍然像生物神经元一样工作
它只是汇总来自下一层的加权输入
并应用某种激活函数
现在我们在AI领域的研究和开发，已经从生物学基础中脱离，我们正在改进我们创造的人工神经元
现在，人工神经元仍然像生物神经元一样工作
它只是汇总来自下一层的加权输入
并应用某种激活函数
并且将其结果传递给上一层
你可以认为这是另一种机器学习模型
你在神经网络的底部输入特征数据或属性
预测标签从顶部输出
这通常是某种分类
网络使用已知标签的数据进行训练，这些标签可能是热编码的
就像我们在特征工程部分讨论的那样
在训练过程中
它计算出每个神经元之间的理想权重，以在顶部获得正确的答案
你看到每个神经元之间的连线连接了一切
有很多这样的 每一条线都有一个与之关联的重量
你看，还有一个偏差项也被添加进去了
深度神经网络的任务是学习合适的权重
以及偏差
以生成你想要的最终结果
现在 训练的细节对于考试并不重要
但你只需要知道这叫深度学习
因为这里有多层神经元
所以我们只有一層在那裡
那將不會是一個深層神經網絡
但是當你有多層時
那就是我們談論深學習的地方
現在讓我們談談我們如何讓這些東西成為現實
以及你可能使用的框架你可能會使用神經網絡它們自己非常適合並行化
所以這是一個很好的事情這意味著我們可以使用gpu來做這件事
因為個別神經元簡單到足以在gpu上模擬
並且gpu是用於在大規模平行處理
最初用於一次生成你電腦屏幕上的所有像素
但是我们可以利用同样的技术一次性执行整个神经网络
因此，GPU能够并行处理大量的神经元
你也可以在一个计算节点上拥有多个GPU
你可以在一个集群中有许多节点
因此你可以无限地扩大这个规模并构建一些巨大的东西
甚至可能是大脑尺度
某天
然而 我们需要某种框架，从编程角度来看使用这个东西
实际上定义我们要设置的网络
在他们被派往你的GPU进行培训之前
一个非常受欢迎的选择是TensorFlow
这是由谷歌开发的
它还集成了一个更高级的API叫做Keras，我们将在这里查看
这是实际的Keras代码片段
这是在设置一个神经网络
类似于我们刚刚查看的那个
它基本上有一个包含64个神经元的密集层，输入维度为20
我们正在输入20个输入神经元
这可能是20个不同类别的热编码类别
例如 为了正则化编写一个dropout层
我们稍后会讨论
中间有一个神经元层
有64个神经元，使用名为relu的激活函数
我们也会讨论另一个dropout层
最后有一个输出层，在最后有10个输出分类神经元
使用softmax选择其中一个
我们还定义了优化器函数
在这种情况下称为sgd
并且我们编译了模型
从那以后我们可以训练它并预测
除了tensorflow和keras
还有称为apache mxnet的东西
这是非常相似的东西
它基本上做同样的事情
但它不是谷歌制作的
它是由apache制作的
我不知道
也许这与
为什么亚马逊倾向于使用mxnet而不是tensorflow有关
你会发现aws会支持两者
但大多数亚马逊自己的深度学习产品
它们倾向于基于mxnet构建
这基本上是tensorflow的替代品
有三种主要的神经网络在野外
一种是基本的前馈神经网络
就是我们刚刚看过的
你有一堆堆叠的神经元层
你在底部输入特征
预测和分类输出在顶部
这是最简单的类型
也有卷积神经网络
或cnns 简称
它们通常用于图像分类
因为它们设计用于处理二维数据
如果你需要找出图像中有停车标志
你可能需要使用cnn
也有循环神经网络，称为rnns
它们通常用于处理某种序列
时间序列，例如 预测股票价格随时间变化
或者任何有顺序的东西
例如
如果你想要理解句子中的单词 并且那些单词的顺序很重要
你可能需要使用rnn来捕捉这些关系
并预测如何完成句子
在机器翻译中，你可能需要使用rnn来理解句子中的单词顺序
或者如何将一句话从一个语言翻译到另一个语言
在这个领域里几个关键的缩写词是lstm，它代表长
短期记忆和gru
一种带门的循环单元
这些基本上是rnns的不同版本，它们工作得很好
如果你看到这些术语
lstm或gru
记住这些只是rnns的类型 让我们深入探讨cnns和rns
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/063_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p63 03. Activation Functions.ai-zh.srt

```
# 🎬 063_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p63 03. Activation Functions

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们之前提到过激活函数
但让我们深入探讨一下，因为它们是一个重要的概念
原来有许多不同类型的激活函数可以选择
选择正确的一个对于神经网络的性能可能是重要的
现在，激活函数就是在一个给定的节点上或神经元内
无论你叫什么
汇总所有进入该神经元的输入，并决定应该发送什么输出
到下一层的神经元
它就是那个说
给定输入 我的输出应该是什么
这就是激活函数的全部，现在最简单的激活函数
被称为线性激活函数
它只是做输出
输出它得到的输入
这只是一条直线
线性激活函数的问题是它实际上并没有做什么
它只是输出输入的内容
因此它不能进行任何有趣的学习
它不能进行反向传播
因为它实际上是通过网络学习和尝试优化事情
也因为它只是输出任何它作为输入的东西
在线性激活函数的情况下，拥有多个层真的没有任何意义
如果它只是转回来并将一层的输入
作为下一层的输出
真的没有必要拥有多于一层的情况
因为它将始终是一样的
因此，线性激活函数实际上并不很有用
你真的很少看到这些在行动
线性激活函数的近亲是二进制阶跃函数
这一切只是说
如果我没有输入神经元任何东西，我将不会输出任何东西
但如果有任何东西输入
我将会输出一个正值
这是一个固定值
所以它是开或关，这个问题是
有几个问题 首先，它无法处理多分类
它是一个二进制函数
它只能开或关
这只能表明这个神经元代表一个事物
所以如果你想说一个给定的事物属于多个类别
没有一种方法能用二进制步函数来实现
它只能说这个属于单一类别
但更大的问题是这涉及到数学和微积分
当我们通过这个网络学习时
我们必须对这些激活函数做很多微分
当我们这样做时
结果发现垂直斜率和微积分真的不能共存
这个的导数是无穷大
这就是让你的数学崩溃的原因
这就是为什么 在这个特定的例子中
你看到阶跃函数并不是真正的阶跃
它在旁边 有一点偏移
它有一点倾斜
这就是为了不让数学爆炸
当你发现自己做这些事情只是为了不让数学爆炸
这可能是自然告诉你你正在构建的东西不够稳定
所以再次
二进制阶跃函数概念上非常简单
它有点像他们最初开发神经网络时开始的地方
但他们很快发现它们非常有限并且有数学问题
所以你今天很少看到它们被使用
我们想要专注于非线性激活函数
非线性函数的好处是
你可以创建这些复杂的输入和输出之间的映射
这样可以传递更多的信息从一层到另一层
因为它们有一个有用的导数
它们不仅仅是 你知道 直线是水平的和垂直的
这使得你可以做更有趣的学习当你向后传播和学习如何优化这些层之间的权重
你的网络
他们也允许有多个层次
因为它们不是只是退化的线性函数输出输入的任何内容
因为它们更复杂
实际上你可以从多个层次中受益于你的神经网络
非线性是关键
你可能从像这样的功能开始
在上面我们有一个称为sigmoid激活函数
也称为logistic激活函数相同的东西
在底部我们有一个称为tan h
或双曲正切激活函数再次
两个不同的名字相同的东西
这些的好处是我们谈论过的
它们很平滑 他们有漂亮的导数
唯一的真正区别是在上面的sigmoid函数
逻辑激活函数将输出缩放到0和1
而双曲正切函数将一切缩放到负一到正一
一般而言
当你处理机器学习时
有一个均值为零的东西是很好的
所以在实践中 你会发现双曲正切函数是这两个函数中的选择
你不太会看到sigmoid或逻辑函数被使用
你会看到双曲正切函数
在这个环境中
而且他们也非常适合递归神经网络
但它们有一些问题
尽管 所以，一个问题是，当你接近输入的极端正负值时
输出开始变化得很慢
所以，如果你想象这些图形向左或向右延伸得更远
这些值的变化变得非常非常小
这就是我们所说的梯度消失问题
实际上，到了一定点，数值精度问题可能会成为一个问题
我们将在后续课程中详细讨论梯度消失问题
但这就是它的来源
这是用Sigmoid函数之一的缺点
逻辑正切或双曲正切激活函数
它们计算成本也很高
计算机在快速做三角函数方面并不擅长
如果你使用复杂的激活函数，如这个
这会导致你的神经网络收敛时间更长
并且真正进行学习
解决这个问题的方法被称为修正线性单元
或简称为relu
这是目前非常受欢迎的选择
正如你所看到的，非常简单
这只是在正方向上一条直线
而在负方向上则是一条平坦的零线
它或多或少与二进制阶跃函数有很多共同之处
只是它不是一个阶跃
这种函数的主要优点是它很容易也很快就能计算出来
所以计算机在计算直线方面非常高效
与三角函数相比
所以使用像这样的简单函数让模型收敛非常快
这是一件好事
这里也没有垂直线
所以我们不需要有任何
你知道 奇怪的微积分问题不会导致垂直梯度及其导致的无穷大
但也有一些缺点
尽管它是流行的选择，并且非常快速和高效
那就是当我们有零或负值时
我们会退化到线性函数这里
并且随之而来的所有问题
这被称为死亡relu问题
在很多情况下这并不是一个问题
但在某些模型中这可能确实会造成麻烦
解决死亡relu问题的方法就是简单地不让它
在负方面上变平
而是让它在那里以某种浅的角度倾斜
相反，这在x轴下方引入了一个小的负斜率
通常它不会那么陡峭
通常它会更接近于水平
但这就是leaky relu的想法
而不是左边一条笔直的平直的水平线
我们稍微放慢了一点
但你怎么知道什么斜率是最好的呢
你可以任意决定
有些人就是这样做的
但更好的方法是一个叫做parametric relu或preluu的东西
这与leaky relu完全相同
但在负部分斜率是通过反向传播自己学习的
所以我们回去
学习我们神经网络节点之间的权重
我们也在学习relu激活函数的负部分的最佳斜率
缺点 当然
这非常复杂且计算量大
所以我们放弃了relu的一些优点
效果可能因人而异
不同的问题会受益更多或更少
所以你会发现在大多数情况下
直接连接relu或恒定的leaky relu会满足需求
但是如果你需要做得更好
参数化relu或prelu值得探索
还有其他的ray lu变体
在外面一个叫做指数线性单元的这里显示在上面
我们的elu
这是你知道的像leaky relu
但是在负侧实际上是在做一个指数函数而不是直线
所以再次
微积分倾向于这些平滑的曲线
所以有时候这可能比leaky relu更好
在这里底部，我们也有接近swish的东西
这是谷歌开发的一个功能，它表现非常好
这是一个relu或leaky relu或其他东西的良好替代品
大多数情况下，您在swish中看到好处
如果您有非常深的网络
您需要拥有40层或更多层，才能真正开始看到swish的好处
不过，swish是由谷歌开发的，而不是亚马逊
因此，您不太可能在亚马逊考试中看到这一点 但了解这一点总是好的
但是了解这一点总是好的
另一种变体被称为max out
这是一个非常简单的功能
它只输出进入该节点的所有输入的最大值
从技术上讲，ray lose是这种情况的特例
从数学角度来看
然而，max out会加倍你需要训练的参数数量
因此，在实践中并不实用
这将非常昂贵
你知道，通常不值得为此付出努力
我们还需要讨论的另一种激活函数是softmax
并且你经常看到这种作为多分类问题的最终输出层
所以如果你有一个one hot编码的分类
比如 你知道这张图片里有什么物体
诸如此类 经常
你会看到softmax应用到神经网络最终输出层的激活函数上
而softmax做的事情是
它把那些最后一层的输出转换为每个分类的概率
这使得我们能够将这个神经网络的输出转换为概率
那就是试图对某事进行分类
并解释那些最终概率输出，那就是事物属于每个分类的概率
如果你需要选择
你知道哪个分类是最好的猜测
你可以只从这个输出的集合中选择最大的一个
另一个事情是，你知道它不能对某事产生多于一个标签 尽管如此
如果你有一个情况，你想要说这个图片包含这个东西和这个东西
仍然如此 所以
而且这东西它真做不了那件事
它会选择一个或另一个
啊，sigmoid激活函数
然而 对于多标签的东西，sigmoid是好的
你知道多分类
好的 所以这是一个重要的区别
这是sigmoid的另一个好处
你不必担心softmax函数的实际机制
为了考试 你需要知道它是用来做什么的
它是用于多分类
以及将神经网络的输出转换为每个神经元表示的给定分类的概率
所以这是很多激活函数
你如何选择一个呢
这不难
真的 如果你要做多分类
你需要使用softmax激活函数
当你试图为一个对象选择一个分类时
你想在输出层使用softmax
这就是softmax的作用
如果你有一个循环神经网络
这些通常使用tanh或双曲正切激活函数表现良好
对于所有其他情况
你可能想从relu开始
如果relu不够好
试试leaky relu
如果leaky relu不够好
尝试使用预激活
甚至可能最大化
如果你有一个非常深的神经网络，有四十层或更多
你可能想考虑使用swish
也要记住，sigmoid激活函数适用于
如果你需要为同一事物分配多个分类
就是这样 这就是所有激活函数的作用
或者大部分我们没覆盖的
这些都是主要的 虽然它们是流行的
以及如何选择适合特定问题的激活函数
这可能会出现在考试中 所以确保你理解这一点
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/064_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p64 04. Convolutional Neural Networks.ai-zh.srt

```
# 🎬 064_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p64 04. Convolutional Neural Networks

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们更深入地探讨CNN的第一个方面
通常你听到关于CNN的是与图像分析相关的
它们的整个目的是在你数据中找到东西
这可能不是你预期的地方
从技术上讲，我们将这称为特征位置不变
这意味着如果你在数据中寻找某种模式或特征
但你不知道它可能在数据中的确切位置
CNN可以扫描你的数据，无论它在哪里，为你找到那些模式
例如 在这个图片中
这个停止标志可能在图片中的任何地方
但CNN能够找到停止标志
无论它在哪里
这不仅限于图像分析
它也可以用于任何你不知道具体特征可能在数据中位置的问题
机器翻译和自然语言处理就来到这一点
你不一定知道你关心的名词、动词或短语可能在你分析的段落或句子中的位置
但CNN可以找到它并提取它
情感分析可能是CNN的另一个应用
你可能不知道表示快乐情感或沮丧情感的短语可能在哪里
但CNN可以扫描你的数据并提取它
你将看到它的想法并不像它听起来那么复杂
这是人们使用复杂的词汇使事情听起来比它们实际上更复杂的另一个例子
我应该指出，在使用CNN进行语言和情感分析时，存在一些更合适的变体
例如，我们称之为注意
因此，在这些特定类型的问题中，确实很重要
但你确实需要跟踪它
但那不是太重要的细节
所以CNN是如何工作的
CNN，卷积神经网络，又受到视觉皮层的生物启发
它们从你的大脑如何处理眼睛接收到的图像中吸取灵感
这又是一个令人着迷的涌现行为例子
你的眼睛工作方式是，一组神经元服务于你视野的特定部分
我们称这些为局部感受野
它们是一组神经元，响应于你看到的部分 它们从你视网膜接收的图像中子采样
并且有专门处理你看到视野特定部分的神经元群
现在，这些局部感受野来自每个局部感受野的小区域相互重叠，以覆盖你整个视觉场
这些区域来自每个局部感受野的小区域相互重叠，以覆盖你整个视觉场
现在 这些小区域来自每个局部感受野的小区域相互重叠，以覆盖你整个视觉场
这就是所谓的卷积
卷积只是一个华丽的说法，意思是
我将这个数据分割成小块，分别处理这些块
然后系统会组装
一个更大的画面，就是你在高层看到的
它在你大脑中的工作方式是，你有很多层
就像深度神经网络识别各种特征的复杂性一样
如果你愿意
从你的卷积神经网络进入的第一层
在你的头脑中可能只识别出水平线或不同角度的线
或者特定的边缘
我们称这些为滤波器
它们会传递到上一层
那一层会组装它识别的低层线条成形状
也许上面还有一层
它会根据你看到的形状模式识别物体
我们有一个层次结构来检测线条和边缘
然后从线条中识别形状
再从形状中识别物体
如果你处理的是彩色图像
我们必须将一切都乘以三
因为你的视网膜中有专门检测红色
绿色 和蓝色光的细胞
它们会单独处理，然后在后期组装
这就是所有CNN做的事情
它只是取一个来源图像或任何类型的数据
真正将其分割成小块，称为卷积
然后组装它们，寻找越来越高级的模式
在你神经网络的更高层次
你的大脑是如何知道你看到的是停止信号的
嗯 让我们用更通俗的语言来谈谈
如我们所说 你有独立的局部感受野，负责处理你看到的特定部分
这些局部感受野扫描你的图像
它们彼此重叠
寻找边缘
你可能会注意到你的大脑对世界看到的对比和边缘非常敏感
那些往往会吸引你的注意
是的 这就是幻灯片上的字母吸引你的注意
因为字母与白色背景之间的对比度很高
在很低的层次上，你捕捉到了停止信号的边缘
以及停止信号字母的边缘
现在 更高一层可能会识别出停止信号的形状
那一层说 哦
那里是一个八边形，对我有特殊的意义
或者这些字母组成了stop这个词
这对我来说也意味着特别的东西
最终这会与你大脑中对停止标志的分类模式进行匹配
所以无论哪个感受野在某一层捕捉到了停止标志
它都会被识别为一个停止标志
而且因为你在处理彩色数据
你也可以使用停止标志是红色的信息
并且进一步利用这一点来帮助识别这个物体的真实身份
所以某处在你的头脑中
有一个神经网络会说嘿
如果我看到边缘排列成八边形图案，里面有很多红色
并且说在中间停下来
这意味着我可能应该把我的车刹车踩到底
在某个更高的层面上，你的大脑在进行更高级的推理
那就是发生了什么
有一种模式在说嘿
这里即将出现一个停车标志
我最好在我车上踩刹车
如果你已经开了足够长的时间
你已经不再认真考虑它了
你 感觉好像已经内置
而且这实际上可能是事实
卷积神经网络和人工卷积神经网络工作
以相同的方式 这是同一个想法
那么如何使用keras和tensorflow构建cnn
嗯 显然你可能不想在很低的层次上做
即使你可以cnns可以变得非常复杂
因此，像keras这样的高级API变得至关重要
首先 你需要确保你的数据源具有适当的尺寸
以及你将要保存的适当形状
图像的实际二维结构，如果你处理的是图像数据
你的数据形状可能是宽度乘以
长度乘以 颜色通道的数量
如果是黑白图像
只有一个颜色通道表示黑白之间的一些灰度值
图像中的每一个点
我们可以用一个值来表示这个点
但如果是彩色图像
就会有三个颜色通道
一个代表红色 一个代表绿色
一个代表蓝色
因为可以通过组合红色、绿色和蓝色来创建任何颜色
红色、绿色和蓝色可以组合在一起
在keras中有一些专门的图层类型可以使用
当你处理卷积神经网络时
例如 有一种称为comm two d的层类型，它会在2D图像上执行实际的卷积
这是一种2D图像 再次，卷积只是将图像分解成重叠的小子区域，以便进行单独的处理
每个子区域都会进行独立的处理
同样，conv one d和conv three d层也是可用的
你不必只使用图像与CNN
正如我们所说，它也可以用于文本数据
例如 这可能是一维数据的一个例子
comm three d层也是可用的
如果你处理的是三维某种类型的三维数据
所以有很多可能性
keras中用于CNN的另一种特殊层是max pooling two d
同样，它也有一维和三维的变体
这个想法就是减少你的数据大小
它只取图像中给定块中的最大值，并将其减少到层数
只保留最大值
这是为了缩小图像的一种方式
这样可以减少CNN的处理负担
正如你所看到的，处理CNN是一个非常耗时的操作
你能做的越少，就越好
你越少做 工作
所以 如果你图像中的数据比你需要的多，max
池化 Two D层可以有助于将其浓缩为
你分析所需的基本要素
最后，你需要将数据喂入一个平坦的神经元层
在某个时候，它只是进入一个感知器
在这一阶段，我们需要将2D层展平为一维层
以便我们可以将其传递给一个神经元层
从那一点 它就像任何其他前馈神经网络或多层感知机
CNN的魔法真的发生在较低层次
它最终被转换为
看起来与我们之前使用的多层感知机相同的类型
魔法发生在
实际处理你的数据
卷积和将其减少到可管理的东西
使用CNN进行图像处理的典型用法看起来像这样
你可能从comm two d层开始
它实际上对你的图像数据进行卷积
你可能在之上添加一个max pooling two d层
这将图像缩小
只是减少了你需要处理的数据量
你可能然后在其之上添加一个dropout层
这仅仅用于防止过拟合
到那时 你可能需要添加一个扁平层
以便能够将数据喂给感知机
这就是密集层发挥作用的地方
在keras中，密集层实际上就是一个感知机
它实际上是一个神经元的隐藏层
从那里我们可能会再进行一次dropout，以进一步防止过拟合
最后进行softmax以选择神经网络输出的最终分类
如我所说 CNNs计算量很大
它们对CPU的要求很高
对GPU和内存要求也很高
处理和卷积所有数据的开销非常快，除此之外
还有许多我们称之为超参数的东西
有很多可以调整的旋钮和开关
因此你可以调整CNN
除了可以调整神经网络的拓扑结构之外，你还可以调整优化器、损失函数和激活函数
你还可以选择核大小
那就是你真正卷积的区域
你有多少层
你有多少个单元
你有多少池化
当你缩小图像时
这里有很多选择
配置CNN的可能性几乎无穷无尽
但往往获取用于训练CNN的数据是最难的部分
例如
如果你拥有一辆特斯拉
它会拍摄你周围世界的图像
以及你周围的道路、路标和交通信号灯 以及你每天晚上开车时
它会将这些图像发送到某个数据服务器
以便特斯拉可以基于这些数据运行其神经网络的训练
如果你在晚上驾驶特斯拉时突然刹车
这些信息将发送到某个大型数据中心
特斯拉将处理这些数据并说
这里有模式可以被学习吗？
我从摄像头中看到的
这意味着你应该在思考时刹车
考虑到这个问题的范围
仅仅是处理、获取和分析所有数据的规模
本身就非常具有挑战性 现在
幸运的是，调整参数的问题不需要像我描述的那么困难
防止过拟合
最后 幸运的是 调整参数的问题不需要像我描述的那么困难
以上是翻译结果 有专门设计的卷积神经网络架构，它们能够进行
一些工作为你做
大量的研究投入到寻找卷积神经网络（CNN）的优化拓扑结构和参数中
对于给定类型的问题
你可以把它想象成你可以从中汲取的图书馆
例如 你可以使用的有琳奈特五建筑风格
那适合手写识别
特别地，还有一个叫做alex net的
对于图像分类来说，哪个是合适的
这是一个比LeNet更深的神经网络
在我们之前幻灯片上讨论的例子中
我们只有一个隐藏层
但你可以有多少你想要的
这只是一个计算能力的问题
还有一种叫做谷歌LeNet的东西
你可以猜出是谁想出了这一点
它甚至更深 但它有更好的性能
因为它引入了一个叫做Inception模块的概念
植入模块
组 卷积层一起
这是它如何工作的一个有用优化
最后 今天最复杂的一个被称为resnet，意为残差网络
这是一个更深的神经网络
但它通过所谓的跳跃连接来保持性能
它在感知机的层之间有特殊的连接，以进一步加速
它建立在神经网络的基本架构之上，以优化其性能
正如你所见 CNNs在性能上要求很高
ResNet在AWS的世界里经常被提及
通常ResNet的变体，如ResNet五十 特别是在SageMaker和其他地方的图像分类算法中
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/065_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p65 05. Recurrent Neural Networks.ai-zh.srt

```
# 🎬 065_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p65 05. Recurrent Neural Networks

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在我们深入探讨一下循环神经网络，以及它们的原理
所以，RNN 到底是什么
RNN 的主要用途是什么？
基本上，它们用于处理时间序列数据
你可以用它来处理时间序列数据
比如，你可以分析一系列数据点随时间的变化
然后预测未来随时间的变化
反过来
RNN 本质上是用于处理某种类型的序列数据
一些时间序列数据的例子可能是网络日志
你网站在不同时间的点击量不同
或者传感器日志中传感器输入不同
来自物联网
或者你可能通过查看历史股票交易信息来预测股票行为
这些都是循环神经网络的潜在应用
因为它们可以查看行为随时间的变化
并尝试在作出未来预测时考虑这种行为
另一个例子可能是
如果你试图开发一辆自动驾驶汽车
你可能有你的车过去的轨迹的历史
也许这将影响你的车在未来如何转向
所以你可能需要考虑到你的车一直在沿着曲线转弯
预测它应该继续沿着曲线行驶直到道路变直
序列不一定只发生在时间上
它可以是任意长度的任意序列
另一个想到的是语言
句子只是单词的序列
是的 所以你可以将RNN应用于语言或机器翻译，或为视频或图像生成字幕
这些都是顺序单词在句子中可能重要的例子
句子的结构
这些单词的排列方式可能比
单独看这些单词
没有上下文
RNN可以利用这些单词的顺序
RNN的另一个有趣应用是机器生成音乐
你也可以把音乐
看作是一种文本
而不是单词或字母的序列
而是音符的序列
实际上你可以构建一个神经网络，它能够处理现有的音乐并加以扩展
使用循环神经网络
试图学习过去音乐中的美学模式
从概念上讲，单个循环神经元看起来像这样
在模型方面
它看起来与我们之前看到的人工神经元非常相似
主要的区别是这里这个小循环环绕着它
当我们在这个神经元上运行一个训练步骤时
一些训练数据被喂入其中
或者这可能是我们神经网络中前一层的输入
它将在将所有输入相加后应用某种阶跃函数
在这个例子中，我们将使用类似于双曲正切函数
因为从数学上讲，我们希望确保随着时间的推移保留一些信息
以一种平滑的方式
现在 通常我们会输出那个求和的结果
以及那个激活函数的输出作为这个神经元的输出
但我们也会将那个结果反馈回同一个神经元
所以下一次我们运行数据通过这个神经元时
之前运行的数据也会作为额外的输入被加到结果中
所以我们不断运行这个程序，就会有新的数据进来
这些新的数据会和上一次运行神经元的输出混合
这个过程会一遍又一遍地重复
你可以看到随着时间的推移
神经元过去的行为会影响它的未来行为，影响它的学习方式
另一种思考方式是将其展开在时间轴上
这个图表示的是同一个神经元
在三个不同的时间步骤
当你深入研究RNN的工作数学时，这是更有用的方式
如果我们将左边的视为时间步0
你可以看到有一些数据输入进入这个循环神经元
经过它的激活函数后会产生某种输出
这个输出也会传递到下一个时间步
所以如果在这个时间步的中间是时间步1，同一个神经元
你可以看到这神经元不仅接收到新的输入
也接收到来自上一个时间步的输出
这些加起来
然后应用这个激活函数
也会产生输出
这个组合的输出会传递到下一个时间步
调用本次时间步为二
这里引入一个新的时间步
二步的新输入被喂入这个神经元
并且前一步的输出也被喂入
它们被加在一起
激活函数被运行
我们得到一个新的输出
这叫做记忆单元
因为它能够保持其前一步输出的记忆
你可以看到，尽管在每个时间步上进行求和，但它能够保持记忆
那些早期的行为在某种程度上被稀释了
所以我们在时间步零到时间步一之间添加
然后这两者的总和最终进入时间步二
所以记忆单元的一个属性是
较近的行为往往对当前时间步有更大的影响
这在某些应用中可能是一个问题
但我们可以稍后再讨论一些方法来对抗这一点
继续前进
你可以有一个循环神经元的层
在这个图中
我们正在观察四个单独的复发神经元，它们一起工作
作为层的一部分
你可以将输入作为整个层的输入
输入这些四个不同的复发神经元
然后这些神经元的输出可以反馈到下一步
每个神经元
我们所做的就是将此水平扩展
而不是单个复发神经元
我们有四个复发神经元的层
所有那些神经元的输出都反馈到那些神经元的行为
在下一步的学习中
你可以扩展到多个神经元，学习更复杂的模式
结果
RNN打开了广泛的可能性
因为我们现在能够处理不仅仅是信息的向量
或者某种状态的静态快照
我们也可以处理数据的序列
RNN可以处理的有四种不同的组合
我们可以处理序列到序列的神经网络
如果我们的输入是时间序列或某种数据的序列
我们也可以有输出是时间序列或某种数据的序列
如果你试图根据历史交易预测股票价格
这可能是序列到序列拓扑结构的一个例子
我们也可以混合匹配序列与使用多层感知机预测的老旧向量静态状态
我们将其称为序列到向量
如果我们开始时有数据序列
我们可以生成一个状态的快照
作为分析该序列的结果
例如，分析句子中的单词序列
以产生该句子传达的情感想法
你可以反向操作
你可以从向量到序列
例如，从图像开始，这是信息的静态向量
然后从该向量生成一个序列
例如，单词
在句子中生成一个从图像创建的标题
我们可以以有趣的方式将这些东西串接在一起
也可以
我们可以构建编码器和解码器，它们相互反馈
例如 我们可能会从一个句子的信息序列开始
将该句子的含义转化为某种向量表示 然后将其转换为另一种语言的新单词序列
这可能是机器翻译系统的工作方式
你可能从法语单词序列开始
构建一个称为嵌入层的东西
只是一个体现该句子含义的向量
然后生成一个新的英语或其他语言的单词序列
你可能从法语单词序列开始
构建一个称为嵌入层的东西，只是一个体现该句子含义的向量，然后生成一个新的英语或其他语言的单词序列
这是一个使用循环神经网络进行机器翻译的例子
这里有很多令人兴奋的可能性
训练RNN就像训练CNN一样困难
在某些方面，它甚至更难
主要的转折点是我们需要反向传播
不仅在训练过程中通过我们的神经网络及其所有层
而且在时间上，实际上也需要通过时间步骤
每个时间步骤最终都看起来像是我们神经网络中的另一层
当我们试图训练它时
这些时间步骤很快就会累积
随着时间的推移，我们需要训练一个越来越深的神经网络
在实际进行梯度下降时，随着神经网络越来越深，成本也越来越大
为了限制训练时间
我们通常限制反向传播的时间步数
我们称之为截断的通过时间的反向传播
当你训练RNN时，这一点值得注意
你不仅需要反向传播你创建的神经网络拓扑
你也需要反向传播到你构建到该点的所有时间步数
9: 你也需要反向传播到你构建到该点的所有时间步数
10:
现在我们已经谈论过你的事实
当你构建一个RNN时
早期的状态随着时间的推移而逐渐减弱
因为我们只是将前一步的行为输入到当前的步骤
我们的RNN中
这可能是个问题
如果你有一个系统，旧行为并不比新行为更重要
例如 如果你正在查看句子中的单词
句子开头的单词可能比句子末尾的单词更重要
如果你试图学习一个句子的意思
每个单词的位置与它可能多么重要之间没有内在的关系
在许多情况下
这是一个你可能想要做一些事情来抵消这种影响的例子
一种这样做的方法是被称为lstm细胞的东西
它代表长期短期记忆细胞
这里的想法是它保持短时和长时状态的独立概念
它以相当复杂的方式这样做
幸运的是，你并不需要真正理解这些细节
这些细节是如何工作的
这里有一个图像供你查看
如果你好奇 但你使用的库会为你实现这一点
重要的是要理解，如果你处理的是数据序列
你不想给更近期的数据给予优先待遇
你可能想使用LSTM单元而不是仅仅使用普通的RNN
在LSTM单元上也有优化
称为GRU单元，即门控循环单元
这是对LSTM单元的简化，性能几乎一样好
所以如果你需要在模型性能之间做出妥协
在训练时间方面，性能如何
A g r u cell可能是一个好选择
G u cell在实践中非常流行
如果认为cnns很难，那么rnn的训练真的很难
等你看到rns时，你会大吃一惊
它们对您选择的拓扑结构非常敏感
以及超参数的选择
由于我们需要在时间上模拟事情
而不仅仅是通过您网络的静态拓扑结构
如果您做出错误的选择，它们可能会变得非常耗资源
您可能会得到一个不收敛的循环神经网络
它可能会完全无用
即使您运行了几个小时以查看它是否实际上起作用，所以再次
在您能够尝试找到一些拓扑结构和参数集，它们在您试图解决的问题上表现良好时，再次研究之前的研究是非常重要的 当您能够尝试找到一些拓扑结构和参数集，它们在您试图解决的问题上表现良好时，再次研究之前的研究是非常重要的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/066_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p66 06. Modern NLP with BERT and GPT, and Transfer Learning.ai-zh.srt

```
# 🎬 066_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p66 06. Modern NLP with BERT and GPT, and Transfer Learning

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                所以cnn的和rnns的种类是大型深度学习开始的地方，对吧
但是今天的深度学习模型
你知道的那些你在新闻中看到的gpt
和三系统，它们非常擅长模拟人类
它们倾向于是自然语言处理模型
它们倾向于非常复杂和非常庞大
在它们后面有成百上千亿的参数
你不能真正理解这一点，对吧
所以考试基本上只期望你知道它们是什么
以及如何调整它们
当前的最新技术是所谓的transformer深度学习架构
这是自然语言处理的热门领域
这就是模拟一个能够以现实方式说英语的系统的问题
或者以你的语言说话，模拟人类，以及如何通过语言与其他人互动
自注意力机制是transformer深度学习架构的秘密
所以它会权衡输入数据的每个部分的重要性
它仍然会顺序处理数据
我们看到你可以使用rnn来做自然语言处理
理解一个句子中的单词流
自注意力与自注意力的区别在于，它可以同时并行处理所有这些单词
所以它在这方面比rnn更强大
注意力机制为每个单词提供了上下文
所以我们不需要逐个单词处理
上下文已经包含在内 允许我们并行处理
有很多基于transformer架构的不同类型的模型
包括burroberta t five
gpt two
以及它的所有兄弟姐妹
gpt three现在很流行 还有distillbert
distillbert是由知识蒸馏实现的 这是一种压缩和减小模型大小的方法
在这种情况下大约减少了40%
bert代表双向编码器表示从transformer
你可能会在考试中看到这一点
你只需要理解上下文
他们在谈论什么
bert是一种基于transformer的自然语言处理模型
它是一个非常大的神经网络
用于自然语言处理
这就是bert的全部
gpt只是为了完整性 它是预训练的生成式transformer
这就是gpt的全部
这就是bert的全部
这就是gpt的全部
这是你在谷歌新闻中读到的那个
关于那个令人不寒而栗的
并且已经骗过了不止一个人
它可能真的有智能
但你知道，这当然存在争议
我认为共识并不是完全像gpt
当然，chat gpt的基础
目前非常流行
在2023年 当我录制这段时，che bt是基于gpt3的
当你在看这个视频时
它可能会取决于gpt四或五
或者谁知道还有什么别的 但这是当今人工智能热潮背后的核心技术
是的
那么您是如何使用这些模型的呢
它们非常复杂
正确 所以你不能只是从零开始搭建一个这样的玩意
你需要从一个预训练的模型开始
这已经训练过了，并且基于此进行了一些构建
这些最新的模型可能有数百亿个参数
甚至更多，就像现在的万亿级
对吧 所以你不想每次都要重新训练
每次你想要开始进行自然语言处理时
那样会消耗巨大的计算能力
你没有那么多钱
可靠的 所以我们使用称为迁移学习的技术，来使用预训练的模型
并且根据我们自己的需要调整它们
所以外面有一些预训练模型库
其中最大的一个现在叫做hugging face
他们有一个预训练模型的仓库，你可以导入并开始使用
他们有几千种不同的自然语言处理模型，你可以直接导入
你可以直接使用这些模型
或者尝试用你自己的训练数据来调整这些模型以适应你的具体需求
hugging face现在与sagemaker集成，使用hugging face deep learning容器
深度学习容器
我们称之为深度学习容器的缩写
顺便说一句 现在
像之前所说，你可以直接使用这些模型
或者你可以根据自己的需求进行微调
在考试中可能会问到这一点
所以我们来谈谈这个问题
例如 假设我们想从Hugging Face导入一个BERT模型
作为我们自然语言处理工作的起点
所以再次 Hugging face提供了一个深度学习容器
用于SageMaker的预训练BERT模型
它在书籍语料库和维基百科上进行了预训练
它通过阅读我们可以提供给它的每一本书来学习英语
以及我们可以提供给它的每一篇维基百科文章
也许你想用你自己的会话数据来训练它
你的文本 并使其适应你的特定用例
你可能不想只谈论书籍和维基百科
你可能想谈论与你公司更相关的事情
在这种情况下，你可以用你自己的额外训练数据微调模型
我们通过迁移学习来做到这一点
我们会更深入地讨论这一点
这样做的方式是
如果你有自己的训练数据
你必须首先将这些单词转换为与BERT相同的格式进行训练
最初这可能是困难的部分
你不直接喂给它原始单词
你喂给它令牌
每个单词的数值表示
你必须确保这是以一种一致的方式进行的
就像模型最初训练时所做的那样
然后你就开始用你的数据训练它
你从预训练的BERT模型开始，该模型训练了世界上的每一本书
和每一篇维基百科文章
然后你也开始用你的数据训练它，并进一步微调它
你基本上解冻了神经元中的权重
并用你自己的训练数据进行进一步的训练
在进行此操作时，你应该使用较低的学习率
以确保你不会过于偏离，而不会丢弃所有那个巨大的预训练
你知道，你想要的是微妙的
所以这里有一点艺术
有各种迁移学习的方法
我们谈论了微调
嗯 我们谈论了微调
如果你有比你能拥有的多得多的训练数据
从预训练模型开始进行训练是明智的
也许用你的数据用较低的学习率进一步训练
这就是微调
再次 如果你只想用你的数据来逐步改进模型
并且现有模型有一个预训练的数据集
那就是 你知道
不是 你不想失去微调可能是一种方法
你也可以在新的模型顶部添加新的可训练层
你可以使用预训练的模型
冻结模型中的所有权重
然后添加一些更多的层在它的上面
这样你就可以添加几层
或者你想要多少 你需要学会如何将旧的特征转化为对新数据的预测
你也可以同时做这两件事
这也是很常见的
所以你可能会看到，在一个现有的预训练模型上添加新的层
并且冻结它
然后使用这个组合模型进行进一步的微调
嗯 通过进一步在它自己的数据上进行训练
你也可以从头开始重新训练它
嗯 如果你有大量的训练数据
并且你只关心模型本身
它的实际架构
你可以丢弃预训练的权重从头开始
你只可能在有大量自己数据的情况下这样做
并且你有足够的计算能力来处理重新训练
这些模型可以非常复杂
极其深
从头训练它们可能会非常昂贵
所以这是一个特殊情况
或者你可以直接使用它
也许模型的训练数据已经非常接近你想要的
无论如何
也许你只是想训练一个模型，让它听起来像一个书籍或者维基百科文章 如果是这样的话
你可以从hugging face或者其他地方获取一个预训练模型
然后直接使用它
这是完全可以的
同样
你可能在考试中被问到如何选择这些不同的方法 所以把这些不同的使用场景内化
你知道的
如果你想要微调一个模型并做出渐进式的改进 微调可能是一个好方法
如果你想要学习如何将旧的特征转化为对新数据的预测
你可以在模型上加一些额外的层
或者你可以从头开始重新训练它
如果你有这样的计算能力和数据
或者你可能只是使用它因为它已经满足了你的需求
这就是自然语言处理中的迁移学习的精髓
把这些概念结合起来
因为现代的NLP模型非常复杂并且有非常多的参数
迁移学习几乎是一个必需品
所以这两者现在紧密相关 这就是自然语言处理中的迁移学习
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/067_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p67 07. Deep Learning on EC2 and EMR.ai-zh.srt

```
# 🎬 067_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p67 07. Deep Learning on EC2 and EMR

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们已经谈了很多关于深度学习的一般概念
这没问题 考试确实讨论了深度学习的一般概念
不一定与aws相关
谈了很多 但让我们把它变成现实，谈谈如何在亚马逊硬件上部署深度学习
emr支持mxnet和tensorflow
以及pytorch和gpu实例类型，以加速这些，就像我们讨论过的那样
gpu特别适合深度学习应用，因为它们可以非常高效地在并行中进行大量操作
那里提供了多种实例类型供您选择
当你在EC two上部署自己的硬件时
C Two或EMR
P3将是一个更昂贵的解决方案，配备特斯拉V100 GPU
P2可能是一个更经济的选择
如果你在做一些更小的规模
或者实验，16K80 GPU内置
还有G3
另一种重型解决方案，配备4M60 GPU
顺便说一句 这些都是英伟达的芯片
所以目前aws提供的所有服务都是基于英伟达GPU硬件构建的
最近有g5g实例类型
这是基于aws s自己的graton2处理器为CPU对
与英伟达t4g张量核GPU
这些尚未在emr中可用
但你可以部署一个e
C Two实例与他们
看起来这些主要是为安卓游戏流服务而设计的
但它们也可以作为深度学习的副产品
最近还有p4d类型
这些是英伟达a100芯片
你可以将它们构建成他们所谓的超级集群
你可以构建一个由数千个p4d组成的集群
并将它们全部聚集在一起用于超级计算应用
如果你想构建自己的巨型大脑并与chat gpt竞争
构建一个p4d的超级集群可能是在云中的一种方式
当然不便宜
但这就是为你准备的
此外，还有一个是在外面的e
C 两个亚马逊机器镜像
你可以直接选择其中一个，自动配置它以进行深度学习
这是你开始工作的一种简单方法
最后，sagemaker
这是做机器学习在亚马逊的一种更现代的方式
亚马逊还有用于自动部署您自己集群的方法，使用您想要的任何架构
但我们还没有谈论sagemaker
所以我们将在2023年到达那里
亚马逊推出了几种新的实例类型用于机器学习
特别是为了应对生成式AI的世界在大型语言模型中迅速发展
以及高效训练这些工具的需要
第一种是trn
一种实例类型
他们说它是由tranium驱动的
这延续了公司为深度学习开发自己的自定义硬件的趋势
请记住，这种硬件是在2023年中期推出的
在聊天gpt风靡之后
每个人都想同时训练大型语言模型
所以如果你在之前观看
说二零二四
可能还不会在考试中出现
但还是值得知道的
这些实例类型优化了训练大型语言模型或神经网络。
他们声称它在其他现有产品上节省了百分之五十
C 两种类型 如果你正在做的是训练一个大型的神经网络
它也配备了每秒800千兆位的弹性织物
适配器和网络，用于真正快速的集群
所以，如果你在训练一个非常庞大的模型
就像gpt三或gpt四或无论什么接踵而至的东西
你需要非常快速地在模型中不同节点上进行信息的来回传递
你在不同的虚拟神经元之间进行训练
如果你愿意 所以
那相当令人印象深刻
那就是呃 这给你提供了一个非常快速的信息传输方式
并且可以同时训练一个非常大的集群
如果你需要更多的速度
他们还有trn one n实例
n代表网络
我猜测这提供了更多的带宽
每节点高达1600千兆比特每秒
在推理方面还有inf two实例类型
这是指 你知道 这一代的这
之前有一个版本由aws inforna 2驱动，所以再次
使用定制硬件
它们优化了快速推理
如果你阅读亚马逊关于这方面的博客文章
他们谈论
目前 至少到2023年
大部分的计算能力都投入到训练这些大型语言模型中
但随着时间的推移
更多的人会使用这些预训练模型
所以他们预测未来
会有更多的计算资源投入到推理模型中
实际上比投资于模型本身的训练更多的获取这些训练模型的信息
所以他们在赌
过一段时间训练就会完成
你知道 这些模型将会达到它们能达到的最好状态
真正的需求将会是在这些模型上用于实际应用
因此推理可能会是未来需要优化的重要事情
这就是f2实例类型所起的作用
我们稍后会在课程中更详细地讨论转换器和大语言模型 但现在它们正在推出这些实例类型来支持这一点
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/068_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p68 08. Tuning Neural Networks.ai-zh.srt

```
# 🎬 068_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p68 08. Tuning Neural Networks

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈如何调整你的神经网络
再次强调，这是考试中的部分内容
他们试图筛选出那些没有在现实生活中真正做过的人 这不是通常教授的内容
但我会尽力
尽可能清晰地传达
嗯 这是我能最好的方式
这对考试非常重要，伙计们
让我们谈谈学习率
首先 我们提到的学习率是什么意思
嗯 你需要理解这些神经网络是如何进行训练的
它们使用梯度下降技术或者类似的技术
有很多不同的方法
基本思想是我们从一个随机的权重点开始，在我们的神经网络中
我们尝试不同的解决方案
不同的权重集
试图最小化我们定义的成本函数，经过多个周期
所以这些都是关键词
我们在这里有很多轮次
在每个轮次中我们对其进行迭代
我们在神经网络中尝试不同的权重集
试图最小化某些成本函数
这可能是它在验证集上预测的准确性
因此我们需要有一种韵律和理性
来决定我们如何制作那些不同解决方案的不同样本不同权重
如果你愿意 如果我们可以将其简化成一个二维图形
也许它看起来像这样
我们在曲线上的不同点进行采样
我们试图找到最小化成本函数的点
这里是y轴
我们试图找到这个图中的最低点
我们通过在不同的点进行采样来尝试到达那里
并从每个之前的样本中学习
这就是梯度下降的全部内容
学习率决定了这些样本之间的距离
在这里你可以看到
我们可能会从这里开始，我的学习率说
好的 我在这里再试一个点
在这里再试一次，等等
就这样直到我终于找到这条曲线上的最低点
并把它作为我的最佳解决方案
所以不难理解学习率对你的训练的影响
对吧 如果你的学习率太高
你可能会完全错过那个解决方案
所以想象一下，我的学习速率非常大
我从这里直接跳到这里
如果我的学习速率过高
我可能会完全错过底部的点
但你可以看到，如果我的学习速率过小
我将在这里采样大量的不同点
这将需要很多轮次
很多步骤来找到最优解
因此学习速率过高可能会导致我完全错过正确的解
但学习速率过小可能会导致训练时间过长
学习速率是超参数的一个例子
它是你在训练深度学习模型时使用的一个旋钮
它可以影响模型的最终结果
通常这些超参数对模型质量的影响
与模型的拓扑结构、特征工程一样重要
所以这又是你需要通过实验找到的另一个拼图块
除了学习速率
另一个重要超参数是批次大小 它指每个批次中使用的训练样本数量
在每次轮次中
伙计们，把这个牢牢记住，因为它有点反直觉
你可能会认为较大的批次大小是好事，对吧
数据越多，越好，但不是这样
实际上，较小的批次大小
它有更好的能力摆脱我们所谓的局部最小值
例如，在这里
你可以看到有一个局部最小值 在这里有一个凹陷
这里有一个相当不错的
损失函数的值，这是我们试图优化的
但是存在一种风险，即梯度下降
我们可能会被困在这个局部最小值中
实际上，更好的解决方案在这里
在某处
所以我们需要确保在梯度下降的过程中
我们有能力摆脱这个东西并找到更好的解决方案 事实证明，较小的批次大小
比较大的批次大小更有效地做到这一点
所以较小的批次大小可以摆脱这些局部最小值
但较大的批次大小可能会被困在那里
就像被压住一样 如果批次过大
可能会陷入错误的解决方案中
更奇怪的是
较大的批次大小可能会陷入错误的解决方案中
而较小的批次大小则可以摆脱这些局部最小值
但较大的批次大小可能会被困在那里
就像被压住一样
如果批次过大
可能会陷入错误的解决方案中
那是因为你通常会在每次训练轮次的开始时随机打乱你的数据
这可能会导致每次运行结果非常不一致
如果我的批量大小只是有点太大
也许有时我会卡在这个最小值上
而有时我不会
你知道，我在最终结果中看到从一次运行到另一次运行
有时我会得到这个答案
而有时我会得到这个答案
所以伙计们，把这个敲入你们的脑袋
这对考试非常重要
较小的批量大小倾向于不卡在局部最小值上
而较大的批量大小可能会随机收敛到错误的解决方案上
较大的学习率可能会导致错过正确的解决方案
而较小的学习率可能会增加训练时间
所以记住这一点
写下来 重要的事情
再次强调 这是大多数人通过经验学到的东西的例子 但我正在尝试在这里提前教你
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/070_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p70 10. L1 and L2 Regularization.ai-zh.srt

```
# 🎬 070_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p70 10. L1 and L2 Regularization

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来我们来谈谈L1和L2正则化
这比深度学习更广泛
这适用于整个机器学习领域
但我们会期望你知道两者之间的区别
以及在何时使用哪一个
这是比较高级的东西
但你知道这就是这次考试的主题
它是看你在机器学习上的水平
这样做的方式是添加一个我们称之为正则化项
L1只是那些权重的总和
而L2是那些权重的平方和的总和
在图形的右边
你可以看到L1最终会变成那种菱形
而L2变成正方形
最终会变成圆形
它是圆形 它是一个圆
你也可以将同样的想法应用于损失函数
而不仅仅是在训练过程中的正则化
这两种东西之间的实际区别是什么
嗯
L1作为权重的总和
在实践中它会进行数学上的特征选择
它可以导致整个特征变为零
所以正则化项可以导致很多术语变为零
并且实际上选择一些特征比其它特征更重要
但它在计算上效率低，结果稀疏
因为它最终会删除信息
L2 然而
权重的平方和的总和
这会导致所有特征都被考虑
它只是给它们不同的权重
没有任何东西变为零
你只是给不同的特征分配非常小的或非常大的权重
基于事情如何发展
它在计算上更高效，结果更密集
因为它没有丢弃任何东西
所以L2听起来很酷对吧
我的意思是 我想保留我的信息
但是 但我有使用L1的情况
再次 当我们开始谈论特征工程和特征选择时，谈到了维度诅咒
与L1正则化，这是一种自动实现的方法
在极端情况下
你知道
在你拥有的一百种不同特性中
也许只有十种特性在实际应用中会有非零系数
最终得到的稀疏性
可以弥补L1正则化本身的计算效率低下
尽管计算L1正则化稍微耗时
但最终你会得到一个更小的特征集
这可以显著加快机器学习模型的训练速度
从总体训练时间来看
如果你认为你的一些特性并不重要
并且你想将它们减少到一个较小的特征子集
L1正则化可能更适合你
然而 如果你认为你的所有特性都很重要
那么选择L2正则化
因为它不会进行特征选择
它不会将整个特性完全消除
它不会让这个正则化项降到零
它只是会降低它们的权重
这就是区别 L1正则化进行特征选择
L2正则化保留所有特性
但会给它们不同的权重 这就是主要区别 这就是L1和L2正则化的主要区别
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/071_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p71 11. Grief with Gradients The Vanishing Gradient problem.ai-zh.srt

```
# 🎬 071_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p71 11. Grief with Gradients The Vanishing Gradient problem

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在考试中让我有点惊讶的是
他们对训练神经网络各种边缘案例的深度期望
其中一些涉及到梯度和它们一些数值问题
让我们深入探讨一下
你可能没有听说过消失的梯度问题
但我们来谈谈这个问题
好的
这里发生的事情是随着学习曲线的斜率
记住我们在看损失函数的y轴
你知道
在x轴上的一些权重
我们在梯度下降期间尝试过的
随着这条曲线的斜率接近零
这将发生在曲线的底部
是的 这就是斜率变得平坦和水平的地方
你可以认为斜率再次是这条曲线的一阶导数
从数学上讲 如果你记得你的微积分
但当斜率最终接近零
你从很小的数字开始工作，这可能会导致问题
从数值上讲 这可能会减缓你的训练
甚至可能会进入CPU的精度错误领域
随着你接近曲线底部，斜率变得越来越小
那就是你想要到达的地方 这对计算机来说可能确实是一个具有挑战性的地方
这尤其对更深的神经网络来说是一个问题
以及RNNs
因为消失的梯度最终会传播到更深的层次
所以消失的梯度问题，想想看
这是说的 它说的是我有一个消失的梯度
梯度趋向于零意味着在那个点的斜率趋向于零
或者一阶导数趋向于零
基本上这意味着我们正在达到那些曲线的底部
这可能是一个局部最小值
但也可能是正确答案
所以这是一个问题
结果表明 还有相反的问题
爆炸性梯度问题，情况正好相反
它们变得越来越垂直
就像在这张图的开头
这也可能是一个数学问题
那么我们该怎么做呢
有几种不同的方法可以解决梯度消失问题
一种方法是使用所谓的多级层次结构
想法是将你的神经网络分成多个层次，这些层次单独训练
而不是一次性训练整个深度神经网络
所有层次一起训练
你训练一些层次
然后你再训练其他层次
采用这种层级化的方法，每个子网络单独训练
这可以帮助解决这个问题
限制那些消失的梯度在神经网络中传播的距离
此外，有一些特定的架构设计来解决这个问题
一个是长短期记忆或LSTM
你可能记得那是特定的
一种我们之前讨论过的循环神经网络
还有一些残差网络，如ResNet
这在物体识别领域非常流行
这是一种非常流行的卷积神经网络架构
它也被设计用来解决消失的梯度问题
选择更好的激活函数也可以起到很好的效果
激活函数基本上是一个神经元决定
它的权重是否应该触发信号的传播
你可以有不同的功能来实现这一点
有一种叫做relu的功能，它有一个特性，就像一个45度的角度
当它是正的时候 这实际上避免了一些梯度计算的问题
当你对这个函数求导的时候
所以 仅仅使用relu也可以避免梯度消失的问题
所以记住这些内容
如果你遇到了梯度消失的问题
这些都是解决问题的方法
这就是人们通常通过经验学到的事情
但是 我在这里给你一个优势
记住relu是解决梯度消失的问题
就像多层层次结构
以及使用特定的架构如lstm和resnet
关于梯度的话题
有一种叫做梯度检查的东西
如果你没听说过
这只是一种调试技术
所以，当你实际开发一个神经网络框架时
检查训练过程中计算的导数的数值可能是一个好主意
以确保它们应该是什么就是什么
如果你正在验证神经网络工作方式的底层代码
梯度检查是一个良好的诊断工具，以确保那些梯度
那些学习曲线的第一导数
正是你所期望的值
你可能不会在工业中亲自进行梯度检查
因为这通常发生在你工作的代码和框架的较低层次
Than the code and frameworks that you're working with
但是那就是梯度检查的作用
如果考试中抛出这个问题，你可能需要了解它的含义
所以这就是梯度带来的麻烦
以及梯度在训练中可能引起的具体问题 训练中的梯度问题
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/072_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p72 12. The Confusion Matrix.ai-zh.srt

```
# 🎬 072_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p72 12. The Confusion Matrix

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在考试中你会经常看到混淆矩阵的概念
所以我们来深入了解这些是什么
混淆矩阵是什么
准确性有时并不能完全说明问题
混淆矩阵可以帮助你理解模型的更微妙的结果
例如 一种用于检测罕见疾病的测试可能只有99.9%的准确性
通过总是猜测你没有这种疾病
一个这样做模型的看起来在纸上会有非常高的准确性
但实际上比无用更糟
是的 所以你需要理解这种情况
真阳性或真阴性的重要性
以及假阳性或假阴性的重要性
与你试图实现的目标
以及衡量你的模型在每个这些方面的好坏
混淆矩阵只是展示模型准确性的一种方式
一种可能看起来像这样
这是一个通用的格式
所以想象一下我们有一个二进制情况
我们只是在预测
是或不如我有这种疾病
或者我没有这种疾病
或者你知道我检测出这种药物
或者我没有检测出这种药物
这张图片里有一只猫
或者这张图片里没有猫
这就是它可能看起来的样子
所以你看到在行我们有预测值
在列我们有实际值
如果我们预测某事是真实的
并且它 它实际上是
那么那是一个真阳性
如果我们预测是
但实际上不是
实际上为负
如果我们预测不是
但实际上是 那是一个假阴性
如果我们预测不是
但实际上不是
那是一个真阴性
我的意思是它有点令人困惑
但如果你仔细想想
这一切都是说得通的
在实际的混淆矩阵中
这些单元格将包含实际数字
你的模型实际上这样做了多少次
在它的测试数据集中
你必须注意标签
没有真正的惯例来确定这个顺序
有时你会在这里看到预测值，而在这里看到实际值
啊 不要仅仅假设混淆矩阵以某种格式存在
注意标签
并确保在得出结论之前你理解它告诉你什么
这里有一个值得注意的事情，通常你想要大多数值在这里和这里
是的 所以混淆矩阵的对角线是你应该大多数结果的地方
嗯 这就是准确性所在的地方
这就是我有一个真正积极的地方
这就是我有一个真正消极的地方
你想要那些是
你知道，假阴性和假阳性是
你知道 可比较低的数字
希望如此 所以准确的模型在这里的对角线上会有高的数字
让我们插入一些实际的数字，看看那看起来像什么
所以假设我有一个机器学习模型
它试图确定一个图像是否包含猫的照片
嗯 如果我们预测我有一只猫
它确实在测试集中发生了50次有猫
但有时我预测它是一只猫
但它不是猫 它是一只狗或一条鱼什么的
这发生在五次
如果我预测那不是一只猫
但它确实发生了10次
猫
如果我说它不是一只猫
但它确实不是猫，这在这种情况下发生了100次
这就是如何解释混淆矩阵
我们将讨论如何从这些数据中制作更有用的指标，用于分析
有时你会看到混淆矩阵以不同格式出现
我们在每一行和每一列中实际上加起来
所以这可能是你偶尔会看到的
所有这些都只是添加
你知道我们有多少实际节点
我们有多少实际是的
我们有多少预测节点
以及我们总共有多少预测是的
所以这就是你看到的那个格式的样子
内部分
尽管只是之前我们看过的混淆矩阵
再次
记住，预测值和实际值可以互换
所以请确保你注意这些标签
你知道我能说什么
混淆矩阵可能会让人困惑
嗯 关于它们的标准并不统一
不幸的是 所以请确保你注意它们告诉你的内容
在考试中回答关于它们的问题之前，请确保你理解它们
有时你会看到这种格式
所以可能我们这里也有多分类模型
想象一下我们有 嗯
一个手写识别系统，试图识别某人在写零到九的值
因此，一个更复杂的混淆矩阵可能看起来像这样
哪里 而不是仅仅说“是”
没有答案 实际上我们有多个分类
但是它以同样的方式工作
所以在这个轴上我们有预测的标签，在这个轴上我们有真实的标签
所以我们说如果我预测某物是5
而它实际上是5
那么这种蓝色对应于这里的某个数字
所以这里有两个不同的地方
首先，我们拥有多于是/否的选项
在这里，我们没有选项
我们有多个分类
所以我们的混淆矩阵更大
让我们深入另一个例子
只是为了强调这一点
所以有时我预测这是一个一
但实际上这是一个八
那里稍微有点淡蓝
你知道 也许发生过 你知道在这个例子中大约发生了20次
我们也在使用一种称为热图的东西
而不是在这些单个单元格中显示数字
我们将那些数字映射到列
那个颜色的黑暗程度对应于它的数值高低
你应该在这里看到一条较暗的线
代表在真阳性和真阴性上的良好准确度
并且你知道，在这里的理想情况下，会有一些更稀疏、更浅的颜色
但那个颜色会映射到一个实际的值
这让它容易可视化你的混淆矩阵是如何布局的
好的 大家明白了，这就是混淆矩阵的全部
这可能有点令人困惑
但是 只要看看这些例子
你应该能理解
最后，我想谈谈这种混淆矩阵的特定风格
它仍然是热图风格
我们刚刚讨论过
我收到了一些人的报告
这种风格实际上还在考试中出现
这里的这种格式
这让我很惊讶
因为我是从aws机器学习服务文档中提取的
这是从哪里开始的
那个服务已经很长时间被废弃了
所以我以为他们已经删除了所有内容
但以防万一，我猜你需要知道这一点
所以让我们谈谈如何分解这个格式
这是我们刚刚在多分类热图中讨论的
我们在上一张幻灯片上看到的
但是有一点不同的形式
我不想让你混淆
如果你看到这一点
那么我们再来分析一下我们看到的这里
每个单元格的颜色显示每类正确和错误的预测数量
再次 从左到右向下的对角线将代表每类的正确预测
如果你看看浪漫和浪漫的交叉点
你应该希望那是一个很好的蓝色
在推断你实际上更经常得到正确的预测
在这种情况下 我们可以推断出
浪漫预测中有80%或更多正确并正确分类为浪漫
并且我们也可以推断出对于惊悚和冒险来说
在那张图表上，错误预测的比例低于百分之二十
在 f one 列中，你也可以看到每个类别的 f one 分数。
在井的两边我们都有总列
底部右下角既有总列又有总行
所以右边的总列在那里
你知道在F列的左边
那就是你的真实班级频率
他们正在向你展示那里 那就是那些百分比的含义
真正的类频率
在底部的那行将是预测的类频率
好的 这就是术语
这就是你需要知道的一切 你可以直接从这个图表中读取
你所需要知道的就是如何阅读这个图表
真的都对
所以再次 我不确定这是否会出现在长期考试中
因为这种做法是从他们废弃的服务中遗留下来的 但以防万一这就是你阅读这些家伙的方式
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/073_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p73 13. Precision, Recall, F1, AUC, and more.ai-zh.srt

```
# 🎬 073_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p73 13. Precision, Recall, F1, AUC, and more

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈可以从混淆矩阵中导出的一些指标
这在考试中非常重要，伙计们
所以请留意
所以让我们重新审视我们的朋友
混淆矩阵再次
在这个特定的例子中
我们有实际值列在列中，预测值列在行中
这可能不同
但在这种格式中，我们在左上角有真阳性的数量
在右下角有真阴性的数量
左下角的假阴性数量
右上角的假阳性数量
好的 确保你理解真阳性和真阴性在哪里
假阳性和假阴性在哪里
当你开始查看混淆矩阵时
再次强调，这取决于混淆矩阵的布局
让我们从召回率开始
召回率计算为真阳性除以真阳性加上假阴性，你应该认真记住这一点
左下角的假阴性数量，右上角的假阳性数量，好的，确保你理解真阳性和真阴性在哪里，假阳性和假阴性在哪里，当你开始查看混淆矩阵时，再次强调，这取决于混淆矩阵的布局，让我们从召回率开始，召回率计算为真阳性除以真阳性加上假阴性，你应该认真记住这一点
你需要知道这一点
它也有其他名字
只是为了让事情更加混乱
所以它也被称为敏感性
真阳性率和完整性和完整性
这有点回溯到它在信息检索领域的原始用途
所以当你非常关心假阴性时，这是一个很好的度量标准
好的 所以欺诈检测是您可能关注的召回率的一个很好的例子
因为在欺诈的世界里，假阴性意味着某事是欺诈
但你未能将其识别为欺诈
你有一笔欺诈交易被标记为完全正常
没问题 这是对于应该检测欺诈的系统来说最糟糕的结果
你想要在假阳性的一边
而不是假阴性 在这种情况下
所以记住，选择好的指标是当您关心假阴性时，真阳性/假阴性
欺诈检测作为那个例子是真阳性/真阳性加假阴性
写下这个
考试前，给自己做一个小笔记，快速复习这些东西
好的 让我们用一个例子来让它变得真实
所以，在这个特定的混淆矩阵例子中
召回率是真正例除以真正例加上假负例
我们从这个混淆矩阵中提取值，按照这个特定的布局
真正例会是五
假负例会是十
所以我们就说五除以五加上十
结果是五除以十五
或者一个 第三或三
三点 三
百分之三 正确
所以这就是召回，一个伙伴在犯罪是精确度
精确度计算为真阳性除以真阳性加假阳性
这也被称为其他名称
包括正确的阳性率或相关结果的百分比
这是一个衡量标准，用于信息检索中的相关性
当你关心精确度时，你应该何时关注它
这是一个重要的指标，当你关心假阳性时
一些例子是医疗筛查或药物测试
你不想告诉某人
嗯 你知道
那会对他们的生活和职业生涯产生非常坏的影响
所以再次
精确度 当你关心假阳性时
比假阴性更关心
药物测试是这方面的经典例子
它被计算为真阳性除以真阳性加假阳性
再次 我们将深入探讨一个例子
在这个特定的混淆矩阵中
真阳性将是五个
在这个例子中的假阳性是20
因此精确度被计算为五除以二十五
这是20%
还有其他指标
例如 特异性，真阴性除以真阴性加假阳性
也称为真阴性率
f1分数被广泛使用
你应该记住这个公式
两个家伙写下来
那就是两倍
真阳性除以两倍真阳性加假阳性加假阴性
你也可以计算为两倍的精确度
乘以召回除以两倍的精确度
加上召回
它既是精确度和敏感性的调和平均值
如果你关心精确度和召回
记住召回和敏感性是同一件事
F1分数是一个衡量标准，它能平衡精度和召回率
如果你知道你的模型不仅仅关注准确率
并且你想要同时捕捉到精度和召回率
F1分数可能是实现这一目标的方法
但你知道 在现实世界中
你可能更关心精度或召回率而不是两者
因此，认真思考你更关心的是什么非常重要
使用F1分数
在我看来 是一种捷径
有点懒惰 但考试可能要求你了解它是什么以及如何计算它
此外 RMSCE经常被用作度量标准
它就是一个直接的准确度衡量
它就是字面意义上的根均方误差
所以你只需将所有预测值与其实际真值之间的平方误差相加
然后取其平方根
就是这样 它只关心对与错的答案
它不涉及精度和召回率的细微差别
如果你只关心准确率
RMSCE是常用于那个度量的标准
它绘制的是真正例率
或召回率与假正例率在不同阈值设置下的关系
随着你选择不同的真与假的阈值
这个曲线会是什么样子
基本上
是你希望它在那条对角线上方
理想的曲线会在左上角形成一个大右角
在那里
整个图形都在图形的上方
如果你向左偏离了对角线
越好，这就是解读的方式
我们也可以谈论曲线下面积
就是字面意思
你可以解读这个值
作为分类器随机选择一个正例
比随机选择一个负例排名更高的概率
AU C值为0.5的情况是你会看到的
如果你在对角线上
是的 如果你在对角线下的面积
即没有比随机更好的情况
结果是0.5
正确 这是有道理的
如果你看到一个au c为0.5或以下的
那就是无用的，甚至更糟
完美的分类器会有一个au c和c为1.0
这将再次是一个完美的情况，其中曲线只是一个完美的直角
有一个1.0在零点
有一个1在上方的左上角
这将包括整个图形的区域，计算结果是1
au c可以用来比较不同的分类器
值越高
效果越好，这就是了
一些常见的评估分类器的指标
精确率和召回率
F1分数
给自己做一个小抄 兄弟们
这些知识很重要
好的 还有一个我在考试中听到的，那就是PR曲线
我们之前谈论了精确率和召回率
你也可以将它们绘制成精确率-召回率曲线
就像右边这里看到的
记住，PR曲线下的面积越大
效果越好
在这个例子中
蓝色曲线比绿色曲线效果更好
因为曲线下的面积更大
但更适合信息检索问题
这是记住的关键
当你搜索数百万文档时，只有少数是相关的
因此很难可视化
只是因为数值太小
但是PR曲线在大规模信息检索问题中更容易可视化
所以将这个也加到你的小抄上
PR曲线，精确率-召回率曲线 PR曲线对于信息检索非常有用
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/075_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p75 01. Introducing Amazon SageMaker.ai-zh.srt

```
# 🎬 075_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p75 01. Introducing Amazon SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                到目前为止
考试中出现最多的服务是亚马逊的sagemaker
Sagemaker基本上是亚马逊机器学习提供的核心
所以我们将在这方面花费大量时间
让我们从宏观层面开始
那么sagemaker是什么
它旨在管理整个机器学习工作流程
所以理想情况下，你在现实生活中的过程应该是这样的
你从获取、清理和准备一些训练数据开始
并在其上进行所有特征工程
然后将这些数据喂给一个模型进行训练
你可以评估该模型并说
好的 看起来不错 让我们继续部署该模型
实际上在生产中使用该模型，用于对之前未看到的观察结果进行推理
现在 一旦该模型在生产中运行
我们可以从结果中学习
看看它实际上做得如何并收集更多信息
使用这些信息来获取
清理和准备更多的数据
然后利用这些知识来做更好的特征工程
然后循环再次开始
我们将使用这些新数据
重新训练我们的模型
再次部署
接收更多数据
重新训练模型
再次部署 并希望随着时间的推移，事情会越来越好
Sagemaker允许您管理所有这些内容
它将启动训练实例以在大规模上进行训练
它为您提供笔记本，您可以实际进行数据准备
它将在e中启动实例c2以实际部署您的模式并作为端点等待
在生产中做出推理 等待在生产中做出推理
从架构上讲，这就是想法
让我们从底部开始 当我们进行训练时
我们已经准备好的训练数据将位于s3存储桶中
Sagemaker的任务是去那里为实际训练分配大量训练主机
现在，实际用于训练的代码
实际模型本身来自注册在弹性容器注册表中的docker镜像
它将从docker镜像中获取训练代码
并将其部署到实际执行训练的一堆主机上 47: 代码本身来自一个注册在Elastic Container Registry中的Docker镜像
48: 它将使用那个训练代码的Docker镜像
49: 部署到实际执行训练的一堆主机上
50: 部署到实际执行训练的一堆主机上
并从S3获取训练数据
当它完成时 它会将训练好的模型和任何相关的文件保存到S3
到这个阶段，我们准备好部署该模型，并将其投入生产
所以，到这个阶段，我们也会在ACR中有一个Docker镜像
那就是推理代码
它可能相对简单
它的唯一目的是接收传入的请求
并使用保存的模型根据请求进行推理
再次从ECR拉取推理代码
它将启动尽可能多的主机
它需要实际提供端点和处理这些请求
它也会启动端点
我们可以用它与外界通信
现在我们可能有一个客户端应用程序向模型发送请求
那个端点会很快做出预测并返回结果
你知道 例如
也许我们有一个客户端在拍照
我们希望知道照片中有什么
它可能说嘿
终端点 这里是一张图片
告诉我里面有什么
然后它会引用到那个推理代码和我们训练好的模型文件
好的 我认为这是一张猫的照片，然后发送给客户端应用
这只是众多例子中的一个
与sagemaker合作的方式有几种
最常用的可能是使用sagemaker笔记本
它就是一个在e上运行的笔记本实例
C 两个实例 某个你指定的地方
你可以从控制台创建这些。
它非常容易使用
正如你所见，你的sagemaker笔记本电脑可以访问s3
因此它实际上可以访问其训练和验证数据在那里
或者你需要的任何其他东西
你可以使用scikit来做一些事情
学习或使用spark
如果你想的话，也可以学习tensorflow
它拥有广泛的内置模型
所以外面有预构建的docker镜像，里面包含了各种各样的模型
你可以直接使用
我们将花很多时间讨论这些
你也可以在你的笔记本中启动训练实例
在你的笔记本中
你可以说去
启动一整队的服务器
当你的训练完成并保存到S3时
你也可以从笔记本中说
好的 将该模型部署到整个端点舰队中
并允许我在大范围内进行预测
而且你可以从笔记本中说
好的 进行自动超参数调优工作以尝试不同参数在我的模型上
并尝试找到理想的参数集以使该模型尽可能好地工作
所有这些都可以在笔记本中完成
你也可以在SageMaker控制台中做很多这样的事情
嗯 笔记本显然给你更多的灵活性
因为你实际上可以在那里编写代码
但有时你会一起使用它们
所以启动一个训练作业
或在笔记本中从笔记本启动超参数调优作业
然后切换到控制台并继续关注它
并看看它做得如何
让我们谈谈数据准备阶段以及它与SageMaker如何交互
再次 SageMaker期望您的数据来自S3某处
所以我们假设您已经使用其他方法准备好了数据
如果您需要
格式期望将根据算法而变化
与您部署的ECCR实际训练代码
对于内置算法，通常是记录
I/O Proto Buff格式
这是一种非常适合作为深度学习和其他机器学习模型的输入的数据格式
但这些算法通常也会接受
你知道的
直截了当的 CSV数据或您可能有的任何数据 但记录IO，Proto Buff通常在将数据转换为该格式时会更有效
如果您能做到的话
您可以在SageMaker笔记本中预处理数据
如果您想要的话，那是可以的
这些天您并不限制S3作为SageMaker的输入源 您也可以从Athena EMR，Redshift，Amazon Key Spaces数据库中摄取数据
如果您想要的话
您还可以将Spark与SageMaker集成
这相当酷 因此，如果您想使用Apache Spark对数据进行大规模预处理
您实际上可以使用SageMaker在Spark中
在Spark中预处理数据
这很酷 如果您想使用Apache Spark对数据进行大规模预处理
您实际上可以使用SageMaker在Spark中
我们将在后续看到那个例子
在课程中 您也拥有在jupyter笔记本中可以使用的常规工具
scikit 学习numpy pandas
如果您想使用这些工具来切片、切块和操作您的数据，以便将其喂入训练工作
这是完全可以的
所以从概念上讲，处理阶段将像这样
您将在s3中有些数据
我们将在后续看到那个例子
这就是你想要处理的数据
这可以是你想要的任何桶
这将被喂入你指定的处理容器中
这可以是内置的sagemaker之一
或者是你自己的用户提供的容器和docker，做你想要的任何事情
无论你梦想什么
输出又将回到s3桶中
如果你选择的话
只要在这个架构下，你可以把它融入到你的sagemaker笔记本中作为处理过程的一部分
使用那个处理阶段
无论你需要为训练类似的过程做什么
这将作为输入
您训练数据的S3存储桶的URL
如果您在前一阶段进行了处理
那就是处理的输出
正确 这将启动所需的机器学习计算资源来执行训练
您将提供一个S3存储桶的URL以将输出放入其中
一个指向你训练代码的ecr路径
这样你可以自定义
当然，你可以选择许多内置的训练选项
这里有我们即将讨论的所有内置训练算法
你也可以指定与spark兼容的东西
ml lib库
你可以使用你自己的自定义Python代码，支持tensorflow或mxnet
现在pi torch也支持
你也可以使用Python中的scikit-learn
在Python中使用rl estimator进行强化学习应用
此外，还可以提升Hugging Face和Chainer的所有模型
它们将供您使用
或者您可以提供自己的Docker镜像，它可以做您想做的任何事情
所以 如果您想带自己的代码
您可以将其集成到Stage Maker Training中
此外，AWS市场上有许多算法
您可以购买它们进行训练，以满足更个性化的需求
以及您在训练模型后需要支付费用的事项
现在是部署并使用它的时候了
所以你可以将你训练好的模型保存到s three中
这样你可以随时拉取并使用它
有几种部署的方式之一就是使用持久化端点
如果你需要一个总是准备好做预测的东西
从某个api或外部应用
你可以这样做，允许你根据新观察结果按需做出单个预测
或者你可能想要做批量处理
是的 也许你有一堆数据需要基于它做出预测
你可以设置一个sagemaker批处理转换来为整个数据集获取预测
然后关闭你的模型
当你完成这些
在训练过程中有很多酷炫的选择
我们有一个叫做推理管道的东西，可以用于更复杂的处理
当你需要以更复杂的方式将步骤连接在一起并将它们全部连接在一起时
还有一个叫做sagemaker neo的东西，可以将你的模式部署到边缘设备上
这可以将你的模式更接近于你已知的分布在世界各地的终端应用
对于那个应用，特制的推理实例类型是可用的
还有自动扩展能力
所以你可以自动增加你需要的端点数量
如果你在进行持续部署，用于按需预测
最后有一个叫做阴影测试的东西
它可以自动评估你的新模型与当前部署模型的性能
如果它表现不佳
我们可以谈论模型的性能，指的是预测
或者它需要多长时间
这可以用于在模型实际部署之前自动捕捉错误
所以这里有一个安全网，你也可以使用
我们会详细讨论这些
但现在 让我们深入研究我们拥有的内置训练模型 并了解一下我们有什么
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/076_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p76 03. Linear Learner in SageMaker.ai-zh.srt

```
# 🎬 076_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p76 03. Linear Learner in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们开始探索sagemaker提供的内置算法的长列表
实际上你会被期望了解这些内容
以及每个算法的特殊和古怪之处
所以大家注意听，记笔记
这些都是重要的
我们从线性学习者开始
这是一个相当简单的概念
对吧 我的意思是线性回归，这可能是你在机器学习中学到的第一件事
它只是将一条线拟合到训练数据集中的想法，对吧
一旦我有一条适合某些数据的直线
我就可以说 好的
给定一个x值 我将使用那条直线方程来预测其y值
但是线性学习者能做的远不止这些
重要的是要记住它是线性的
我的意思不是所有数据集都真的适合线性函数
对吧 不是所有数据集都能被一条直线分开
有时候你需要更弯曲的曲线才能真正理解正在发生的事情
所以你知道 你要确保你正在查看散点图并找出
一条线是否真的对你所尝试的事情是一个合适的拟合
但如果是的话，它实际上可以做比线性回归更多的事情
它可以处理回归和分类
这有点反直觉
我的意思是 通常当我们想到回归时
我们正在考虑数值问题
不是分类问题
但线性学习器实际上可以通过使用称为线性阈值函数来做两者
在分类情况下
它可以做二进制或多类分类问题
线性学习可以做很多事情
只要直线适合你想要做的
它的输入格式
我们将在每个算法中查看这一点
它希望记录io包装protobuf文件格式在float 32数据
这将是您最高性能选项
但它也可以处理原始的CSV数据
在这种情况下，它将假设第一列是标签数据
紧随其后的所有特征数据
它支持文件或管道模式，无论哪种情况
那么我提到的文件模式是什么意思呢，在SageMaker中
它将将所有训练数据复制为一个单个文件
一次性复制到您训练舰队中的每个训练实例中
而管道模式将实际从S3管道流式传输，按需读取
显然，管道模式将更加高效
特别是在处理大型训练集时
所以如果你遇到一个问题
S三需要太长时间来训练
即使开始也很难获得良好的时间
一个非常简单的优化是将管道模式而不是文件模式进行使用
记住，为了考试大家
我们将探讨每个算法
他们一些特别的事情
所以对于线性学习者
有些事情你必须知道的是，训练数据应该被归一化
你可以自己这样做，或者让线性学习算法自动为你这样做
但是你必须记住要做这或者做那
要么提前规范化你的数据
要么记得告诉线性学习者实际上为你的数据规范化
你也需要确保你的输入数据被打乱
这将对获得良好的结果非常重要
现在 线性学习者在底层实际上做了一些相当复杂的事情
它使用sgd随机梯度下降
你可以选择一个或多个优化算法
包括sgd的变体如atom和a grad
实际上它同时训练多个模型，并在验证步骤中选择最优的模型
所以非常复杂的东西
它还提供L1和L2正则化
你可以再次调整这些
这些都是防止过拟合的方法
L1最终会进行特征选择
而L2则是更平滑地加权你的单个特征
当你实际调整线性学习模型时，这些是重要的参数类型
一种是平衡的多类权重
你需要能够设置它，使每个类别在损失函数中具有同等的重要性
记住这一点非常重要，你可以调整学习率和批量大小
就像我们讨论过的那样 当我们讨论调整神经网络的时候，方式是一样的
你也可以调整L1正则化项
以及L2
L2在这个特定的超参数集中被称为权重衰减
还有一些可能或不重要的考试参数是目标
精确度和目标召回率
所以无论是哪一个
如果你将二进制分类器和模型选择标准设置为适当的值
要么召回率达到目标精度，要么精度达到目标召回率
你可以设置一个目标
你想要达到的具体精度或召回率的值
它会要么保持召回率在那个值，同时最大化精度
要么保持精度在那个值
同时最大化召回率
了解线性学习器中实例的选择类型，要么是单机，要么是多机
要么是CPU，要么是GPU
所以拥有多台机器是有帮助的
但是一台机器上拥有多个GPU是没有帮助的
记住这一点非常重要，只是为了强调
这里并不是你父亲线性回归的时代
他们给你的一个例子实际上是使用线性学习器来分类手写识别
使用mnist数据集
这有点酷
你可以实际上使用线性学习器作为分类器
并且可以输入原始像素数据并让它进行分类
它实际上可以工作 我的意思是这太疯狂了
对吧 但是再次
如果你看看它如何在底层工作
它并不那么不同
这与我们训练神经网络的方式相似
即使它不是神经网络
它在某些方面以类似的方式工作
所以这有点酷
这就是线性学习者的基本概述
我们将继续深入探讨每个算法 并讨论可能在考试中出现的重要点
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/077_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p77 04. XGBoost in SageMaker.ai-zh.srt

```
# 🎬 077_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p77 04. XGBoost in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                Next
让我们谈谈xg boost及其内置的算法用于sagemaker
xg boost最近非常流行
它代表极端梯度提升
当我们谈论提升时，我们稍微提到了它
它是一个增强的决策树组
这意味着我们有一系列决策树
我们一直在创建新的树，以便纠正前一棵树的错误
它们彼此构建，以创建一个越来越好的模型
它在使用过程中使用梯度下降
这似乎是许多这些算法的关键
它利用这一点来最小化损失
随着新树的加入
Xg boost最近在Kaggle比赛中赢得了很多
所以它是目前最热门的算法之一
它也非常快
嗯 你也不需要为此支付巨大的计算成本
尽管你倾向于在分类问题的上下文中考虑决策树
结果发现你也可以用它进行回归
用于预测数值
使用的是称为回归树的方法
对于这种情况
Xg boost有点与众不同
因为它并不是从底层为sagemaker设计的
在aws的实现中
他们只是直接从每个人都使用的开源版本的xgboost开始
并在此基础上进行构建
最初，xgboost在sagemaker中只接受csv或lib svm输入
但现在
在2020年秋天
AWS最近扩展了它，接受记录
Io Proto Buff和Parquet输入格式
因此，他们已经开始自己制作它
但本质上
它是开源的XGBoost
由于这一点，您的模型仅使用Python中的Pickle进行序列化和反序列化
您也可以将其用作SageMaker笔记本中的结构框架
使用sagemaker xgboost
这样你不必将其部署到训练主机上
或者你不必使用它的docker镜像
你可以直接在笔记本中使用它
如果你想要 当然，在这种情况下，它只是在你笔记本实例上运行
但你也可以将其作为内置的sagemaker算法使用
并引用xgboost docker镜像和acr
并将其部署到一支训练主机队伍中进行大规模训练
嗯 培训工作
关于XGBoost的事情
然而 它有很多超参数
调整它们是获得良好结果的关键
这里有一些
我的意思是 有很多
有子采样超参数
可以用来防止过拟合
所以，如果你被问到如何防止过度拟合随机梯度提升树，
调整子样本参数是实现这一目标的好方法，
同样，步长收缩系数也起到防止过度拟合的作用，
它也是用于防止过度拟合的参数之一，
其他参数包括gamma，
alpha和lambda，
gamma对应于创建新树分支所需的最小损失减少量，
alpha和lambda对应于L1和L2正则化项，
在两种情况下，
较大的值将产生一个更为保守的模型。
一些更多的超参数，你可能会在考试评估中看到
下划线指标，允许您设置您正在优化的指标
在训练期间 如果您通常关注的是准确性
您可能会将其设置为错误或rmsc
但可能您处于一种情况中，您更关心像假阳性这样的东西
在这种情况下，您可能希望优化像曲线下面积这样的东西
而不是a c 所以记住，您有这个可用的旋钮
还有缩放暂停权重，允许您调整正负权重的平衡
所以，如果你处理的是不均衡的数据
其中负面案例和正面案例很多
或者相反
调整权重可以帮助你处理这种情况
再次 如果你处理的正面和负面案例不平衡
调整权重可以帮助你补偿
建议将其设置为负面案例之和/正面案例之和
最后，让我们提到max depth，正如其名
它设置了xgboost使用的树的最大深度
我认为默认是6
但你不需要记住这一点，你需要记住的主要是
虽然如果你将其设置为高
你可能会过度拟合你的模式
你的树越深，模型就越复杂
模型将会变得越复杂
如果你的模型太复杂
那么你就有过度拟合的风险
这就是为什么max step可以是一个重要的设置
至于使用实例类型的建议
记住，XG Boost
至少在其CPU实现中是内存受限而不是计算受限
一般而言
所以，如果你不打算使用GPU
M5将是一个不错的选择，以优化内存
然而 在XGBoost 1.2版本中
可以在可用的单实例GPU上训练
这为使用P2和P3实例的可能性打开了大门
以便进行单实例GPU训练
你必须设置树方法
超参数到gpu历史记录，这样可以更快的训练
也可以更有成本效益
所以利用新的能力也在1.2中
我们也添加了g4
dn和g5实例类型作为xgboost的可能性
所以你可以得到更多的力量如果你需要它
甚至在1.5中，2023年5月引入的更好
你现在也可以进行分布式gpu训练
所以你可以有一个g5集群
例如 并将你的xgboost训练分散到那个上面去做
你还必须将树方法设置为gpu hist
就像你在1.2中做的那样
但除此之外
你还必须将使用dask gpu训练设置为true
当你设置训练输入时
你需要将分布参数设置为完全复制
这将自动将你的训练数据分散到集群中
记住分布式gpu训练 目前只支持csv或parquet输入
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/078_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p78 05. Seq2Seq in SageMaker.ai-zh.srt

```
# 🎬 078_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p78 05. Seq2Seq in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来
我们将讨论sagemaker的seek to seek算法，即序列到序列
这可能会让我们想起我们谈论过的rnns
这是一个例子，你可以输入一个令牌的序列
并输出一个令牌的序列
正如我们所说，一个常见的用途是机器翻译
你可以把一个句子看作是一个令牌序列
一个单词序列，你的输出将是基于同一短语的翻译单词序列
它也用于文本摘录
也许我可以输入一个对应文档中单词的令牌序列
输出对应该文档摘要进行的词汇序列
语音转文本 另一个例子可能是我将音频波形进行分词处理
我想要输出分词后的单词和文本
实际上在幕后
它可以使用RNNs或CNNs加注意力机制来实现这一点
这是一种种替代方法
因此，尽管我们谈论顺序数据适合于RNN
它们同样适合CNN
它期望以记录i o protobuf作为其输入
在哪里，令牌是整数
这有点不寻常
因为阶段制作中的大部分算法都需要浮点数据
但如果你仔细想想
序列将是令牌对吧
所以我们会有一些事物的序列
这些可能是词汇文件中单词的整数索引或其他东西
它真的需要记录io protobuf格式
因为这真是为像这样的神经网络提供良好的输入而量身定制的
因此，您需要以令牌化文本文件的形式提供训练输入
你不能直接把一个包含文档的文件传递进去，对吧
你需要创建一个词汇表文件，将每个单词映射到一个数字
因为电脑喜欢数字
而不是单词 所以你需要提供给它
词汇表文件和分词文本文件或其他任何东西
作为输入 你正在分词
在sagemaker中有示例代码展示了如何实际这样做
并将一切都转换为记录
我期望的缓冲区格式
最终看起来它实际上很像我们之前做的那个tf idf实验室
但最终，你提供了训练数据
验证数据集
并且需要一个词汇表，它可以用来实际映射那些分词文件
实际的话语 或者无论你是在尝试编码什么
现在 正如你所想象的那样
嗯 构建一个能够将两种不同语言相互翻译的机器学习模型是一个相当大的挑战
是的 训练这些东西可能需要几天时间
即使使用sagemaker的计算能力
你知道的 需要庞大的训练主机群
幸运的是，已经有预训练好的模型可以使用
如果你查看示例笔记本，他们提供了如何访问一种语言到另一种语言的预训练模型
他们实际上会向你展示如何访问预训练模型以进行翻译
考试不需要关注细节
但你只需要知道你可以做到这一点
并且有 你知道
公共训练数据集也存在
你可以用于特定翻译任务的
你不需要出去并像你所知道的那样
构建你自己的英语中每个单词的词典
以及这如何对应于德语中的每个单词
例如 已经有人做了这项工作所以再次
寻求的最常见用途之一是在机器翻译中
通常，这是在超参数上下文中听到它的上下文
寻求的参数是你最期望的
对于神经网络 批量大小
优化器类型 原子主义
GT或RMSProp
学习率 我有多少层
嗯 它的不同之处在于
虽然优化器
那么你优化的是什么
你可以只优化准确率
你可以提供一个验证数据集说
这是我的想法 这个词的正确翻译是
这句话应该是
它会很好地衡量
我说得对吗
但在机器翻译的世界里
结果往往更加微妙
是的 所以你更有可能使用不同的指标
例如 蓝色分数实际上可以比较你的翻译与多个参考翻译
所以它在那里可以有更多的灵活性
至于什么是正确的
还有一个叫做困惑度的指标
这在太空中被使用 这是一个交叉熵指标
但你知道为了考试的目的
只需记住蓝色得分和困惑度非常适合衡量机器翻译问题
显然作为一个深度学习
嗯 算法它利用了gpu
你应该仅仅使用gpu大小来为此
正如我们所说这是一个非常
耗时的算法
所以你可能会想用它来投掷一个p三节点
不幸的是你只能单用一台机器进行训练
所以它实际上不能在多台机器上并行化
这有点令人失望
鉴于这个训练是多么的耗时
然而 好消息是它可以在同一台机器上使用多个gpu
至少所以你想要
如果你在做一个非常大的训练工作
寻求 使用一台非常强劲的主机
嗯 p 三有很多gpu
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/079_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p79 06. DeepAR in SageMaker.ai-zh.srt

```
# 🎬 079_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p79 06. DeepAR in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来在我们sagemaker bestiary中的是deep ar
Deep ar用于预测一维时间序列数据
所以它是经典的
RNN用例，即循环神经网络
我们正在查看时间序列中的数据点
并试图预测该序列在未来会做什么
例如 查看股票价格并试图预测未来股票价格
与直接的RNN有点不同
RNN
那就是你可以实际训练深度学习模型，一次性处理多个相关时间序列
所以它不限于训练单个时间序列
如果你有许多时间序列，它们以某种方式相互依赖
它可以实际上从这些时间序列之间的关系中学习
以创建对预测任何单个时间序列更好的模型
它能够找到频率和季节性
所以你知道它将比单纯的线性回归更好
这是一种方法 它可以实际学习你数据随时间的细微差别
并将这些信息反馈到未来用于训练输入
它有一个嗯 它可以接受的种类繁多
我能处理json行格式，并且可以压缩成gzip格式
或者使用 Parquet 格式以获得更好的性能
基本上，每条输入数据的记录都必须有一个起始时间戳
紧随其后的是时间序列的值
如果可以选择，它也可以包括动态特性
例如，你知道的是在时间序列中对产品进行的促销活动或产品购买
它也可以包括类别特征
所以，如果你有那种额外的数据
它也可以从中学习
下面有一个小例子，展示了输入格式可能看起来的样子
基本上，您需要包含起始时间戳，然后目标
这是该时间序列的时间序列值列表
然后，您可以选择性地包含类别特征和动态特征
它是如何使用的 它的特点是什么
在训练、测试和推理时，您总是包括整个时间序列
即使您可能只对其某个窗口感兴趣
你想给它你所能提供的所有数据
以便它能更好地学习
总是使用整个数据集作为测试集
只是移除最后的时间序列用于训练
因此，对于RNN和时序数据的训练和测试有点奇怪
这样，你想要在保留的数据上进行评估
但同时，仍然使用那之前的原始数据来进行实际评估
你不想使用很大的预测长度值
所以不要尝试超过
不要说预测未来400个数据点
在那之后，事情开始变得有些摇摆不定，再次如此。
如果你能训练多个相关的时间序列
并且不仅仅是一个，它会释放深度学习的全部力量给你
深度学习中的一些超参数是你所期望的
无论何种类型的神经网络
训练的epochs数量
批次大小
学习率 麻木的细胞是
我们在它中使用了多少神经元
我们也有上下文长度
尽管 这基本上是模型在预测之前看到的时间点数量
这可以实际上小于你的季节性周期
但请记住，模型无论如何都会滞后一年
所以即使你用的更短的上下文长度
它仍然能够回溯时间
并捕捉到长达一年的季节性
这就是为什么总是要给它
整个数据集进行训练很重要
而且不仅仅是你关心的那种窗口
你可以根据更多的历史数据捕捉到季节性趋势
这是深度学习实例类型的最新指导
你可以使用CPU或GPU实例
这可以在一个或多个机器上运行
所以它非常可扩展
你可以将这个部署在集群中
如果你愿意
然而 即使你可以使用GPU实例并不意味着你应该这样做
官方的建议来自aws是您应该从cpu实例开始
具体来说，ml点c四点二叉大
或者，如果您需要更多的ml点c四点四x大型实例类型
并且只升级到GPU
如果必要
原因在于这真的只对较大的、深度较深的模型有帮助
所以，如果你在使用一个非常大的模型
或者你正在使用大批次大小，特别是大于512的批次大小
那么GPU实例可能对你有意义
这只是用于训练的
尽管 所以目前仅支持CPU类型进行推理
请记住，您可能需要更大的实例进行调优
调优阶段可能需要很长时间，并且非常计算密集
因此，有时在调优阶段使用更强大的实例是有意义的 然后稍微降低实际生产阶段的性能
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/080_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p80 07. BlazingText in SageMaker.ai-zh.srt

```
# 🎬 080_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p80 07. BlazingText in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来让我们探讨一下“炙热文本”算法
你会认为，名字叫“Blazing Text”的东西
这将是一个极其通用的巨大自然语言处理事物
但实际上它能做的事情非常有限
别让名字骗了你
它到底是用来干什么的
它可以用于几件不同的事情
一个是文本分类
因此它可以为给定的句子预测标签
如果你用现有的句子和与之关联的标签来训练系统
所以这是一个监督学习系统
在你用一组句子和标签训练它
你已经与那些句子联系起来了
它可以利用这些信息预测新句子的标签，即使你将它们输入到未来。
这对于像网络搜索或信息检索这样的东西是有用的
但请记住，它的设计仅适用于句子的使用
并非整个文档
所以，这是一个重要的区别，需要在考试中记住
你可以做的事情之一是我们称之为词嵌入（word to vec）的东西
这是一个创建词嵌入层的东西
所以它创建了单词的向量表示
语义相似的单词会在嵌入层的向量空间中彼此接近
基本上它在寻找相似的单词
所以你可能会问自己，好吧
这本身有什么用呢
它通常没什么用
但对于其他自然语言处理算法 它可以成为一个有用的组件
所以你可以使用那个给出相似单词的嵌入层
当你在做机器翻译或情感分析等事情时
以及其他事情
但请记住 Word2Vec只适用于单词
它不适用于整个句子或整个文档
它只是给你提供一个嵌入层，将相似单词放在一起
它只能做这些
Blazing Text 两件事
有监督的文本分类
或者词到向量
这是一个词嵌入层，用于找到相似的单词
它期望的训练输入非常具体
嗯 所以对于监督模式
当你进行文本分类时
基本上你将会一行一行地输入一个句子
那个句子中的第一个所谓的词将是下划线_label
下划线_下划线
紧随其后的是那个句子的实际标签
它还接受我们称之为增强的明细文本格式
如果你愿意并为word to vec模式
它只需要一个包含每行一个训练句子的文本文件
下面显示的是监督模式下输入的可能示例
你看到了我们有下划线下划标
下划线下划线四
这意味着我将把标签四与下面的句子相关联
Linux ray for prime time
Blah 请注意我们已经将文本分词
这样每个单词都由空格与其他单词分开
这也适用于标点符号
你会注意到逗号和句点周围也有空格
那里也有 我们还将所有文本转换为小写
以确保算法不需要处理大小写问题 To
所以这就是实际输入的示例
请记住，这些输入格式实际上很重要，可能会出现在考试中
这里也显示了增强的文本格式
这是一种稍微更结构化的方式来组合它
我们有一个来源在标签字段中
我们再次有分词和预处理的句子以及实际的标签编号
Word to vec有几种不同的操作模式
嗯 有一种叫做CBoW（连续词袋模型）
还有一种叫做Skip-gram
如果你希望一个词袋
它就像那样
你试图想象的想法
是这些单词并不是以一种结构化的方式
顺序不重要
它是一个词袋，学习单词之间的关系
所以单词的顺序在训练过程中被丢弃
只有单词本身重要
但是 它也有两种其他模式
跳过单词和批量跳过单词
它们是不同的
那些可以在批量跳过单词的情况下实际分布在多个CPU节点上
这有点好 跳过单词
我们在谈论n个单词
因此，顺序实际上很重要
在那种情况下
重要的超参数再次
这 uh 根据您使用的模式而异
对于一个词向量 我们在那里做嵌入
模式显然是非常重要的
我们使用连续词袋
Skip gram或bat Skip gram
学习率
窗口大小 向量维度和负样本数都是调整性能的重要参数
文本分类模式的性能
然后，神经网络中常见的因素在这里发挥作用
训练轮次
学习率 以及单词n
词向量的维度，实际查看多少个单词会同时被考虑
在blazing text中，连续词袋模型和skip gram模式推荐使用p3节点
你也可以使用单CPU或单GPU实例
但这将利用GPU的优势
然而，它只能单机使用
然而批量跳过单词可以用多个CPU实例代替
所以如果你需要水平扩展
批量跳过单词将是一个不错的选择
但你将使用CPU而不是GPU
对于那一个
对于文本分类模式
他们推荐一个C5节点
如果你使用少于2GB的训练数据
如果你需要更大的东西
你可以升级到P2 XL或P3 2XL 所以它可以与那个CPU或GPU一起使用
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/081_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p81 08. Object2Vec in SageMaker.ai-zh.srt

```
# 🎬 081_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p81 08. Object2Vec in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来我们将讨论对象到向量
如果你记得我们谈论过快速文本
我们谈论过单词到向量
那是局限于在句子中寻找单个单词之间的关系
对象到向量是一个类似的想法
这是一个嵌入层，它显示相似的事物
但它更具通用性
它适用于任意对象
而单词到向量在单个单词级别上操作，对象到向量
你可以实际上操作整个文档
或者其他种类的东西
如果你想要
再次，它只是一个嵌入层 它创建了这个高维对象的低维密集嵌入
它基本上将所有对象的特征和属性
简化为更低维度的表示
其中每个对象的向量表示它们之间的相似性
你可以用它做很多事情
嗯 计算对象的最近邻用于聚类目的
你可以可视化这些集群
如果你愿意 你可以用它进行类别预测
所以那些在对象
嵌入空间中接近的
可能在同一类别
如果我们处理电影或音乐之类的东西
类似物品或类似用户的有用性
也适用于推荐和推荐系统
当你向人们推荐东西时
显然你想显示类似用户也喜欢的东西
或者可能显示你喜欢的类似物品
有几种方法可以做到这一点 但无论哪种方式，对于类似物品或类似用户
物品和用户都可以作为对象
找到输入之间的关系
嗯 你必须先将数据分词为数字
首先 你知道
你不能只是扔一个图像进去并期望它工作
你需要给它一个整数列表
基本上代表其他东西
关于
对象到向量的奇怪之处在于它由一对令牌或令牌序列组成
所以你在处理像句子对或标签和序列
例如 类别到单词序列
客户对
或者产品配对 或者用户到项目配对
所以我们试图根据这些配对属性找到事物之间的关系
如果您愿意 下面是实际格式可能看起来的样子示例
如果您想看看
所以它工作的方式是我们基本上有两个并行的路径
对于每对组件
记住我们可以有不同的配对不同的事物
所以我们有两个输入通道
每个输入通道都有自己的编码器和自己的输入路径
这两个输入通道输入到一个比较器
它生成最终的标签
所以每个输入路径您可以选择自己的编码器
并且有几种选择
一种是平均池化嵌入
一种是使用卷积神经网络和双向LSTM
对于您的数据什么最有效可能会有所不同
当然
比较器本身实际上是跟随一个前馈神经网络
所以到最后您有一个基于深度学习的相当复杂的系统
它能做的事情相当令人印象深刻
您只需要在考试中理解这个高级概念
你知道细节不会很重要
如果您需要调整这个东西
嗯 深度学习中常见的问题再次出现这里
早期丢弃
停止epoch的数量
学习率 批处理大小 您有多少层
您正在使用什么激活函数
您正在使用什么优化器
您的权重衰减是多少
我的意思是这完全是深度学习的优化
还有编码器
一个网络和编码器二
这就是您实际上为每个输入通道选择编码器类型的地方
这又可以是一个卷积神经网络
一个LSTM或者一个池化嵌入层
这是关于对象2vec实例类型的最新指南
目前您只能单台机器上训练
因此您在这里受到一台机器的限制
它可以是CPU或GPU实例
然而可以是多GPU实例
这就是如何获得多个处理器的好处
并且在这里用于训练的目的
他们建议您从CPU开始
尽管他们建议从m5.2xlarge开始，但我却选择了这个
如果你想在gpu上开始
那么从p2.xlarge实例类型开始
如果你发现这不能满足你的需求
如果你有一个非常大的工作
你可以在cpu方面加强自己
也许在m5.4xlarge或a12xlarge
在gpu方面
支持的选项是p2
p3 并且新的g4实例类型
以及g5实例类型
这些gpu实例类型也适用于推理
虽然他们建议特别使用ml p3.2xlarge进行推理
他们还建议使用inference_preferred环境变量
以优化编码器嵌入而不是分类或回归 如果您认为这有意义
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/082_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p82 09. Object Detection in SageMaker.ai-zh.srt

```
# 🎬 082_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p82 09. Object Detection in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈一个有趣的话题
SageMaker中的对象检测算法
这里的想法是识别图片中的所有对象
以及它们所在的位置，通过边界框来标识这些对象的存在
例如，在这个例子中
在这里的右边 我们有一张小办公室的照片
它告诉你 嘿
嗯 有人过得不好
他们那里拿了一瓶红酒和一个酒杯
它告诉你这些物品在图像中的位置
并且给出了它们周围的边界矩形
还有一个椅子
一台笔记本电脑 一个碗
这就是它为你做的事情
它使用深度神经网络检测和分类图像中的物体
这些类别也会以置信度分数返回给你
所以你可以看到这是多么确定那是一个葡萄酒杯
而不是其他东西
你可以从这些从头开始训练，或者使用基于imagenet的预训练模型
在mxnet的情况下
或者也有tensorflow的变体
这样你有更多的选择
是的 让我们从技术上谈谈
在tensorflow中有两种不同的对象检测算法
一个是对象检测算法mxnet
我相信这是更老的原始版本
然后是物体检测算法 tensorflow
这将为您提供更多的选择
在两种情况下 它们将输入图像并输出图像中所有对象的实例
与它们相关的类别和信心分数
现在 mx net 版本使用卷积神经网络
使用单次多框检测器算法
ssd 在引擎盖下，那个cnn可以是vgg十六或resnet五十
它具有转移学习和增量训练模式
所以你可以使用一个预训练模型的基础网络权重而不是随机初始权重
如果你需要内部
它使用翻转 缩放和抖动来避免过拟合
然后我们有tensorflow版本
物体检测算法tensorflow允许你从许多不同模型中选择
这里有各种resnet efficient net mobile net可供选择
来自tensorflow模型园
如何训练它们以及如何使用它们的细节
如何配置它们将根据您选择的模型而变化
这有点像一个包装器
如果您愿意的话，这是一个用于tensorflow模型的花园和模型现在，mxnet的情况
嗯 既然我们知道它使用的模型
它对其训练输入更具体
对于这种情况 您想要使用mx nets记录
I o格式或图像格式
Jpeg png
并且，你需要为每个图像提供标注数据的json文件
当你在训练时 这就是你使用自己数据训练物体检测算法的情况
你的数据描述了图像中物体的位置
你需要提供这样的json文件
文件中包含 这张图片包含的内容
图片的尺寸
标注了图片中物体的位置
这里是该地区的类别
在tensorflow的情况下，等等
不过
这取决于你选择的模型
对于mxnet和tensorflow，有一些重要的超参数
mxnet被称为mini batch size
TensorFlow称之为batch size
同样的东西，你知道的 标准的深度学习调整你训练的批次大小
学习率
你可以选择多种优化器之一
根据实例类型的推荐，选择最适合你数据的
这是对训练的密集操作
你需要使用GPU实例
你可以使用多GPU实例，甚至多个GPU实例
在任何情况下都可以工作
他们推荐p2 x large或p3 2 x large实例类型
如果你需要更多 你可以选择这两种类型中的16 x large
此外，新的g4dn和g5实例类型也在考虑之列
在推理方面
您可以选择GPU或CPU
他们推荐在CPU方面使用m5，而在GPU方面
任何选择都可以
P2 P3 G4dn都是可以的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/083_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p83 10. Image Classification in SageMaker.ai-zh.srt

```
# 🎬 083_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p83 10. Image Classification in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                现在，物体检测算法的一个近亲是图像分类算法
你可以大致预期会有关于两者之间差异的问题
所以，物体检测算法会告诉你图像中的物体在哪里
图像分类只会告诉你图像中的物品
它会为图像分配一个或多个标签
就是这样 它只给你标签
所以给一个猫的图片
它会说嗨 这是只猫
它不告诉你物体在哪里，只告诉你图像中有什么物体
因为你并不总是需要知道它们在哪里
这取决于你的应用程序，就像物体检测一样
这个算法有mxnet的不同版本
以及更新的tensorflow版本
所以有mxnet图像分类和tensorflow图像分类
在mxnet版本中，这些被视为两种不同的生物
有一个完整的训练模式，网络以随机权重初始化
然后有一个迁移学习模式，以预训练权重初始化
并且那个顶部全连接层以随机权重初始化
然后使用您的新训练数据微调网络
这样您可以训练它识别之前不知道的对象
这与当前的物体检测算法非常相似
mxnet的默认图像大小是三通道
那就是红色 绿色和蓝色
二二四乘二二四
这将与imagenet数据集相匹配
这就是它在tensorflow中存在的原因
尽管 与物体检测一样
具体细节将根据您从tensorflow hub中选择的特定模型而变化
因此，您可以从移动网Inception，Resnet和Efficient Net算法中选择，用于使用图像分类的TensorFlow
您只需要查阅您想要使用的特定模型的文档
您需要查阅您想要使用的特定模型的文档
我不期望考试
我不期望您了解每个模型的所有细节
所以我不会深入探讨
然而 在这个情况下，顶层分类层也供你使用，无论是微调还是进一步训练。
因此，你可以使用任何这些预训练模型。
你可以将它们扩展到识别它们之前不知道的新对象。
一些重要的超参数再次。
它们的具体名称可能根据你使用的具体底层模型而变化。
但是批处理大小，
学习率和优化器，
当然会可用。
并且，
你想要调整的东西就像任何神经网络一样。
然后是优化器特定的参数
比如权重衰减
beta1 beta2
epsilon gamma
具体细节在mxnet和各种子模型之间会有所不同
关于tensorflow实例类型的推荐
再次 这是一个强大的东西
当你进行图像分类时
在进行训练时，你一定想使用gpu实例
p2s p3s
g4dns或g5s都是可以的
你可以使用多gpu实例，甚至多实例的多gpu实例
如果你真的需要在推理方面扩大规模
在推理方面你有更多的灵活性
你可以使用m5cpu实例
或p2 p3
g4 dn 或g5
gpu实例
在所有这些情况下 当然
我说的是机器学习变体
ml m5
mlp2 等等
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/084_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p84 11. Semantic Segmentation in SageMaker.ai-zh.srt

```
# 🎬 084_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p84 11. Semantic Segmentation in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们还在继续我们的计算机视觉项目
让我们谈谈语义分割
你可能记得图像分类只能检测图像中的对象
目标检测算法更进一步，告诉你这些对象在图像中的边界框
语义分割更进一步
所以它实际上是像素级别的
对象分类
所以你得到的是一个像这样的面具
这意味着对于图像中的每个像素
你认为它是什么图像中的对象
所以在这种情况下
你可以看到
这很可能是某种填充动物或什么东西
前面有一个玫瑰和一个桶
对于每个像素 它可以告诉你
这是一个填充动物 这是一个玫瑰
这是一个桶 你知道这显然在自动机动车辆中发挥作用
你真的需要知道事物的具体位置
再次
我们不仅仅是为整个图像或甚至边界框分配标签
我们实际上在这里降到像素级别
除了自动驾驶汽车，其他应用还有医学影像
诊断 机器人感知
诸如此类，我们将这个面具称为分割面具
它将单个像素映射到标签或分类
如果你愿意
当你训练时 它期望jpeg或png文件，附带注释
无论是训练数据还是验证数据
当然它也有标签图来描述那些注释的英文
它还可以接受增强和manifest图像格式，如果你想使用管道模式
这可以允许你从s3中流式传输数据，从而获得一个大的性能提升
而不是复制一切
嗯 对于推理 它会接受一个jpeg图像来说你
好的 嗯 这里有一张照片
告诉我图像中的每个像素
它基于gluon和gluon cv
后者又建立在apache mxnet之上
它给你三种不同算法的选择
你可以使用全卷积网络或fcn
还有一个叫做金字塔的算法
场景解析 或PSP
最后，深度实验室v3
具体细节对于考试不重要，所以我不会占用你的大脑
你可以选择几种ResNet口味
作为神经网络的基础架构
ResNet50或ResNet1
零一 两者都将在ImageNet数据库中训练
它了解大多数日常对象
就像我们讨论过的其他计算机视觉技术一样
你也可以增量训练，或者从头开始训练
你想做什么就做什么
你可以对它进行全新的对象训练
或者从ImageNet开始构建
如果你想
深度学习超参数这里通常的嫌疑人
Ebox学习率 批量大小优化器
也 你选择的算法当我们谈论时显然是重要的
选择主干
也是重要的参数
当你对语义分割进行超参数调整时
当前关于实例类型的语义分割指导
目前只支持GPU实例进行训练，这意味着目前是P2
P3 G4DN或G5实例
你也只能单机训练
只有语义分割在推理时你有更多的灵活性
你可以使用CPU实例
他们推荐C5或M5
或者你可以使用GPU实例 如果你想要的话，P3或G4DN
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/085_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p85 12. Random Cut Forest in SageMaker.ai-zh.srt

```
# 🎬 085_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p85 12. Random Cut Forest in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来我们将讨论随机切割森林
这是亚马逊用于异常检测的算法
它在无监督环境中工作
基本上，它正在查看一系列数据
并试图找出那些在该系列中不是异常的数据
你知道，哪些东西突出可能是有点奇怪
它可以查找像断裂和周期性的东西
有些东西就是不可分类
对于每个数据点，它将分配一个异常分数给亚马逊
他们自己开发了这个算法
他们写了一篇关于它的论文 他们看起来很自豪
这似乎是因为它一直在侵入他们所有的不同系统
所以这是一个相当安全的赌注
你会在考试中看到关于这一点
因为嗯 亚马逊似乎对此感到非常自豪，并在各处使用它
所以期望结果是CSV格式或记录
我 O 协议缓冲区 你可以使用文件或管道模式，无论哪种情况
如果只想计算准确率或精确度
或者召回率或F1分数在标记数据上
这是一个无监督算法
所以没有真正的训练
但你可以提供一个测试频道并尝试根据已知异常情况测量其准确性
它内部是如何工作的
如何在您知道异常的情况下工作
How does it work under the hood
好吧，他们关于这一点有一个完整的视频，可以在aws上找到
但你只需要知道，基本上它是创建一棵树的森林
每一棵树
就像一个决策树
是对训练数据的一个分区
它做的事情是
它看预期增加树的复杂性
由于添加一个新的点
如果你向决策树中添加一个新的数据点
并导致许多分支形成
它说很好 这可能是异常的
所以这有点酷
他们基本上在使用决策树的属性并说
好的 一个决策树需要制作许多新的分支以适应一些新的数据
可能意味着那个数据点有些奇怪
那就是基本想法
数据是随机采样的
那就是随机切割森林的随机切割部分
然后我们训练它
所以随机切割森林出现了
如我所说 到处都有
它也出现在kinesis analytics中
在那里，它可以用于对数据流进行异常检测
是的 我也可以处理流数据
它不限于处理数据批次
但本质上，它基于查看一系列数据
可能是一个数据时间序列，并标记异常
随着它前进
重要的超参数是树的数量
如果你有更多的树
它会减少噪声或每棵树的样本数量
建议你选择这样，使得每棵树的样本数量分之一
大约等于异常数据与正常数据的比率
如果你在事前有大致了解你的数据中有多少是异常的
你可以调整 嗯，每棵树的样本数量，以更好地识别异常
它不利用gpu
它本质上是一个相当简单的算法
所以你应该使用m4 c4 c5进行训练
推荐使用c5. l进行推理
那就是随机切割森林
记住，随机切割森林用于异常检测 你应该没问题
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/086_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p86 13. Neural Topic Model in SageMaker.ai-zh.srt

```
# 🎬 086_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p86 13. Neural Topic Model in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来我们将深入探讨主题建模
这就是我们试图确定文档主题的地方
基本上在高层次上
这些都是无监督方法
所以你可以只把这些文档扔进这些工具中并从另一边得到主题
请记住
这些只是算法生成的人工主题
它们不是人类可读的 不一定有意义
不管怎样 Sagemaker提供了几种方法
其中之一是神经主题模型
我们又一次试图将文档组织成主题
我们可以用它进行分类或甚至对文档进行摘要
根据我们认为它们代表的主题
这比仅仅使用tf-idf要好
我们试图搜索特定的术语
实际上是将这些术语分组到更高层次的主题中
例如 术语
自行车和汽车和火车
和里程 和速度
可能都属于一个主题 这可能代表更一般的交通
再次
模型不会知道将该主题称为交通 它只是将这些文档分组在一起
讨论自行车、汽车和火车
这实际上是一个无监督算法
因此它不知道如何称呼这些主题
因为我们没有训练它已知的主题名称
但工作原理是底层算法称为神经变分推理
如果你对此感兴趣
训练再次 这是一个
在这里不太合适的词
但你可以做一个训练
传递 并在验证或测试通道中传递
如果你希望实际在已知数据集上测量其性能
你知道主题的地方
你可以记录io
Proto buff或csv数据以实际分析一组文档
并且这些单词首先必须被分解成整数
你不仅输入原始文本
你必须实际将这些文档分解并转换为每个单词的令牌
并且还必须传递一个词汇表文件
将那些单词映射到实际表示它们的数字
因此对于每个文档，你都有一个词汇表中每个单词的计数
经常是一个CSV文件
它是分开的 辅助通道用于那个词汇数据
你可以使用文件或管道模式
两者都可以 显然管道总是更快
其中一些怪异之处
嗯 你定义你想要多少主题在最后
所以基本上主要的
超参数你有的是我想要生成多少主题
所以这将会控制试图组织成多高的级别
所以它会给你像你说的那样多的主题
那些主题不会映射到具体的可读的单词
它们只是处于更高级别的
概念，在不受监督的情况下学习
那些主题将会是我们称之为潜藏的表现
基于你文档中的前排单词
是的 它只是试图找到这些难以发现的底层关系
再次 这是其中两个
嗯 主题建模算法SageMaker提供的
我们将谈论下一个
所以你应该尝试它们两个
看看哪个效果最好
作为一个神经主题模型
嗯 批大小和学习率显然是一些重要的超参数
就像我们说的
你想要的主题数量真的是你想要调整的主要事情
这将会控制你的主题最终处于多高的级别
它可以使用GPU或CPU
我们推荐使用GPU进行训练，因为它是一个神经网络 但是，你知道的 CPU更便宜，CPU可能会对推理足够
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/087_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p87 14. Latent Dirichlet Allocation (LDA) in SageMaker.ai-zh.srt

```
# 🎬 087_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p87 14. Latent Dirichlet Allocation (LDA) in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们深入研究sagemaker的另一个主题建模算法
Lda 它代表潜在狄拉克粘土分配
我认为它发音为粘土
关于如何发音存在一些争议
实际上 有些人说 steer
我想它听起来像法语
不管怎样 它是另一个不以深度学习为基础的主题建模算法
同样它是无监督的
就像神经主题模型一样
它只是不在背后使用神经网络
但除此之外它以同样的方式工作
从使用角度来看
它是同样的东西
主题本身是无标签的
它们只是基于文档共享的共同单词进行分组
你可以实际上用它来做其他事情
如果你想要
你可以甚至 根据他们的购买将客户分组
或者做和谐
分析音乐
如果你想要的话
使用相同的算法
即使它最常用于主题建模和文档
它实际上是一个更通用的目的
你也可以用它来做其他事情
这是基于无监督聚类
基于每个对象共有的东西
它需要一个训练通道来实际分析数据
并且可以可选择地传递一个测试通道
如果你只是想要测量准确性
它是无监督的 所以它实际上不会进行任何测试
输入可以是记录
Io Proto Buff 或 csv
并且我们再次需要先对数据进行分词
每个文档将只包含每个文档中每个单词的词频
所以我们将传递一个单词列表
整数表示每个单词
以及每个文档中该单词出现的频率
而不是文档本身
与神经主题模型相同
如果你使用管道模式
它仅支持记录
I O lda的主题格式
再次，它是无监督的
它将生成 您指定的任何主题数量
就像神经主题模型一样
并且num_下划线_topics
参数是您想要调整的主要旋钮
可选地，您可以传递测试频道
如果您只想评估结果
结果将以每字对数概率形式返回
我们使用的度量标准是词概率对数，用于衡量lda的效果
功能上它与神经主题模型相同
但底层方法完全不同
它是基于CPU的
而不是基于GPU的
因此，结果可能稍微便宜且更高效
因为不需要昂贵的GPU实例来训练它
主题数量将是您关心的主要超参数
嗯 它将控制主题生成的粗细程度
以及它们生成的方式
这些主题并不具有可读性
我们谈论的是 它们是文档的组
我们将得到您指定的主题数量 这取决于您来解释它们的含义
另一个重要的超参数值得调整的是
lda背后的alpha_zero
这是所谓的集中参数的初始猜测
较小的值将生成更稀疏的主题混合
而较大的值产生均匀混合
但你不需要那么详细的信息
可能只需记住主题数量是主题建模的主要调整点
在lda的情况下
我们没有深度学习的调整数据
我们只有alpha_zero参数
CPU训练
并且只能单实例运行
无法并行训练
只需一个CPU节点 这就是您需要的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/088_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p88 15. K-Nearest-Neighbors (KNN) in SageMaker.ai-zh.srt

```
# 🎬 088_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p88 15. K-Nearest-Neighbors (KNN) in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                接下来是knn
这可能是世界上最简单的机器学习算法之一
k最近邻 但sagemaker将其提升到了新的水平
如果你不熟悉knn及其工作原理
它是一个非常简单的
基本上，你将数据点绘制在某种特征空间中
对于给定点
你只需取k个最接近该样本的点，并返回出现频率最高的标签
你只需查看k个最相似的项目
基于您与训练数据中其他点的距离度量
并从这些相似数据点中返回最频繁的标签
事实证明，您实际上也可以使用 canon 进行回归
所以它可以做的不仅仅是分类
它只是对它有一个很简单的变化
而不是仅仅从最近的邻居中返回最常见的分类标签
您返回您试图预测的特征的平均值
所以您知道，没有什么大不了的 kn
一个非常简单的分类或回归算法
它只是做的是查看 k 个最近的邻居
字面意义上 你的数据点在某个特征空间
所以你可以给它一个包含你数据的训练通道
测试通道可以用于测量准确性
或者 训练的MSC准确性指标
它可以记录
IO protobuff或csv
嗯，对于csv
第一列将是标签数据
紧随所有功能
你可以在任何情况下使用文件或管道模式
如我所说 Sagemaker将之提升到新的水平
首先 它实际上会抽样数据
假设你有太多数据需要处理
嗯 Canon在某些情况下存在缩放问题
此外，它还会对数据进行降维处理
所以如果你有很多特征
它会首先尝试将这些特征简化，以避免维度诅咒
这样找到最近的邻居会变得更容易
如果你在一个更低维度的空间
但这会牺牲一些噪声和准确性
当然 所以你知道你在做这个时要小心一点
在底层有一些选项可以做这件事
嗯 Sign或fgt是那里的选项
一旦完成 它会建立一个索引，以便在运行时快速查找邻居
然后它会序列化模型
然后你只需查询模型以获取给定k值
我想要查看多少个邻居才能得到结果
平均值或我想要分类的值
显然最重要的超参数
kNN中的k
我需要查看多少个邻居
调整它有点像一门科学
你需要实验并看看
在k值较高的情况下，我们开始得到递减的收益
此外，样本量也是一个需要调整的重要因素
否则 虽然它很简单
kNN 如果你以前做过机器学习，你应该知道它是关于什么的
你可以在CPU或GPU实例上训练它
我觉得这有点有趣
你可以利用GPU进行kNN
建议使用m5.d2.xlarge或p2.d.xlarge进行训练
对于推理 嗯
CPU或GPU
你想要什么
CPU可能会给你更低的延迟
但GPU会给你更高的吞吐量 如果你试图在大批数据上同时运行kNN
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/089_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p89 16. K-Means Clustering in SageMaker.ai-zh.srt

```
# 🎬 089_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p89 16. K-Means Clustering in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                下一项是K均值，另一个非常普遍的机器学习算法
Sagemaker也提供了它，再次将它们提升到一个新的水平
那么K均值到底是什么
嗯 它是一种无监督的聚类技术
重要的是要记住
K通常
重要的是要记住，K实际上是一种有监督的技术
在那里我们从训练数据的标签中学习，获取最近的邻居
并获得那些标签
正确k意味着
然而，是无监督的
所以基本上 它只是看一下你的数据
并尝试以有意义的方式将其分成特征空间
它工作的方式是将你的数据分成k组
这就是k和k-means的来源
其中每个组的成员尽可能相似
所以它试图找到你知道在这些特征空间中这些聚类的中心
以及如何划分这些聚类
所以，定义相似是什么意思，完全取决于你
如何实际测量两个点彼此之间的相似度
通常，这就是在归一化特征空间中的欧几里得距离
但你可以随心所欲
真的 Sagemaker带来的是web级别的处理能力
K均值聚类
在大规模上进行这一点可能会很具有挑战性
因为我们必须查看其中的每个数据点并找出答案
所以Sagemaker带来了一种在大规模上进行处理的方法
训练输入
您需要一个训练频道
显然为了实际上
嗯 您知道
分析您的数据
但由于它是无监督的
测试频道是可选的
那只会被用于测量您的结果与一些测试数据集
您可能现在拥有
当你在sagemaker中进行训练和测试时
你可以指定它与s3的交互方式
所以对于训练
你想要使用按s3键分割的标志
当你实际进行训练时
而在测试时
你想要使用完全复制的方式
这就是说 我将把我的所有数据从s3复制到每个训练节点
或者不是 在训练的情况下
在这种情况下 你不想复制所有数据
如果你想更好地扩展
你想要按这三个关键点分割数据
所以 如果你要用多个机器进行训练
这允许它更有效率
格式本身可以是记录i
O Protobuff或CSV
你可以使用文件和管道模式，无论哪种情况
那么k-means在底层是如何工作的
嗯，我们开始时就像我们使用常规k-means一样，我们为每个观察结果映射到n维空间
其中n是特征的数量
你可以想象这是一个多维空间
每个维度都是一个特征
我们基本上根据它们在该维度空间的距离来测量观察结果的相似性
在这个维度空间中
k-means的工作就是优化那些k个聚类的中心
实际上我们可以在sagemaker中指定额外的聚类中心
这将进一步提高准确性
我们最终想要得到的聚类数量是小k
我们最终实际工作的聚类数量是大k
其中k是我们想要最终得到的聚类数量
x是额外的聚类中心项
我们将实际使用大k
这是一个我们开始时使用的更大聚类集
然后在结束时将其减少到我们想要的数量
这就是sagemaker的一个小技巧
它实际上在过程中开始时使用更多的聚类
并试图随着时间的推移将其减少到我们想要的聚类数量
它在实际算法中的另一个扩展是确定初始聚类中心的过程
标准k-means会随机选择这些初始聚类中心
但这可能导致聚类过于接近
为了避免这个问题
它使用称为k-means++的方法
k-means++所做的就是将这些初始聚类中心尽可能远地分开
这使得训练工作稍微好一点
我们在训练数据上迭代
每次迭代都试图计算更好的聚类中心
同时我们在减少大k
考虑到额外的聚类中心
减少到我们最终想要的聚类数量小k 如果我们使用k-means++，它特别使用称为Lloyd的方法进行这种减少
重要的超参数
显然k
选择k的值很棘手
选择正确的k值很棘手
k-means++
人们经常使用所谓的肘部方法
所以你绘制簇内平方和作为k的函数
并尝试找到k的更大值不会产生任何额外收益的点
你应该优化簇的紧致度
这就是簇内平方和测量的内容
你还应该查看小批量大小
额外中心因子
那就是x参数
以及初始化方法
这将是随机的 或k-means++作为调优sagemaker的k-means模型的方式
例如实例类型
他们推荐使用cpus进行训练
如果你愿意，也可以使用gpu实例
如果你使用gpu实例
记住每个实例只能使用一个gpu
因此他们特别推荐使用ml g4dn x large
如果你希望使用gpu加速进行训练或推理 他们还支持p2s、p3s、g4dns和g4实例类型
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/090_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p90 17. Principal Component Analysis (PCA) in SageMaker.ai-zh.srt

```
# 🎬 090_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p90 17. Principal Component Analysis (PCA) in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们这里只有几种算法需要处理
伙计们 所以请继续关注
接下来是PCA
它代表主成分分析
PCA是一种降维技术
它的任务是处理高维数据
即包含许多不同特征或属性的数据，并将其转换为低维空间
以便更容易处理 所以再次
你知道 我们正在避免维度诅咒
如果你愿意 PCA是一种实现这一目标的方法
一种将特征减少到更少数量的特征的方法
这些特征可能不代表具体的事物
但它们代表了对模型重要的事物
我们基本上将高维数据投影到低维空间
同时在过程中最小化信息损失，这就是PCA所做的
所以你可以想象
例如
一个数据集包含许多不同的特征
我们将其投影到二维特征空间
最终得到一个二维图
将特征映射到最重要的两个维度
以保留数据集中的信息
这些减少的维度被称为成分
这就是为什么我们说的主成分分析
这些维度可能与现有的特征不匹配
你知道它可能位于某个地方
你知道在这个空间内
但大致垂直
如果它要保留信息
基本上，第一个组件将给你回最大的可能变异性
我们试图以最少的维度捕捉到你数据的变异性
第一个组件
我们得到的第一个维度
在数据中捕捉变异性的工作做得最好
第二个组件将具有次大的变异性，依此类推
诸如此类
主成分分析也很酷
因为它是无监督的
你可以直接将其应用于数据集
我们不需要训练它
它会提炼出数据的基本组成部分
预期记录i
O Protobuff或csv数据
你可以使用文件或管道模式处理任意数据
因此，它在底层的工作方式
正在创建所谓的协方差矩阵
然后它使用了一种叫做奇异值分解的算法
或者使用SVD来简化这一点
在sagemaker中，它具有两种不同的操作模式
一个是常规模式
通常用于稀疏数据
如果你有中等数量的观察值和特征
但如果你有大量的观测值和特征
随机模式可能是一个更好的选择，它使用近似算法代替
并且效果更好
PCA的主要超参数是算法模式，就像我们刚刚讨论的那样
还有被称为减去均值的参数
它有效地提前去除数据偏差
这显然很有用
例如 可以使用GPU或CPU的类型
嗯 在SageMaker中并不具体
啊 开发者指南，关于如何在两种之间做出决定
它说这取决于输入数据的具体细节
你可能想尝试使用gpu和cpu实例来实际运行pca
如果你非常关心性能
但是再次 使用pca的主要事情是要记住它是一种降维技术
它可以处理具有大量特征和属性的数据
并将这些数据压缩成较少的特征和属性
再次 这些特征和属性本身可能没有特定的名称
但是他们将代表数据中的特征
这些特征实际上捕捉了你有效拥有的数据的变异性 所以PCA是关于降维的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/091_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p91 18. Factorization Machines in SageMaker.ai-zh.srt

```
# 🎬 091_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p91 18. Factorization Machines in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们在算法动物园的下一站是因子分解机
那么因子分解机都是关于什么的呢
因子分解机擅长处理稀疏数据进行分类或回归
我指的是稀疏数据是什么意思
因子分解机的一个常见例子是推荐系统
为什么推荐系统
处理稀疏数据 嗯，推荐系统
我们试图预测用户可能喜欢的页面或产品
但问题是你可能有大量的页面或大量的产品
正确，单个用户与绝大多数产品没有互动，
因此，我们没有足够的数据用于训练，
我们没有足够的数据来训练模型，
比如，这个用户喜欢这个产品， 但不喜欢那个产品，
或者，这个用户喜欢这个产品，
但不喜欢那个产品， 实际上，用户对产品目录中的大部分产品都没有任何反馈，
因此，我们只有很少的数据，
这就是为什么我们称之为稀疏数据。
我们对每个用户的了解非常有限
并且，取决于我们来尝试找出他们可能会怎么想
所有这些他们没有互动的事情
试图填补那种稀疏
嗯 点击预测是另一个例子
在一个个体用户或会话中，可能不会和互联网上的大多数页面进行交互
然而，你知道我们对几页有一些信息
取决于我们来找出这些无数的其他页面中哪一个
他们可能会点击下一个
这就是稀疏数据的意思
这是一个监督方法
它又可以进行分类或回归
例如你可能想说
我想预测这个人是否喜欢这个产品
或者你可能想要实际进行回归，给出一个具体的评分值
他们可能会给那个物品
它仅限于对偶交互
正如我们将看到的，因子分解机涉及到矩阵的因子分解
一个二维矩阵
所以我们至少需要两个维度
其中一个维度可能是用户
另一个维度可能是项目
例如 再次
回到为用户推荐项目的案例
训练数据必须以记录形式存在
I o Protobuff 格式在浮点数三二格式
因为我们正在讨论稀疏数据
CSV实际上并不实用
在这种情况下，你最终会得到一个巨大的逗号列表
因为绝大多数项目不会与任何数据相关联
给定的用户
所以CSV在这个问题上不是一个好选择
它是如何工作的
嗯 所以我们试图将这个东西建模为一个巨大的矩阵
我们可以有一个矩阵将用户映射到项目
并且它被填满了
你知道他们是否喜欢这些东西，或者他们的评分如何
再次
这是一个非常稀疏的矩阵 其中绝大多数单元格将为空
因为大多数用户没有对大多数项目进行评分
所以我们的任务是尝试构建我们可以使用的矩阵因素
以预测给定用户和项目对给定评分的可能值
这是我们不知道的对
这对因子机器所做的就是寻找我们可以用来相乘的矩阵因素
它试图找到这些矩阵因素，我们可以用来相乘
找出答案
给定一个用户喜欢过的物品矩阵
我们预测用户对尚未观看过的物品的评分会是多少
通常这是用于推荐系统中
为了考试的目的
记住主要的事情是，如果你试图选择一个算法
与推荐系统相关的
矩阵分解机可能是一个很好的选择，关注重要的超参数
有初始化方法用于偏差因子和线性项
你可以单独调整每个方法的属性
它们可以是统一的
正常或再次恒定
细节可能不重要
但为了完整性
这些都是分解机器的主要旋钮
它可以使用CPU或GPU
他们实际上推荐使用CPU进行分解机器的训练
GPU只有在你有稀疏数据时才有益
如果你有数据稀疏
不是稀疏数据 你应该问自己
我为什么首先看因子分解机器
所以再次 这里的关键点
因子分解机器
是关于处理稀疏数据的
在稀疏数据上进行分类或回归
它处理数据对
所以用户项目对
用户点击页面 这在推荐系统中经常被使用
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/092_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p92 19. IP Insights in SageMaker.ai-zh.srt

```
# 🎬 092_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p92 19. IP Insights in SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们来谈谈ip洞察算法和sagemaker
ip洞察是关于在你的web日志中找到可疑行为的
所以 这是一个无监督技术，它学习特定ip地址的使用模式
它自动识别给定ip地址的异常行为
它可以识别来自异常ip地址的登录尝试
它可以识别来自异常ip地址创建资源的帐户
所以基本上它是一个安全工具
作为分析你的web日志以查找可疑行为的一种方式
这可能导致你标记某事物或可能关闭一个会话
它可以直接接收用户名和账户ID
因此，您不需要对数据进行大量预处理
它显然有一个训练频道
但由于它是无监督的
验证频道是可选的
如果您想 您可以使用它来计算曲线下面积分数
这是我们之前讨论过的
请记住，输入必须是CSV数据
这是一个非常简单的CSV文件
这只是实体和IP地址
因此，实体可以是用户名、帐户ID或其他标识符
紧随其后的是该实体关联的IP地址
这就是它的工作原理
它使用神经网络学习实体和IP地址的潜在向量表示
所以它在学习实体和IP地址的潜在向量表示 你知道
它正在做一些非常复杂的建模，以尝试学习特定IP地址的行为 那些实体被哈希和嵌入
所以它有一个嵌入层来像
尝试将这些IP地址组织在一起
您需要大型哈希大小才能使其工作
这将成为我们的重要超参数之一
它会在训练期间自动生成负样本，通过随机配对实体和IP
这是对算法的一种小而巧妙的转变
这是一个不平衡的数据集案例
这有点像欺诈检测
大多数交易都不会异常
它通过随机配对实体和IP来生成异常示例
地址 和嗯
你知道 那些是随机的
可疑的东西，可能异常
所以有点有趣的想法
重要的超参数
嗯 实体向量的数量
哈希大小 这就是我们之前讨论的所有内容
他们建议你将其设置为唯一实体标识符数量的两倍
这是一个稍微有点手动的步骤
嵌入向量的大小由向量维度给出
另一个你可能需要调整的参数是结果大小
结果过大可能会导致过拟合
所以这是一个需要注意的地方
由于它是一个神经网络
我们有一些常见的神经网络调整选项
训练轮次
学习率和批处理大小
你可以使用CPU或GPU
但由于它是一个神经网络
如果你能提供足够的资源，GPU是推荐的
至少需要p3 2x large或更高
它可以在一个机器上使用多个GPU
CPU实例的大小取决于你选择的参数
如果你选择使用CPU
所以再次
IP洞察的主要用途是识别IP地址的异常行为
使用神经网络 这就是主要信息
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/094_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p94 21. Automatic Model Tuning.ai-zh.srt

```
# 🎬 094_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p94 21. Automatic Model Tuning

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈sagemaker中的自动模型调优，这是sagemaker系统的一个非常令人兴奋的能力
所以，超参数调优在机器学习世界中是一个真正的大问题
对于我们所讨论的所有这些算法
我们谈到了他们暴露的不同超参数
而且有很多
你如何找到使这些东西达到最佳状态的最优值
嗯
我们有一些关于这些东西的指导 我的意思是
我们谈到了不同学习率和批处理大小的影响 你知道，有些会导致你找到局部最小值，那不是正确的答案
有些可能会导致你的模型过拟合
诸如此类
但找到这些东西的绝对最佳值 这很难
我是说，这些是非常复杂的系统 而且我们还没有找到更好的方法
除了尝试不同的值，看看哪个效果最好
通常你必须尝试这些参数的不同值
以得到一个尽可能优化的模型
这有点像机器学习的秘密
我们不完全理解里面的情况
有很多都是试错，看看什么有效
这个问题很快会变得非常严重
当你有很多不同参数想要同时优化时
所以如果我有10个不同的学习率值我想要尝试
嗯
那没问题
我可以训练我的模型并测试10次，找出哪个学习率效果最好 但如果我有10个不同的学习率 和10个不同批处理大小我想要尝试
嗯，现在我有10x10种不同的尝试
如果我还想加入不同的网络深度
我就把规模又扩大了一个数量级
随着你添加越来越多的超参数想要同时调整
这个问题就会呈指数级增长
你必须尝试所有可能值的每一种组合
每次你还要训练模型并评估模型
正如你所见
这真的
真的很昂贵
无论是时间还是金钱，很快就会变得非常昂贵 所以这就是sagemaker中的自动模型调优试图解决的问题
基本上，你只要定义你关心的超参数
以及你想要在这些超参数上尝试的值范围
以及你优化的指标
这就是自动模型调优在sagemaker中的作用
Sagemaker可以启动我们称之为超参数调优作业
它将训练你所允许的所有组合
因此，你可以设置一个上限，指定你想要运行的训练步骤，以控制成本
它会尽力在那个范围内工作
随着它的进行
它将启动训练实例以尽可能多地并行运行
可能涉及许多实例
并尽可能快速地尝试所有不同参数的组合
这可能需要大量的计算能力
但至少你知道
我们可以利用sagemaker的并行能力
以及启动整个单独实例的能力来为你做这件事
以尽可能快地做到这一点
一旦完成
产生最佳结果的超参数集可以转换为高度优化的模型
该模型使用你找到的最佳参数
但这里才是真正酷的地方
自动模型调优和sagemaker的特别之处在于
它是随着进行而学习的
它实际上不会尝试所有可能的组合
它可以实际上学习随着时间的推移
知道，朝着这个参数的方向走有积极的影响
而这个有负面影响
它可以利用这个来更智能地尝试实际参数
因此，它不一定会尝试所有可能的参数组合
嗯 随着进行而学习以尝试智能地
找出哪些参数在下次尝试时最有意义
通过这样做
它可以节省大量资源以进行超参数调优
在进行自动模型调优和sagemaker时，有一些最佳实践你应该遵循
这很重要
记住这些
首先 不要同时尝试优化太多超参数
就像我们提到的那样 这会迅速爆炸
随着你添加更多超参数
这基本上是你需要探索的另一个参数空间维度
并且它会以指数方式爆炸
所以请专注于你认为会对模型准确性产生最大影响的超参数
或者你正在优化的指标
从开始
你可以在其他参数上进行更多调优作为第二次尝试
稍后
还确保将范围限制在最小的范围内
如果你有一些关于可能工作的参数的指导
不要探索那些范围之外的疯狂值
因为这将产生你不需要完成的工作
随着进行而学习
另一个关键点是在适当的时候使用对数刻度
所以每当你进行自动模型调优工作时
你会告诉它不仅范围
而且还有你想要探索这个范围的尺度
线性将以线性方式进行
但如果你有一个超参数
其值倾向于从零点零到零点一
例如
你可能想用对数尺度来尝试 而不是线性尺度
如果你使用线性尺度
你将花费很长时间 但是对数尺度将更快地探索
就像我们讨论的sagemaker的参数调优一样
嗯 随着它学习
如果它同时做所有事情
它不能那样学习
它工作得更好如果你只运行
允许sagemaker从那些结果中学习
这可能会限制过程学习的效果
这真的是sagemaker在做超参数调优时的效率的关键
也
嗯最后
你必须确保最终报告的是正确的目标指标
你知道
所有那些训练实例的结果
你想要确保它与超参数调优一起玩得开心
在超参数调优结束时报告你想要优化的目标
在超参数调优结束时
当所有事例都回来一起
但关键点是使用小的范围
如果你可以的话不要同时做太多超参数
因为学习依赖于某种顺序的学习
随着时间的推移
也 在适当的时候 使用对数尺度来探索你的参数空间
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/095_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p95 22. Apache Spark with SageMaker.ai-zh.srt

```
# 🎬 095_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p95 22. Apache Spark with SageMaker

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                那么我们来谈谈sagemaker和apache spark的交集
Apache Spark是一个非常流行的数据预处理框架
它也有一个非常强大的ML lib库
它也可以进行大规模的机器学习
在很多方面
Apache Spark做了很多sagemaker做的事情
但它能做的更多，因为它在预处理数据方面非常出色
基本上 它的工作方式是你将数据加载到Spark中的一个称为数据框的东西中
你可以将数据框的处理分发到某种程度
在Spark上操纵和按摩数据，覆盖整个集群
那么，如果你能将SageMaker和Spark结合在一起，岂不是很酷
实际上，你可以利用AWS和Spark的力量
AWS提供了一个SageMaker Spark库
它基本上让你能够在Spark驱动脚本中使用SageMaker
那看起来是怎样的
你如何使用它
你会像平常一样用Apache Spark预处理你的数据
无论你的数据处理过程是怎样的，收集数据，映射它，减少它
或者你需要做任何其他事情
即使使用Apache Spark，你也会那样做，就像平时一样
当你完成时，至少在Python的世界里
你会得到一个称为数据框的对象
这是Spark中包含所有你预处理数据的对象
而不是使用Spark的ML库
你可以使用称为SageMaker Estimator的东西，它工作方式相同
它会暴露一些在SageMaker中更受欢迎的算法
基本上，你可以在Spark中使用它们
例如 K means和PCA以及XG Boost
Xgboost 作为一个当今非常流行的算法
那就是赢得很多比赛
主成分分析用于降维和K均值聚类用于聚类
这将生成一个sagemaker模型，您可以使用它来进行推理。
所以看起来它像普通的Spark代码，如果你在这里看它
如果你熟悉Spark代码
但是，而不是使用Spark ML库的实现
我们使用的是sagemaker估计器和sagemaker模型
因此对于机器学习部分
我们将事情移交给sagemaker，让它在自己的框架内运行
与火花集群本身相反
我们将离开并启动我们自己的ml实例在sagemaker中
执行最后一阶段
同时使用火花进行所有预处理
在实际操作中，这种方法
您可以将sagemaker笔记本连接到远程弹性map
减少集群运行火花
所以记住emr可以在其上运行火花
我们需要将我们的sagemaker笔记本连接到该火花集群
所以我们可以使用它 或者你可以使用热气球
如果你更喜欢在Spark中预处理和创建的训练数据框
特征列应该以双精度值向量的形式出现
双精度值
以及一个可选的标签列，同样以双精度值形式出现
如果你在做有监督的学习
然后你只需要 嗯
创建一个sagemaker estimator，并在其上使用该数据框进行拟合
这将给你返回一个sagemaker模型
然后你就可以在训练好的模型上使用sagemaker model的transform进行推理
这也适用于Spark管道
所以sagemaker和Spark之间有一个很好的集成
你为什么要费心做所有这些呢
这允许你将Spark中大数据集的预处理能力与sagemaker的训练和推理结合起来
这就是两者的结合
这基本上是两者的结合
是的 Spark实际上也可以进行大规模的机器学习
但如果你有AWS资源想要使用
并利用sagemaker的所有特殊能力
例如自动超参数调优
你可能想同时使用它们 所以知道你可以这样做是很好的
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/096_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p96 23. SageMaker Studio, and SageMaker Experiments.ai-zh.srt

```
# 🎬 096_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p96 23. SageMaker Studio, and SageMaker Experiments

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                让我们花点时间来了解sagemaker的新功能
这些功能在2020年被引入
我们将花更多的时间来讨论2021年的新功能
因为这些功能已经开始出现在考试中
sagemaker已经向ide环境迈出了一大步
sagemaker studio
他们一直在扩展和开发更多的功能
这样你就不需要写太多代码
你可以直接在网页浏览器中完成所有操作
我们会简要介绍2020年的一些新功能
这些功能还不会出现在考试中
但我们会随着它们的出现而详细介绍
主要内容是他们推出了一个叫做sagemaker studio的东西
他们正在建立一个可视化的ide或机器学习集成开发环境
他们真正在尝试为使用sagemaker的开发者体验进行升级
你知道 从当前的实现方式
将aws控制台转换为一个更像实际开发环境的东西
你知道 一个你会享受工作的环境
它集成了我们即将介绍的许多不同功能
今天,sagemaker studio是一个你可以用于sagemaker的独立用户界面
除了当前的界面
它目前处于预览阶段
我不惊讶它会取代现有的ui
而不是仅仅作为一个替代品
但现在它与现有的sagemaker ui并存
就是我们之前看过的笔记本
这就是sagemaker studio中的新笔记本界面
并且 他们正在尝试将jupyter笔记本与sagemaker studio集成
你知道,这大部分只是外观和感觉
它只是感觉更像一个ide
但它仍然是一个jupyter笔记本
但它允许你分享这些笔记本
这真是个酷功能
所以 它允许你与你团队的其他人合作
并将这些笔记本与一群人分享
它还允许你在不同硬件配置之间切换
因为你不需要管理任何基础设施
所以 你正在运行aws管理的硬件
而不是你自己的硬件
所以这只是一个允许你与你的同事分享笔记本的新外观和感觉
与你的同事
sagemaker studio的另一个组件是sagemaker experiments
这是为了让你在一个地方组织、捕捉
并比较和搜索所有你的历史ml工作
所以当你使用sagemaker时
你倾向于积累一个很长的笔记本列表，你在上面尝试过各种东西，也做过各种工作
这是唯一一个可以搜索这些笔记本并理解它们的地方
你可以尝试找出哪个模型表现最好，并且将它们相互比较
这是用sagemaker实验创建的许多不同模型的结果可视化和解释的更有用的方式
sagemaker实验是您使用sagemaker创建的许多不同模型的结果 模型
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/097_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p97 24. SageMaker Debugger.ai-zh.srt

```
# 🎬 097_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p97 24. SageMaker Debugger

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                亚马逊最近在sagemaker debugger上投入了大量资金
这在考试中出现的频率开始增加
因此，值得对这个话题进行更深入的了解
sagemaker debugger
它的想法是它会定期为您保存内部模型状态
这样您就可以回溯并查看训练期间的情况
随着模型的训练
它会随时间存储单个梯度和张量
这样您可以看到随着训练的进展趋势
您不必手动计算
您可以实际定义规则，以便在训练过程中检测到不受欢迎的条件
如果您想要监控某些值超出范围
您可以为这种情况设置规则，并在发生时自动触发某种警报
当它发生时
对于每个规则中定义的内容
将单独运行一个调试任务
当规则被触发时
它会自动记录一个cloudwatch事件
您可以对此做出响应
您可以像响应任何cloudwatch事件一样做出响应
也许您会通过短信发送通知并发送一条消息到您的手机
我不知道您想做什么
这与sagemaker studio集成
它有一个叫做sagemaker studio debugger仪表板
因此您可以在可视化环境中完成这一切
它还可以自动生成训练报告
只需勾选一个框
您可以说想要详细报告训练期间的情况
sagemaker debugger将提供有关训练过程的更多详细信息
sagemaker debugger内置有多种规则
它们属于这三类
一组规则用于监控系统瓶颈
另一组规则将分析您的模式框架操作
例如tensorflow或底层的任何内容
您还可以使用sagemaker debugger调试模型参数
这些框架操作
这些是可以支持的框架以及可以支持的算法
它可以与tensorflow一起工作
PyTorch或MXNet
并且它可以与XG Boost算法一起工作
以及sagemaker通用估计器
这将适用于几乎所有内容
包括自定义训练容器
还有调试器的API在github上可用
亚马逊为您提供这些API
使用这些API
您可以在create training job API下构建自己的钩子和规则
以及描述训练工作API以做您想做的任何事情并连接到调试器
调试库称为sm debug
这是客户端库，它允许您为访问训练数据注册钩子
并将这些反馈输入到sagemaker调试器中进行所有分析和处理
所以sm debug是集成sagemaker调试器的客户端库的名称
与您的培训代码集成
在sagemaker调试器中，最近又添加了一些新功能
在2021年，sagemaker调试器又添加了一些新功能
这里有sagemaker调试器洞察
仪表板，您可以在这里看到
您可以以图形方式查看所有内容
并查看训练过程中随时间的进展
还有调试器中的新兴特定性能规则
我们在上一张幻灯片中简要提到了这一点
但这里有更多的细节
有性能报告
性能规则会启动性能报告
您可以在仪表板上查看
还有硬件系统指标
规则可以进行配置文件
这将监控CPU使用情况
GPU使用情况
以及训练过程中集群中的其他相关系统使用情况
它还允许您监控和配置文件框架指标
例如 最大初始化时间
整体框架指标
步骤异常
这些都是训练框架的超参数
如果您愿意，这将是训练框架的实际运行超参数
还有三种内置操作可以接收通知或响应调试器事件停止训练
您可以调用停止训练以响应调试器事件停止训练
您可以发送电子邮件
或者您可以发送短信
电子邮件和短信通过简单通知服务工作
这些都是您设置调试器规则后自动进行的
因此，您可以设置一个调试器规则
它会自动停止训练或通过电子邮件或短信通知您使用sn s自动进行 这允许您同时配置文件系统资源使用情况和训练过程随时间的进展
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/098_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p98 25. SageMaker Autopilot  AutoML.ai-zh.srt

```
# 🎬 098_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p98 25. SageMaker Autopilot  AutoML

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我们来谈谈sagemaker autopilot和一些更深入的内容
因为现在这也出现在考试中
Sagemaker autopilot只是我们所说的auto ml的一个包装
所以，自动找出正确的模型和算法的想法，基于你的数据进行预测
所以，这是一种自动化的方式 如果你愿意，这是一种自动化
它做了所有的重活 找出应用什么是最好的模型
以及如何调整那个模型以获得最佳结果
自动进行
所以，有一个巫师你可以通过，说我的数据在这里
我想要预测的东西在这里
这是我要训练的数据
它会走开并自己找出答案
它会创建一个优化的模型
找出使用哪种模型
如何调整它并显示给你结果
任何人都可以做到
这有点疯狂
它可以自动化算法选择
也有你的数据预处理
模型的调整
以及所有相关的基础设施
并使它们协同工作
所以，只需点击几下鼠标
你也能成为机器学习工程师
疯了，对吧 它为你做了所有的试错
自动进行 所有的超参数调整
所有的不同模型类型的实验
你将自动获得一个最优结果
更广泛地说，再次
这被称为auto ml
这并不是什么新东西，但sagemaker
Autopilot是他们对sagemaker的auto ml的命名
工作流程看起来像这样
他们允许你有更多的控制
如果你想要的话，第一步是从s three加载你的训练数据
然后你需要选择你想要预测的目标列
然后它会自动启动模型创建
所以它会去尝试不同的模型并找到最适合你数据的那个
以创建最佳预测
然后，到了这一点，它会向你提供一个模型笔记本
所以它并不是一个完全的黑箱
它会给你一个笔记本
所以你可以看到它实际上做了什么
你可以对它的运作有可见性，并且可以控制并调整它
如果你想要的话 所以你可以只使用autopilot作为一个起点
这真是个好主意
这将节省你很多工作
但是，你可以根据自己的判断和精炼来应用它提出的内容
如果你愿意
它还会显示一个模型排行榜
这样，你会看到一个推荐的模型排名列表
那些表现最好的模型
你可以选择一个
如果你愿意 你不必使用首选项
如果你需要，你可以回去更改那个
然后你可以选择并部署那个模型进行监控
并且你可以随时监控它
如果你需要 你可以在那个笔记本中进一步完善它
如果必要的话
或者你可以让它继续运行
是的 这就是来自stage maker的实际截图
自动飞行员 这里 你想运行一个完整的实验吗
是的 它会自动进行
它是 我不太确定我的感受
老实说 它确实有点过于简化
但这就是它的作用
它自动运行，还有一些细节
你也可以在过程中加入尽可能多的人为指导
所以你可以让它自动完成整个过程
或者你可以在过程中注入人为指导
你也可以选择是否使用代码
如果你不想写任何代码
你不必 但如果你想在sagemaker studio中
你可以使用任何aws sdk在过程中注入代码
你可以在自动模式下解决三种主要类型的问题 二分类
某事是否为真或假
或属于某一类别 或另一多分类
以及回归
你知道 基于历史数据点进行简单预测
有三种主要算法类型可供选择
如果你在超参数
自动优化模式下运行或hpo
那就是线性学习
Xg boost
And any number of deep learning algorithms as well
There's also a mode called ensemble mode that automatically mixes together all of those algorithms
And then some And lets them all sort of vote on the best outcome
Based on the ensemble of algorithms as well
That's also an option
Your input data must either be tabular
Csv Or more recently parquet has been added as well as an input type
Little more depth on autopilots training modes
I'm a little surprised as much depth is on the exam
But that's what we're hearing
So there's a few choices you can select when you're setting up autopilot
One is for hyper parameter optimization or hpo
And in this method
Autopilot will automatically select the algorithms that are most relevant to your data set
Talked about that a little bit in the previous slide
The choices that it has to work from are linear
Learner Xgboost
Or a deep learning multi level perceptron sort of thing
And it will automatically set the best range of hyperparameters to tune your model
And then go off and run up to one hundred trials by default
To find the optimal hyperparameters in that range
As you can imagine That might get kind of expensive and resource intensive
But hey It works now
How it actually does That depends on the size of your data set
So if your data set is less than one hundred megabytes in size
It will use something called bayesian optimization
But if it's greater than that
If it's a large data set
Then it will use multi fidelity optimization
That means that it will automatically stop a trial that's running
If it's performing poorly compared to other trials
That it ran earlier
The other choice is ensembling
And again this is just more depth on what we talked about in the previous slide
The way that works is that to train several base models at once
Using something called the auto glue on library
And it has a wider range of models to choose from in the ensembling mode
So in addition to linear learner
Xgboost and deep learning
There are more tree based methods that are available to it in neural network algorithms as well
You know just things like random forests and a bunch of other tree related ones
And and some neural network algorithms that are based on pi torch in ensembling mode
It will run ten trials
所有具有不同的模型和参数设置
然后我们将它们组合在一起形成堆叠集成方法
而不是只有一个模型
它将同时运行这十个模型，并使用这些来找到最佳最优结果
还有自动模式
这可能是大多数人会做的事情
这背后的工作方式是
如果你的数据集大于100MB
它将自动使用超参数优化模式
如果小于100MB
它将使用集成模式
就这么简单
但是这里有一个警告
为了做出这个决定
Sagemaker AutoPilot需要能够确定你的数据集大小
如果不能确定你是否大于100MB或不
那么它无法做出选择并将默认使用HBO
在这种情况下 这可能导致一些情况
它将是 如果你的S3数据存储在虚拟私有云(VPC)中
在这种情况下 Sagemaker AutoPilot无法确定你的数据集大小
它将默认使用HBO
此外 如果你的S3数据类型设置为manifest文件
或者你的S3URI包含超过一千个项目
自动模式将放弃并使用HBO
因为它无法自行确定
这就是它的局限性
所以这些是Sagemaker AutoPilot训练模式的一些细节，可能很重要
再次，AutoPilot不需要是完全的黑箱
这里有一个真正的危险
从伦理角度来看
如果我只是点击一个按钮并创建了我的训练数据的最优模型
我不知道训练数据中可能存在的偏见
我 可能会创建一个非常偏见的东西
并做不道德的事情
我知道 因此，了解自动生成的模型正在做什么非常重要
这不仅仅是因为你想要做正确的事情
还可能涉及审计问题
或者 嗯 你知道的
监督你所尝试构建的东西
因此，为了达到这个目的
Sagemaker AutoPilot与称为Sagemaker Clarify的东西集成
Clarify 这是sagemaker的另一个相对较新的功能
所以澄清它告诉你基本不同特征如何贡献每个预测
它给你更多的透明度关于什么特征进入了你的预测
以及基于那些特征可能存在的偏见
所以你知道 例如
如果你创建一个模型来批准房屋贷款
并且你看到种族是某种非常强的特征
那里出了点问题
你最好回去看看数据源可能存在的偏见
是的 我真的很喜欢这个
这真的很aws
承认 没有进一步的分析你可能看不到偏见的事实
真正的危险是自动ml是你不加思考就推出这些模型
而没有考虑它们
这些种类的偏见可以没有甚至你意识到
所以这些是伟大的功能 所以它确实给你透明度关于模型如何到达他们的预测
它通过使用来自shap baselines的shapley值来确定特征的重要性
这是shapley值的基础
基于这里右边的这篇论文
这项研究的来源实际上是来自合作博弈论领域
所以那里有一个不错的交叉点
它工作的方式是为每个给定的预测分配每个特征的重要性值
所以它是一个回去并找出什么特征在你的训练数据
或者你的数据源实际上对你的最终预测贡献了最多的权重
所以这很重要要知道
并且如果你看到特征的重要性对预测的惊喜
那么
是时候应用人类的知识和直觉
并且关于领域空间的知识来弄清楚 在你的训练数据中是否真的有偏见存在
你需要回去清理或解决
某种方式
一些特征的重要性 你最终预测
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/099_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p99 26. SageMaker Model Monitor.ai-zh.srt

```
# 🎬 099_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p99 26. SageMaker Model Monitor

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                我也会在sagemaker模型监控上花更多的时间
尽管这也是一个相对较新的功能
但它已经开始出现在考试中
已经 这里的想法是自动获取部署模型质量偏离的警报
一旦你部署了一个模型
你不想只是让它永远运行而没有人监控它
随着时间的推移，事物可能会漂移
偏见可能会渗入
质量可能会下降
随着训练数据或实际世界数据的属性随时间变化
所以这可以让你保持关注
它会通过云观察自动工作
所以你可以自动收到通知
一旦事情发生变化到某种程度，值得你的关注
你可以做的一件事是可视化我们称之为数据漂移
数据漂移的一个例子
让我们再次假设 我们做一个贷款模型
也许它开始因为数据漂移或缺失的输入特征而给人们更多的信用
也许随着时间的推移，人们的收入因通货膨胀而增加
或者我们错过了一些数据
因为无论什么原因，人们停止了收集它
随着你用于训练模型的数据发生变化，这就是数据漂移
它允许你随时间可视化这一点，甚至可能通知你
如果事情开始变化太多
模型监控也可以用于检测异常和离群值
如果你开始看到新的数据异常和离群值
模型监控可以设置以随时间观察这一点，并通知你当你看到新东西时
它还可以用于通知你数据中出现的新特征
那是你以前没有看到的
所以如果你有更多的信息
也许你想利用这一点
你知道的
或者也许这是一个新特征，是用来取代旧特征的 这是你需要了解的重要事情
所以模型监控会自动告诉你，如果新特征正在进入，你需要考虑
正如许多这些新的sagemaker特性一样
不需要代码
你可以通过sagemaker studio和一个基于网页的仪表板设置这一切
如果你想要，我们之前提到过
在自动ml和sagemaker autopilot的背景下，简要介绍了clarify
sagemaker模型监控也与clarify集成
再次，clarify旨在检测潜在的偏见
所以模型监控可以与clarify一起工作
告诉你当你的部署模型中出现新的偏见时
例如，你可能看到不同群体、年龄或收入阶层之间的不平衡
sagemaker模型监控也可以与clarify一起工作
例如 你可能看到不同群体、年龄或收入阶层之间的不平衡
你可能想理解那里发生了什么
如果你的预测中最有贡献的特征突然发生了变化
你需要关注数据中的潜在偏见，以便加以解决
因为这可能不是真实的
因此，使用模型监控
你可以监控偏见，并通过云观察接收有关新潜在偏见的警报
与大多数集成一样，它集成了云观察
它还有助于解释模型的行为
它将帮助你理解哪些特征对您的预测贡献最大
所以它只是给你透明度，告诉你发生了什么
当与模型监控集成时
它可以实际通知您当情况发生变化时
当您开始看到新偏见出现，而这些偏见在之前并不存在时
关于模型监控的一些更多细节
数据存储在S3中，并且可以在那里得到安全保护
就像您在S3中保护任何其他数据一样，通过所有S3的常规安全措施
您需要设置一个称为监控日程的任务，以定期重复运行这些监控任务
这只是部署它们的另一个步骤
这些指标再次发送到云观察
因此从那里您可以做任何您想做的事情
你可以使用云观察通知来触发警报
然后你收到警报后
你可以采取纠正措施
比如重新训练模型或审计数据以查看发生了什么
模型监控还与TensorBoard集成
快速站点和Tableau
或者你可以在SageMaker Studio中直接可视化结果
Studio中的东西
可以特别监控的数据漂移
以及数据质量
我们只是指特征的统计属性
所以你知道平均值，标准差
最小值 最大值 诸如此类
这将基于你显式创建的基准
当你创建你的模式监控工作时
你也可以为模型质量创建一个基准，而不是数据质量
所以由模型质量
我们指的是准确性
Rmsse 精确度
召回率很大程度上取决于你想要做什么
就像之前一样，你创建一个模型质量基准来衡量相对事物
它可以提醒你，当事情偏离得太远时
在预测质量的方面
你可以将模型质量监控与真实标签集成
所以你实际上可以看到人类对分类的说法
与你的模型所做的进行比较并提醒
如果这也开始偏离得太多
它还可以监控我们之前讨论过的偏置漂移
使用澄清
它将实际查看您是否在特征数据中看到了新的偏置
它还可以监控特征归因漂移
如果我们看到更多特征归因于您的预测的漂移
这与偏置非常相似
它也会向您发出警报
这更具体
尽管它使用一个称为归一折扣累积增益的指标
或简称ndcg来计算特征归因漂移
它将比较训练数据与实时数据的特征排名
所以它只查看特征在排序中的重要性
并查看它是如何变化的 与您的训练数据和正在流入的实时数据
                
```

### /content/drive/MyDrive/bilibili/Udemy-AWSCertifiedMachineLearningSpecialty2025-HandsOn!part1/100_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p100 27. Deployment Guardrails and Shadow Tests.ai-zh.srt

```
# 🎬 100_Udemy - AWS Certified Machine Learning Specialty 2025 - Hands On! part1 p100 27. Deployment Guardrails and Shadow Tests

                【角色设定】
你是一位精通 AWS 认证知识,Snowflake, Airflow, 机器学习/深度学习等的中英双语技术编辑兼讲师。
你的任务是：纠正、润色并整理我提供的 AI 机翻中文字幕内容，使其成为既自然通顺、又符合 AWS, Snowflake, Airflow, 机器学习/深度学习 等专业表达的学习资料。

【内容背景】
输入的文本是从英文 AWS,Snowflake, Airflow, 机器学习/深度学习 等技术课程自动机翻而来，存在以下常见问题：
- AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语翻译错误或不一致；
- 缺少中文标点，语序不自然；
- 不符合中文技术教学风格。

【执行要求】
收到内容后，请严格按照以下步骤输出：

---
标题
### ✅ 修正版讲解段落（自然语言版）
- 用自然、流畅的中文重新叙述整段内容；
- 修正所有 AWS,Snowflake, Airflow, 机器学习/深度学习 等专业术语，使其与 AWS,Snowflake, Airflow, 机器学习/深度学习 等官方文档一致；
- 语气应贴近日常教学讲解风格；
- 结构清晰，标点完整。

---

### 🧠 AWS,Snowflake, Airflow, 机器学习/深度学习 等 认证笔记版（复习整理）
请将上述内容转化为适合复习的笔记格式，包括但不限于以下要素：

#### 📘 一、定义与功能
简要说明服务或概念的核心定义、作用及常见用途。

#### 🧩 二、核心要点或组件
用表格或列表列出：
- 关键资源对象
- 检测/运行逻辑
- 触发条件

#### ⚙️ 三、运行机制 / 工作原理
解释该服务的扫描机制、自动化流程或数据来源。

#### 🧠 四、考试提示 / 常见考点
总结考试中可能涉及的重点、易混概念与对比。

#### 🪙 五、优势与特点
列出服务优势、特点与常见集成方式。

#### 📌 六、总结表（如适用）
使用“项目/内容”表格形式总结要点。

---

【输出格式要求】
请严格按照下列排版样式输出：

好的 👍 以下是对你提供的 **AI 机翻内容** 进行的完整修正版本。
我首先输出一段 **通顺、准确的讲解版本**（贴近日常理解与 AWS 等官方表述），
然后提供一份 **AWS，snowflake, airflow, 机器学习/深度学习 等认证笔记格式总结版**，方便复习与记忆。

（接着输出上文两个部分的内容）

---

【执行】
收到以下内容后，严格按照上述要求输出，不需额外解释或总结。


                在sagemaker中有一些新的部署安全特性
确保当你部署一个新模型时
坏事不会发生
如果发生了 可以迅速发现
有一个叫做部署护栏的功能
这可以部署到异步或实时推理端点
我知道我们还没有讨论过异步端点
但它们听起来就是这样
它们是一个返回异步响应的端点
它们不会等待响应
它只是 发送请求
随时给我答案 这里的想法是，使用部署护栏
可以控制流量的切换到一个新模型
你不仅仅是部署它
然后祈祷
它允许你做蓝绿部署
想法是你的旧模型在蓝舰队运行
你的新模型在绿舰队运行
有三种不同的执行方式
一种是一次性切换
你将所有事情切换到绿舰队
监控一段时间自动
如果看起来不错
只有在这种情况下，蓝舰队才会被终止
还有一种是金丝雀模式
我们只将一小部分流量切换到绿舰队
并监控一段时间
如果金丝雀看起来不错
然后所有事情都切换到绿舰队并终止之前的蓝舰队
还有一种线性模式，我们将流量以线性步骤切换
你可以有任意数量的步骤，慢慢将流量切换到新模型
并监控整个过程
如果你担心扩展
或者新模型处理流量的能力
这是一个好方法在受控的情况下监控
随着时间慢慢增加流量到新模型
并监控整个过程
它支持自动回滚
如果事情出错
可以自动回滚到蓝舰队之前的模型
至少你知道它工作
另一个特性叫做影子测试
这允许你比较生产模型的性能
与影子变体的性能
影子变体处理流量的一部分
并监控其性能
在这种情况下，你将手动监控 你决定何时将其推广到生产
                
```